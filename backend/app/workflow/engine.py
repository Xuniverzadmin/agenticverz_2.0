# Layer: L4 â€” Domain Engine
# Product: system-wide
# Temporal:
#   Trigger: worker
#   Execution: async
# Role: Deterministic workflow execution engine
# Callers: workers, runtime service
# Allowed Imports: L5, L6
# Forbidden Imports: L1, L2, L3
# Reference: Workflow System

# Workflow Engine (M4)
"""
Deterministic Workflow Engine for multi-step execution.

Design Principles (aligned with PIN-005 Machine-Native Architecture):
1. Deterministic execution - same inputs produce same outputs
2. Checkpoint & resume - workflows survive restarts
3. Golden-file pipeline - CI gates on replay fidelity
4. Per-step policy enforcement - budget, rate limits, sandbox validation
5. Structured outcomes - never throws, always returns data

Usage:
    engine = WorkflowEngine(registry, checkpoint_store, golden)
    result = engine.run(workflow_spec, run_id, seed=12345)
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import random
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Protocol

from .errors import (
    WorkflowError,
    WorkflowErrorCode,
    classify_exception,
)
from .metrics import (
    record_step_duration,
    record_step_failure,
    record_workflow_end,
    record_workflow_failure,
    record_workflow_start,
)

logger = logging.getLogger("nova.workflow.engine")


def _derive_seed(base_seed: int, step_index: int) -> int:
    """Derive deterministic seed for each step."""
    s = hashlib.sha256(f"{base_seed}:{step_index}".encode()).hexdigest()
    return int(s[:16], 16)


def _deterministic_jitter(seed: int, attempt: int, base_ms: int) -> int:
    """
    Compute deterministic jitter for retry backoff.

    Uses a seeded RNG so retries are reproducible during replay.

    Args:
        seed: Step seed for determinism
        attempt: Current retry attempt (0-indexed)
        base_ms: Base backoff in milliseconds

    Returns:
        Jitter in milliseconds (0 to base_ms/2)
    """
    rng = random.Random(seed ^ attempt)
    max_jitter = max(1, base_ms // 2)
    return rng.randint(0, max_jitter)


def _compute_backoff_ms(attempt: int, base_ms: int, seed: int) -> int:
    """
    Compute exponential backoff with deterministic jitter.

    Formula: (base_ms * 2^attempt) + jitter

    Args:
        attempt: Current retry attempt (0-indexed)
        base_ms: Base backoff in milliseconds
        seed: Step seed for deterministic jitter

    Returns:
        Total backoff delay in milliseconds
    """
    exponential = base_ms * (2**attempt)
    jitter = _deterministic_jitter(seed, attempt, base_ms)
    return int(exponential + jitter)


def _canonical_json(obj: Any) -> str:
    """Canonical JSON for deterministic outputs."""

    def _serializer(o: Any) -> Any:
        if hasattr(o, "to_dict"):
            return o.to_dict()
        if hasattr(o, "__dict__"):
            return {k: v for k, v in o.__dict__.items() if not k.startswith("_")}
        if isinstance(o, datetime):
            return o.isoformat()
        raise TypeError(f"Object of type {type(o).__name__} not serializable")

    return json.dumps(obj, sort_keys=True, separators=(",", ":"), default=_serializer)


def _content_hash(obj: Any) -> str:
    """Compute deterministic content hash."""
    return hashlib.sha256(_canonical_json(obj).encode()).hexdigest()


@dataclass
class StepDescriptor:
    """
    Describes a single workflow step.

    Matches the step format used by planners.
    """

    id: str
    skill_id: str
    inputs: Dict[str, Any] = field(default_factory=dict)
    depends_on: List[str] = field(default_factory=list)
    estimated_cost_cents: int = 0
    max_cost_cents: Optional[int] = None
    retry: bool = False
    max_retries: int = 3
    idempotency_key: Optional[str] = None
    on_error: str = "abort"  # abort, continue, retry
    planner_output: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "skill_id": self.skill_id,
            "inputs": self.inputs,
            "depends_on": self.depends_on,
            "estimated_cost_cents": self.estimated_cost_cents,
            "max_cost_cents": self.max_cost_cents,
            "retry": self.retry,
            "max_retries": self.max_retries,
            "idempotency_key": self.idempotency_key,
            "on_error": self.on_error,
        }


@dataclass
class WorkflowSpec:
    """
    Complete workflow specification.

    Loaded from YAML/JSON or generated by planner.
    """

    id: str
    name: str
    steps: List[StepDescriptor]
    version: str = "1.0.0"
    budget_ceiling_cents: Optional[int] = None
    timeout_seconds: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "name": self.name,
            "version": self.version,
            "steps": [s.to_dict() for s in self.steps],
            "budget_ceiling_cents": self.budget_ceiling_cents,
            "timeout_seconds": self.timeout_seconds,
            "metadata": self.metadata,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "WorkflowSpec":
        """Parse workflow spec from dictionary."""
        steps = []
        for s in data.get("steps", []):
            steps.append(
                StepDescriptor(
                    id=s["id"],
                    skill_id=s["skill_id"],
                    inputs=s.get("inputs", {}),
                    depends_on=s.get("depends_on", []),
                    estimated_cost_cents=s.get("estimated_cost_cents", 0),
                    max_cost_cents=s.get("max_cost_cents"),
                    retry=s.get("retry", False),
                    max_retries=s.get("max_retries", 3),
                    idempotency_key=s.get("idempotency_key"),
                    on_error=s.get("on_error", "abort"),
                    planner_output=s.get("planner_output"),
                )
            )
        return cls(
            id=data["id"],
            name=data.get("name", data["id"]),
            steps=steps,
            version=data.get("version", "1.0.0"),
            budget_ceiling_cents=data.get("budget_ceiling_cents"),
            timeout_seconds=data.get("timeout_seconds"),
            metadata=data.get("metadata", {}),
        )


@dataclass
class StepContext:
    """
    Context passed to each step execution.

    Contains deterministic seed, inputs, and workflow state.
    """

    workflow_id: str
    run_id: str
    step_id: str
    step_index: int
    seed: int
    inputs: Dict[str, Any]
    previous_outputs: Dict[str, Any]  # step_id -> output
    meta: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "workflow_id": self.workflow_id,
            "run_id": self.run_id,
            "step_id": self.step_id,
            "step_index": self.step_index,
            "seed": self.seed,
            "inputs": self.inputs,
        }


@dataclass
class StepResult:
    """
    Result of a single step execution.
    """

    step_id: str
    success: bool
    output: Any = None
    error: Optional[Dict[str, Any]] = None
    error_code: Optional[str] = None  # WorkflowErrorCode value
    recovery_hint: Optional[str] = None  # Recovery suggestion
    duration_ms: int = 0
    cost_cents: int = 0
    retries: int = 0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "step_id": self.step_id,
            "success": self.success,
            "output": self.output,
            "error": self.error,
            "error_code": self.error_code,
            "recovery_hint": self.recovery_hint,
            "duration_ms": self.duration_ms,
            "cost_cents": self.cost_cents,
            "retries": self.retries,
        }

    def to_canonical_json(self) -> str:
        return _canonical_json(self.to_dict())

    def content_hash(self) -> str:
        return _content_hash(self.to_dict())

    @classmethod
    def from_error(cls, step_id: str, error: WorkflowError, retries: int = 0) -> "StepResult":
        """Create StepResult from a WorkflowError."""
        return cls(
            step_id=step_id,
            success=False,
            error=error.to_dict(),
            error_code=error.code.value,
            recovery_hint=error.recovery,
            retries=retries,
        )


@dataclass
class WorkflowResult:
    """
    Final result of workflow execution.
    """

    run_id: str
    workflow_id: str
    status: str  # completed, failed, aborted, budget_exceeded
    steps_completed: int
    steps_total: int
    step_results: List[StepResult]
    total_cost_cents: int = 0
    total_duration_ms: int = 0
    error: Optional[Dict[str, Any]] = None
    checkpoint_hash: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "run_id": self.run_id,
            "workflow_id": self.workflow_id,
            "status": self.status,
            "steps_completed": self.steps_completed,
            "steps_total": self.steps_total,
            "step_results": [r.to_dict() for r in self.step_results],
            "total_cost_cents": self.total_cost_cents,
            "total_duration_ms": self.total_duration_ms,
            "error": self.error,
            "checkpoint_hash": self.checkpoint_hash,
        }


class SkillRegistry(Protocol):
    """Protocol for skill registry access."""

    def get(self, skill_id: str) -> Optional[Any]:
        ...


class WorkflowEngine:
    """
    Deterministic Workflow Engine.

    Executes multi-step workflows with:
    - Checkpoint & resume on restart
    - Per-step policy enforcement
    - Golden-file recording for replay testing
    - Deterministic seed propagation

    Vision Alignment:
    - Deterministic state (seed-based execution)
    - Replayable runs (golden-file pipeline)
    - Budget & cost contracts (per-step enforcement)
    - Zero silent failures (structured outcomes)
    """

    def __init__(
        self,
        registry: SkillRegistry,
        checkpoint_store: "CheckpointStore",
        golden: Optional["GoldenRecorder"] = None,
        policy: Optional["PolicyEnforcer"] = None,
        sandbox: Optional["PlannerSandbox"] = None,
    ):
        """
        Initialize workflow engine.

        Args:
            registry: Skill registry for looking up skills
            checkpoint_store: Store for checkpoints (resume on restart)
            golden: Optional golden-file recorder for replay testing
            policy: Optional policy enforcer for budget/rate limits
            sandbox: Optional planner sandbox for validating planner outputs
        """
        self.registry = registry
        self.checkpoint = checkpoint_store
        self.golden = golden
        self.policy = policy
        self.sandbox = sandbox

    async def run(
        self,
        spec: WorkflowSpec,
        run_id: str,
        seed: int,
        replay: bool = False,
        agent_id: Optional[str] = None,
    ) -> WorkflowResult:
        """
        Execute a workflow spec deterministically.

        Args:
            spec: The workflow specification to execute
            run_id: Unique identifier for this run (for checkpointing)
            seed: Base seed for deterministic execution
            replay: If True, running in replay mode (golden-file verification)
            agent_id: Optional agent ID for budget enforcement

        Returns:
            WorkflowResult with status, step results, and totals
        """
        base_seed = int(seed)
        steps = spec.steps
        step_results: List[StepResult] = []
        previous_outputs: Dict[str, Any] = {}
        total_cost = 0
        start_time = datetime.now(timezone.utc)

        # Load checkpoint for resume
        ck = await self.checkpoint.load(run_id)
        start_index = ck.next_step_index if ck else 0

        if ck and ck.step_outputs:
            previous_outputs = ck.step_outputs
            logger.info("workflow_resume", extra={"run_id": run_id, "from_step": start_index})

        # Capture budget snapshot for replay reproducibility
        budget_snapshot = None
        if self.policy:
            budget_snapshot = {
                "step_ceiling_cents": self.policy.step_ceiling,
                "workflow_ceiling_cents": self.policy.workflow_ceiling,
                "accumulated_cost_cents": self.policy.get_workflow_cost(run_id),
                "policy_version": "1.0.0",  # Track policy version for compatibility
            }

        # Record run start in golden
        if self.golden:
            await self.golden.record_run_start(run_id, spec, seed, replay, budget_snapshot=budget_snapshot)

        logger.info(
            "workflow_start",
            extra={
                "run_id": run_id,
                "workflow_id": spec.id,
                "steps_total": len(steps),
                "start_index": start_index,
                "seed": seed,
            },
        )

        # Record workflow start metric
        record_workflow_start(spec.id)

        final_status = "completed"
        final_error = None

        for idx in range(start_index, len(steps)):
            step = steps[idx]
            step_seed = _derive_seed(base_seed, idx)

            # Build context with previous outputs for dependency resolution
            resolved_inputs = self._resolve_inputs(step.inputs, previous_outputs)
            ctx = StepContext(
                workflow_id=spec.id,
                run_id=run_id,
                step_id=step.id,
                step_index=idx,
                seed=step_seed,
                inputs=resolved_inputs,
                previous_outputs=previous_outputs,
            )

            # Policy pre-check (budget, rate limits)
            if self.policy:
                try:
                    await self.policy.check_can_execute(step, ctx, agent_id=agent_id)
                except Exception as e:
                    workflow_error = classify_exception(e, {"step_id": step.id, "run_id": run_id})
                    error_result = StepResult.from_error(step.id, workflow_error)
                    step_results.append(error_result)
                    if self.golden:
                        await self.golden.record_step(run_id, idx, step, error_result, step_seed)

                    # Determine status from error code
                    if workflow_error.code in (
                        WorkflowErrorCode.BUDGET_EXCEEDED,
                        WorkflowErrorCode.STEP_CEILING_EXCEEDED,
                        WorkflowErrorCode.WORKFLOW_CEILING_EXCEEDED,
                        WorkflowErrorCode.AGENT_BUDGET_EXCEEDED,
                    ):
                        final_status = "budget_exceeded"
                    elif workflow_error.code == WorkflowErrorCode.EMERGENCY_STOP:
                        final_status = "emergency_stopped"
                    else:
                        final_status = "policy_violation"
                    final_error = workflow_error.to_dict()
                    # Record failure metric
                    record_step_failure(workflow_error.code.value, step.skill_id, spec.id)
                    break

            # Sandbox validation for planner outputs
            if self.sandbox and step.planner_output:
                report = await self.sandbox.validate_plan(step.planner_output)
                if not report.ok:
                    sandbox_error = WorkflowError(
                        code=WorkflowErrorCode.SANDBOX_REJECTION,
                        message=report.reason,
                        details={"violations": report.violations if hasattr(report, "violations") else []},
                        step_id=step.id,
                        run_id=run_id,
                    )
                    error_result = StepResult.from_error(step.id, sandbox_error)
                    step_results.append(error_result)
                    if self.golden:
                        await self.golden.record_step(run_id, idx, step, error_result, step_seed)

                    final_status = "sandbox_rejected"
                    final_error = sandbox_error.to_dict()
                    # Record failure metric
                    record_step_failure(sandbox_error.code.value, step.skill_id, spec.id)
                    break

            # Execute step
            step_start = datetime.now(timezone.utc)
            result = await self._execute_step(step, ctx)
            step_duration = int((datetime.now(timezone.utc) - step_start).total_seconds() * 1000)
            result.duration_ms = step_duration

            # Record step duration metric
            record_step_duration(step.skill_id, step_duration / 1000.0, result.success)

            step_results.append(result)
            total_cost += result.cost_cents

            # Record to golden
            if self.golden:
                await self.golden.record_step(run_id, idx, step, result, step_seed)

            # Save checkpoint with optimistic locking
            if result.success:
                previous_outputs[step.id] = result.output

            # Use save_with_retry for multi-worker correctness
            # This handles concurrent updates via optimistic locking
            if hasattr(self.checkpoint, "save_with_retry"):
                await self.checkpoint.save_with_retry(
                    run_id=run_id,
                    next_step_index=idx + 1,
                    last_result_hash=result.content_hash(),
                    step_outputs=previous_outputs,
                    status="running",
                    workflow_id=spec.id,
                )
            else:
                # Fallback for InMemoryCheckpointStore in tests
                ck_before_save = await self.checkpoint.load(run_id)
                expected_ver = ck_before_save.version if ck_before_save else None
                await self.checkpoint.save(
                    run_id=run_id,
                    next_step_index=idx + 1,
                    last_result_hash=result.content_hash(),
                    step_outputs=previous_outputs,
                    status="running",
                    workflow_id=spec.id,
                    expected_version=expected_ver,
                )

            # Handle failure
            if not result.success:
                # Record step failure metric
                if result.error_code:
                    record_step_failure(result.error_code, step.skill_id, spec.id)

                if step.on_error == "abort":
                    final_status = "failed"
                    final_error = result.error
                    break
                elif step.on_error == "continue":
                    continue
                # retry is handled within _execute_step

        # Calculate totals
        total_duration = int((datetime.now(timezone.utc) - start_time).total_seconds() * 1000)
        steps_completed = len([r for r in step_results if r.success])

        # Final checkpoint with optimistic locking
        if hasattr(self.checkpoint, "save_with_retry"):
            checkpoint_hash = await self.checkpoint.save_with_retry(
                run_id=run_id,
                next_step_index=len(steps),
                last_result_hash=_content_hash({"results": [r.to_dict() for r in step_results]}),
                step_outputs=previous_outputs,
                status=final_status,
                workflow_id=spec.id,
            )
        else:
            # Fallback for InMemoryCheckpointStore in tests
            ck_final = await self.checkpoint.load(run_id)
            expected_ver_final = ck_final.version if ck_final else None
            checkpoint_hash = await self.checkpoint.save(
                run_id=run_id,
                next_step_index=len(steps),
                last_result_hash=_content_hash({"results": [r.to_dict() for r in step_results]}),
                step_outputs=previous_outputs,
                status=final_status,
                workflow_id=spec.id,
                expected_version=expected_ver_final,
            )

        # Record run end in golden
        if self.golden:
            await self.golden.record_run_end(run_id, final_status)

        # Record workflow end metric
        record_workflow_end(spec.id, total_cost)

        # Record workflow failure metric if failed
        if final_status not in ("completed", "succeeded"):
            error_code = final_error.get("code", "UNKNOWN_ERROR") if final_error else "UNKNOWN_ERROR"
            record_workflow_failure(error_code, spec.id, agent_id)

        logger.info(
            "workflow_end",
            extra={
                "run_id": run_id,
                "status": final_status,
                "steps_completed": steps_completed,
                "steps_total": len(steps),
                "total_cost_cents": total_cost,
                "total_duration_ms": total_duration,
            },
        )

        return WorkflowResult(
            run_id=run_id,
            workflow_id=spec.id,
            status=final_status,
            steps_completed=steps_completed,
            steps_total=len(steps),
            step_results=step_results,
            total_cost_cents=total_cost,
            total_duration_ms=total_duration,
            error=final_error,
            checkpoint_hash=checkpoint_hash,
        )

    async def _execute_step(self, step: StepDescriptor, ctx: StepContext) -> StepResult:
        """
        Execute a single step with retry support and exponential backoff.

        Retry behavior:
        - Only retries if step.retry is True and error.is_retryable
        - Uses exponential backoff: base_ms * 2^attempt + jitter
        - Jitter is deterministic based on step seed for replay reproducibility
        """
        skill = self.registry.get(step.skill_id)
        if skill is None:
            skill_error = WorkflowError(
                code=WorkflowErrorCode.SKILL_NOT_FOUND,
                message=f"Skill not found: {step.skill_id}",
                details={"skill_id": step.skill_id},
                step_id=step.id,
                run_id=ctx.run_id,
            )
            return StepResult.from_error(step.id, skill_error)

        max_attempts = step.max_retries + 1 if step.retry else 1
        last_error: Optional[WorkflowError] = None

        for attempt in range(max_attempts):
            try:
                # Invoke skill with context
                if hasattr(skill, "invoke"):
                    result = await skill.invoke(
                        inputs=ctx.inputs, seed=ctx.seed, meta={"step_id": ctx.step_id, "attempt": attempt}
                    )
                elif hasattr(skill, "execute"):
                    result = await skill.execute(ctx.inputs)
                elif callable(skill):
                    result = await skill(ctx.inputs)
                else:
                    invalid_skill_error = WorkflowError(
                        code=WorkflowErrorCode.INVALID_SKILL,
                        message="Skill is not callable",
                        details={"skill_id": step.skill_id, "skill_type": type(skill).__name__},
                        step_id=step.id,
                        run_id=ctx.run_id,
                    )
                    return StepResult.from_error(step.id, invalid_skill_error)

                # Check if result indicates success
                if isinstance(result, dict):
                    if result.get("ok") is False or result.get("success") is False:
                        error_info = result.get("error", {})
                        # Try to extract code from result, default to STEP_FAILED
                        error_code_str = (
                            error_info.get("code", "STEP_FAILED") if isinstance(error_info, dict) else "STEP_FAILED"
                        )
                        try:
                            error_code = WorkflowErrorCode(error_code_str)
                        except ValueError:
                            error_code = WorkflowErrorCode.STEP_FAILED

                        last_error = WorkflowError(
                            code=error_code,
                            message=error_info.get("message", "Step failed")
                            if isinstance(error_info, dict)
                            else str(error_info),
                            details=error_info if isinstance(error_info, dict) else {"raw_error": error_info},
                            step_id=step.id,
                            run_id=ctx.run_id,
                        )
                        # Only retry if error is retryable and we have attempts left
                        if attempt < max_attempts - 1 and last_error.is_retryable:
                            # Exponential backoff with deterministic jitter
                            backoff_ms = _compute_backoff_ms(
                                attempt=attempt,
                                base_ms=last_error.backoff_base_ms,
                                seed=ctx.seed,
                            )
                            logger.debug(
                                "step_retry_backoff",
                                extra={
                                    "step_id": step.id,
                                    "attempt": attempt,
                                    "backoff_ms": backoff_ms,
                                    "error_code": last_error.code.value,
                                },
                            )
                            await asyncio.sleep(backoff_ms / 1000.0)
                            continue
                        return StepResult.from_error(step.id, last_error, retries=attempt)

                return StepResult(
                    step_id=step.id,
                    success=True,
                    output=result,
                    cost_cents=step.estimated_cost_cents,
                    retries=attempt,
                )

            except Exception as e:
                last_error = classify_exception(e, {"step_id": step.id, "run_id": ctx.run_id})
                # Only retry if error is retryable and we have attempts left
                if attempt < max_attempts - 1 and last_error.is_retryable:
                    # Exponential backoff with deterministic jitter
                    backoff_ms = _compute_backoff_ms(
                        attempt=attempt,
                        base_ms=last_error.backoff_base_ms,
                        seed=ctx.seed,
                    )
                    logger.debug(
                        "step_retry_backoff_exception",
                        extra={
                            "step_id": step.id,
                            "attempt": attempt,
                            "backoff_ms": backoff_ms,
                            "error_code": last_error.code.value,
                            "exception_type": type(e).__name__,
                        },
                    )
                    await asyncio.sleep(backoff_ms / 1000.0)
                    continue

        return StepResult.from_error(
            step.id,
            last_error
            or WorkflowError(
                code=WorkflowErrorCode.UNKNOWN_ERROR,
                message="Unknown error occurred",
                step_id=step.id,
                run_id=ctx.run_id,
            ),
            retries=max_attempts - 1,
        )

    def _resolve_inputs(self, inputs: Dict[str, Any], previous_outputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        Resolve input references to previous step outputs.

        Supports references like:
        - "${step_id.field}" -> previous_outputs["step_id"]["field"]
        - "${step_id}" -> previous_outputs["step_id"]
        """
        resolved = {}
        for key, value in inputs.items():
            if isinstance(value, str) and value.startswith("${") and value.endswith("}"):
                ref = value[2:-1]
                parts = ref.split(".", 1)
                step_ref = parts[0]

                if step_ref in previous_outputs:
                    output = previous_outputs[step_ref]
                    if len(parts) > 1 and isinstance(output, dict):
                        resolved[key] = output.get(parts[1], value)
                    else:
                        resolved[key] = output
                else:
                    resolved[key] = value  # Keep unresolved reference
            else:
                resolved[key] = value
        return resolved


# Import dependencies at end to avoid circular imports
from .checkpoint import CheckpointStore
from .golden import GoldenRecorder
from .planner_sandbox import PlannerSandbox
from .policies import PolicyEnforcer
