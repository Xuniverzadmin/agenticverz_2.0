# llm_invoke Skill Contract
# Version: 2.0.0
# Status: ACTIVE

skill_id: skill.llm_invoke
name: LLM Invoke
version: "2.0.0"
description: |
  Invokes language models with deterministic seeding support.
  Supports adapter pattern for multiple providers (Claude, OpenAI, etc.).
  Returns structured responses with token counts and cost tracking.

# Input Schema
inputs:
  type: object
  required:
    - prompt
  properties:
    prompt:
      description: The prompt text or messages array
      oneOf:
        - type: string
        - type: array
          items:
            type: object
            required: [role, content]
            properties:
              role:
                type: string
                enum: [system, user, assistant]
              content:
                type: string
    model:
      description: Model identifier (defaults to provider default)
      type: string
      default: null
    adapter:
      description: Adapter to use (claude, openai, stub)
      type: string
      enum: [claude, openai, stub]
      default: claude
    temperature:
      description: Sampling temperature (0.0 = deterministic)
      type: number
      minimum: 0.0
      maximum: 2.0
      default: 0.0
    max_tokens:
      description: Maximum tokens to generate
      type: integer
      minimum: 1
      maximum: 100000
      default: 1024
    seed:
      description: Random seed for deterministic output (if supported)
      type: integer
    system_prompt:
      description: System prompt (for simple prompt string input)
      type: string
    stop_sequences:
      description: Stop sequences to end generation
      type: array
      items:
        type: string
    timeout_ms:
      description: Request timeout in milliseconds
      type: integer
      default: 60000
      minimum: 1000
      maximum: 300000

# Output Schema
outputs:
  type: object
  required:
    - content
    - input_tokens
    - output_tokens
    - content_hash
  properties:
    content:
      description: Generated text content
      type: string
    content_hash:
      description: SHA256 hash of content (first 16 chars)
      type: string
    input_tokens:
      description: Number of input tokens
      type: integer
    output_tokens:
      description: Number of output tokens
      type: integer
    cost_cents:
      description: Estimated cost in cents
      type: number
    model:
      description: Actual model used
      type: string
    finish_reason:
      description: Why generation stopped (end_turn, max_tokens, stop_sequence)
      type: string
    latency_ms:
      description: Request latency (non-deterministic)
      type: integer
    seed:
      description: Seed used (if deterministic mode)
      type: integer

# Stable Fields (for determinism when seed is set)
stable_fields:
  - content_hash
  - input_tokens
  - output_tokens
  - finish_reason
  - seed

# Non-Deterministic Fields
non_deterministic_fields:
  - latency_ms
  - content  # Non-deterministic unless seed is set and model supports it

# Cost Model (per 1M tokens)
cost_model:
  claude-3-5-sonnet-20241022:
    input_per_mtok_cents: 300
    output_per_mtok_cents: 1500
  claude-3-haiku-20240307:
    input_per_mtok_cents: 25
    output_per_mtok_cents: 125
  gpt-4o:
    input_per_mtok_cents: 250
    output_per_mtok_cents: 1000
  gpt-4o-mini:
    input_per_mtok_cents: 15
    output_per_mtok_cents: 60
  stub:
    input_per_mtok_cents: 0
    output_per_mtok_cents: 0

# Constraints
constraints:
  max_prompt_tokens: 100000
  max_output_tokens: 100000
  rate_limit_rpm: 60
  rate_limit_tpm: 100000

# Retry Policy
retry_policy:
  max_retries: 3
  retryable_codes:
    - ERR_LLM_RATE_LIMITED
    - ERR_LLM_OVERLOADED
    - ERR_LLM_TIMEOUT
  non_retryable_codes:
    - ERR_LLM_INVALID_PROMPT
    - ERR_LLM_CONTENT_BLOCKED
    - ERR_LLM_AUTH_FAILED
  backoff:
    initial_delay_ms: 1000
    max_delay_ms: 30000
    multiplier: 2.0

# Error Mappings
error_mappings:
  rate_limited:
    code: ERR_LLM_RATE_LIMITED
    category: RATE_LIMIT
    retryable: true
  overloaded:
    code: ERR_LLM_OVERLOADED
    category: TRANSIENT
    retryable: true
  timeout:
    code: ERR_LLM_TIMEOUT
    category: TIMEOUT
    retryable: true
  invalid_prompt:
    code: ERR_LLM_INVALID_PROMPT
    category: VALIDATION
    retryable: false
  content_blocked:
    code: ERR_LLM_CONTENT_BLOCKED
    category: PERMANENT
    retryable: false
  auth_failed:
    code: ERR_LLM_AUTH_FAILED
    category: AUTH_FAIL
    retryable: false
  context_too_long:
    code: ERR_LLM_CONTEXT_TOO_LONG
    category: VALIDATION
    retryable: false
  invalid_model:
    code: ERR_LLM_INVALID_MODEL
    category: VALIDATION
    retryable: false

# Failure Modes
failure_modes:
  - code: ERR_LLM_RATE_LIMITED
    category: RATE_LIMIT
    retryable: true
  - code: ERR_LLM_OVERLOADED
    category: TRANSIENT
    retryable: true
  - code: ERR_LLM_TIMEOUT
    category: TIMEOUT
    retryable: true
  - code: ERR_LLM_INVALID_PROMPT
    category: VALIDATION
    retryable: false
  - code: ERR_LLM_CONTENT_BLOCKED
    category: PERMANENT
    retryable: false
  - code: ERR_LLM_AUTH_FAILED
    category: AUTH_FAIL
    retryable: false
  - code: ERR_LLM_CONTEXT_TOO_LONG
    category: VALIDATION
    retryable: false
  - code: ERR_LLM_INVALID_MODEL
    category: VALIDATION
    retryable: false

# Adapter Interface
adapter_interface:
  name: LLMAdapter
  methods:
    - name: invoke
      async: true
      params:
        - name: prompt
          type: Union[str, List[Message]]
        - name: config
          type: LLMConfig
      returns: LLMResponse
    - name: supports_seeding
      returns: bool
    - name: get_model_id
      returns: str
    - name: estimate_tokens
      params:
        - name: text
          type: str
      returns: int

# Test Cases
test_cases:
  - id: simple_prompt
    input:
      prompt: "Hello, world!"
      adapter: stub
    expected:
      ok: true
      content_hash: "<not_empty>"

  - id: deterministic_with_seed
    input:
      prompt: "What is 2+2?"
      adapter: stub
      seed: 42
      temperature: 0.0
    expected:
      ok: true
      content_hash: "<deterministic>"
      seed: 42

  - id: rate_limit_retryable
    input:
      prompt: "Test"
      adapter: stub
    mock_response:
      error_type: rate_limited
    expected_error:
      code: ERR_LLM_RATE_LIMITED
      category: RATE_LIMIT
      retryable: true
