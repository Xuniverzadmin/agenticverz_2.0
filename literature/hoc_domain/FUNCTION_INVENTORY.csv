domain,layer,file,symbol,signature,called_by,calls,imports,side_effects,async,lines,docstring
_models,L7,alert_config,AlertConfig.can_send_alert,can_send_alert(run_alert_count: int) -> bool,?:alert_emitter | L4:alert_emitter | ?:test_multi_tier_alerts,is_throttled,pydantic | sqlmodel,pure,no,3,"Check if alert can be sent (not throttled, not exceeded max)."
_models,L7,alert_config,AlertConfig.email_recipients,email_recipients() -> list[str],?:alert_emitter | L4:alert_emitter | ?:test_multi_tier_alerts,loads,pydantic | sqlmodel,pure,no,5,Get email recipients as list.
_models,L7,alert_config,AlertConfig.email_recipients,email_recipients(value: list[str]) -> None,?:alert_emitter | L4:alert_emitter | ?:test_multi_tier_alerts,dumps,pydantic | sqlmodel,pure,no,3,Set email recipients from list.
_models,L7,alert_config,AlertConfig.enabled_channels,enabled_channels() -> list[AlertChannel],?:alert_emitter | L4:alert_emitter | ?:test_multi_tier_alerts,AlertChannel | loads,pydantic | sqlmodel,pure,no,5,Get enabled channels as list.
_models,L7,alert_config,AlertConfig.enabled_channels,enabled_channels(value: list[AlertChannel]) -> None,?:alert_emitter | L4:alert_emitter | ?:test_multi_tier_alerts,dumps,pydantic | sqlmodel,pure,no,3,Set enabled channels from list.
_models,L7,alert_config,AlertConfig.is_throttled,is_throttled() -> bool,?:alert_emitter | L4:alert_emitter | ?:test_multi_tier_alerts,now | total_seconds,pydantic | sqlmodel,pure,no,6,Check if alerts are currently throttled.
_models,L7,alert_config,AlertConfig.record_alert_sent,record_alert_sent() -> None,?:alert_emitter | L4:alert_emitter | ?:test_multi_tier_alerts,now,pydantic | sqlmodel,pure,no,4,Record that an alert was sent.
_models,L7,alert_config,AlertConfig.should_alert,should_alert(current_percentage: float) -> bool,?:alert_emitter | L4:alert_emitter | ?:test_multi_tier_alerts,,pydantic | sqlmodel,pure,no,6,Check if alert should be sent based on percentage.
_models,L7,audit_ledger,generate_uuid,generate_uuid() -> str,L7:__init__ | ?:incident_write_engine | ?:overview_facade | ?:policy_proposal | ?:logs_facade | ?:policy_rules_service | ?:policy_limits_service | L5:policy_limits_engine | L5:policy_rules_engine | L5:policy_proposal_engine,str | uuid4,postgresql | sqlalchemy | sqlmodel,pure,no,3,Generate a UUID string (PIN-413).
_models,L7,audit_ledger,utc_now,utc_now() -> datetime,L7:__init__ | ?:incident_write_engine | ?:overview_facade | ?:policy_proposal | ?:logs_facade | ?:policy_rules_service | ?:policy_limits_service | L5:policy_limits_engine | L5:policy_rules_engine | L5:policy_proposal_engine,now,postgresql | sqlalchemy | sqlmodel,pure,no,3,Return current UTC time (PIN-413).
_models,L7,contract,ContractCreate.validate_confidence,validate_confidence(v: Decimal) -> Decimal,?:founder_contract_review | L7:__init__ | ?:founder_review | ?:rollout_projection | ?:contract_service | ?:audit_service | ?:governance_orchestrator | ?:llm_invoke_v2 | ?:json_transform_v2 | L3:claude_adapter,ValueError | field_validator,costsim_cb | postgresql | pydantic | sqlalchemy,pure,no,5,"CONTRACT-007: confidence_score range [0,1]."
_models,L7,contract,ContractImmutableError.__init__,"__init__(contract_id: UUID, status: ContractStatus)",?:founder_contract_review | L7:__init__ | ?:founder_review | ?:rollout_projection | ?:contract_service | ?:audit_service | ?:governance_orchestrator | ?:llm_invoke_v2 | ?:json_transform_v2 | L3:claude_adapter,__init__ | super,costsim_cb | postgresql | pydantic | sqlalchemy,pure,no,4,
_models,L7,contract,InvalidTransitionError.__init__,"__init__(from_status: ContractStatus, to_status: ContractStatus, reason: str)",?:founder_contract_review | L7:__init__ | ?:founder_review | ?:rollout_projection | ?:contract_service | ?:audit_service | ?:governance_orchestrator | ?:llm_invoke_v2 | ?:json_transform_v2 | L3:claude_adapter,__init__ | super,costsim_cb | postgresql | pydantic | sqlalchemy,pure,no,10,
_models,L7,contract,MayNotVerdictError.__init__,__init__(reason: str),?:founder_contract_review | L7:__init__ | ?:founder_review | ?:rollout_projection | ?:contract_service | ?:audit_service | ?:governance_orchestrator | ?:llm_invoke_v2 | ?:json_transform_v2 | L3:claude_adapter,__init__ | super,costsim_cb | postgresql | pydantic | sqlalchemy,pure,no,3,
_models,L7,costsim_cb,CostSimAlertQueueModel.to_dict,"to_dict() -> Dict[str, Any]",L7:governance | L7:external_response | L7:__init__ | L7:execution_envelope | L7:contract | L7:governance_job | ?:alert_worker | ?:provenance_async | ?:circuit_breaker_async | L4:alert_driver,isoformat,orm | postgresql | sqlalchemy,pure,no,14,Convert to dictionary.
_models,L7,costsim_cb,CostSimCBIncidentModel.get_details,"get_details() -> Dict[str, Any]",L7:governance | L7:external_response | L7:__init__ | L7:execution_envelope | L7:contract | L7:governance_job | ?:alert_worker | ?:provenance_async | ?:circuit_breaker_async | L4:alert_driver,loads,orm | postgresql | sqlalchemy,pure,no,5,Parse details JSON.
_models,L7,costsim_cb,CostSimCBIncidentModel.to_dict,"to_dict() -> Dict[str, Any]",L7:governance | L7:external_response | L7:__init__ | L7:execution_envelope | L7:contract | L7:governance_job | ?:alert_worker | ?:provenance_async | ?:circuit_breaker_async | L4:alert_driver,get_details | isoformat,orm | postgresql | sqlalchemy,pure,no,18,Convert to dictionary.
_models,L7,costsim_cb,CostSimCBStateModel.to_dict,"to_dict() -> Dict[str, Any]",L7:governance | L7:external_response | L7:__init__ | L7:execution_envelope | L7:contract | L7:governance_job | ?:alert_worker | ?:provenance_async | ?:circuit_breaker_async | L4:alert_driver,isoformat,orm | postgresql | sqlalchemy,pure,no,14,Convert to dictionary.
_models,L7,costsim_cb,CostSimCanaryReportModel.get_artifact_paths,get_artifact_paths() -> list,L7:governance | L7:external_response | L7:__init__ | L7:execution_envelope | L7:contract | L7:governance_job | ?:alert_worker | ?:provenance_async | ?:circuit_breaker_async | L4:alert_driver,loads,orm | postgresql | sqlalchemy,pure,no,5,Parse artifact paths JSON.
_models,L7,costsim_cb,CostSimCanaryReportModel.get_failure_reasons,get_failure_reasons() -> list,L7:governance | L7:external_response | L7:__init__ | L7:execution_envelope | L7:contract | L7:governance_job | ?:alert_worker | ?:provenance_async | ?:circuit_breaker_async | L4:alert_driver,loads,orm | postgresql | sqlalchemy,pure,no,5,Parse failure reasons JSON.
_models,L7,costsim_cb,CostSimCanaryReportModel.to_dict,"to_dict() -> Dict[str, Any]",L7:governance | L7:external_response | L7:__init__ | L7:execution_envelope | L7:contract | L7:governance_job | ?:alert_worker | ?:provenance_async | ?:circuit_breaker_async | L4:alert_driver,get_artifact_paths | get_failure_reasons | isoformat,orm | postgresql | sqlalchemy,pure,no,20,Convert to dictionary.
_models,L7,costsim_cb,CostSimProvenanceModel.to_dict,"to_dict() -> Dict[str, Any]",L7:governance | L7:external_response | L7:__init__ | L7:execution_envelope | L7:contract | L7:governance_job | ?:alert_worker | ?:provenance_async | ?:circuit_breaker_async | L4:alert_driver,isoformat,orm | postgresql | sqlalchemy,pure,no,18,Convert to dictionary.
_models,L7,cus_models,CusIntegration.disable,disable() -> None,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,utc_now,postgresql | sqlalchemy | sqlmodel,pure,no,8,Disable this integration (user-initiated pause).
_models,L7,cus_models,CusIntegration.enable,enable() -> None,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,ValueError | utc_now,postgresql | sqlalchemy | sqlmodel,pure,no,10,Enable this integration for use.
_models,L7,cus_models,CusIntegration.has_budget_limit,has_budget_limit() -> bool,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Check if budget limit is configured.
_models,L7,cus_models,CusIntegration.has_rate_limit,has_rate_limit() -> bool,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Check if rate limit is configured.
_models,L7,cus_models,CusIntegration.has_token_limit,has_token_limit() -> bool,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Check if token limit is configured.
_models,L7,cus_models,CusIntegration.is_deleted,is_deleted() -> bool,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Check if integration is soft-deleted.
_models,L7,cus_models,CusIntegration.is_usable,is_usable() -> bool,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,7,Check if integration can be used for LLM calls.
_models,L7,cus_models,CusIntegration.mark_error,mark_error(message: str) -> None,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,utc_now,postgresql | sqlalchemy | sqlmodel,pure,no,11,Mark integration as errored (system-detected issue).
_models,L7,cus_models,CusIntegration.update_health,"update_health(state: CusHealthState, message: Optional[str]) -> None",?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,utc_now,postgresql | sqlalchemy | sqlmodel,pure,no,10,Update health state from health check result.
_models,L7,cus_models,CusLLMUsage.is_error,is_error() -> bool,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Check if this call resulted in an error.
_models,L7,cus_models,CusLLMUsage.total_tokens,total_tokens() -> int,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Total tokens (input + output).
_models,L7,cus_models,CusLLMUsage.was_blocked,was_blocked() -> bool,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Check if this call was blocked by policy.
_models,L7,cus_models,CusUsageDaily.cost_dollars,cost_dollars() -> float,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Total cost in dollars.
_models,L7,cus_models,CusUsageDaily.success_rate,success_rate() -> float,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,6,"Percentage of successful calls (non-error, non-blocked)."
_models,L7,cus_models,CusUsageDaily.total_tokens,total_tokens() -> int,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Total tokens (input + output).
_models,L7,cus_models,generate_uuid,generate_uuid() -> str,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,str | uuid4,postgresql | sqlalchemy | sqlmodel,pure,no,3,Generate a new UUID string.
_models,L7,cus_models,utc_now,utc_now() -> datetime,?:cus_schemas | ?:cus_health_driver | ?:cus_health_engine | ?:cus_enforcement_driver | ?:cus_integration_engine | ?:cus_integration_driver | ?:cus_telemetry_driver | L5s:cus_schemas | L5:cus_health_engine | ?:cus_usage_aggregator,now,postgresql | sqlalchemy | sqlmodel,pure,no,3,Return current UTC time with timezone info.
_models,L7,execution_envelope,ExecutionEnvelopeModel.to_dict,"to_dict() -> Dict[str, Any]",?:evidence_sink | ?:invocation_context | ?:kernel | L5:kernel | ?:test_execution_envelope,isoformat,costsim_cb | postgresql | sqlalchemy,pure,no,50,Convert to dictionary.
_models,L7,execution_envelope,ExecutionEnvelopeStats.to_dict,"to_dict() -> Dict[str, Any]",?:evidence_sink | ?:invocation_context | ?:kernel | L5:kernel | ?:test_execution_envelope,isoformat,costsim_cb | postgresql | sqlalchemy,pure,no,15,Convert to dictionary.
_models,L7,export_bundles,generate_bundle_id,generate_bundle_id(prefix: str) -> str,L7:__init__ | ?:export_bundle_service | ?:pdf_renderer | L3:export_bundle_adapter | L5:pdf_renderer | L6:export_bundle_driver | ?:test_export_scope_resolution,uuid4,pydantic,pure,no,3,Generate unique bundle ID.
_models,L7,export_bundles,utc_now,utc_now() -> datetime,L7:__init__ | ?:export_bundle_service | ?:pdf_renderer | L3:export_bundle_adapter | L5:pdf_renderer | L6:export_bundle_driver | ?:test_export_scope_resolution,now,pydantic,pure,no,3,Return timezone-aware UTC datetime.
_models,L7,governance_job,InvalidJobTransitionError.__init__,"__init__(from_status: JobStatus, to_status: JobStatus, reason: str)",L7:__init__ | ?:job_executor | ?:governance_orchestrator | L4:job_executor | L4:governance_orchestrator | ?:test_orchestrator_invariants | ?:test_executor_invariants,__init__ | super,costsim_cb | postgresql | pydantic | sqlalchemy,pure,no,10,
_models,L7,governance_job,JobImmutableError.__init__,"__init__(job_id: UUID, status: JobStatus)",L7:__init__ | ?:job_executor | ?:governance_orchestrator | L4:job_executor | L4:governance_orchestrator | ?:test_orchestrator_invariants | ?:test_executor_invariants,__init__ | super,costsim_cb | postgresql | pydantic | sqlalchemy,pure,no,4,
_models,L7,governance_job,OrphanJobError.__init__,__init__(reason: str),L7:__init__ | ?:job_executor | ?:governance_orchestrator | L4:job_executor | L4:governance_orchestrator | ?:test_orchestrator_invariants | ?:test_executor_invariants,__init__ | super,costsim_cb | postgresql | pydantic | sqlalchemy,pure,no,3,
_models,L7,killswitch,DefaultGuardrail.evaluate,"evaluate(context: Dict[str, Any]) -> tuple[bool, Optional[str]]",?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,get | get_rule_config | lower,pydantic | sqlmodel,pure,no,39,Evaluate this guardrail against context.
_models,L7,killswitch,DefaultGuardrail.get_rule_config,"get_rule_config() -> Dict[str, Any]",?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,loads,pydantic | sqlmodel,pure,no,3,Get rule configuration from JSON.
_models,L7,killswitch,Incident.acknowledge,acknowledge(by: str),?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,utc_now,pydantic | sqlmodel,pure,no,5,Mark incident as acknowledged (PIN-412).
_models,L7,killswitch,Incident.add_related_call,add_related_call(call_id: str),?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,append | dumps | get_related_call_ids | len,pydantic | sqlmodel,pure,no,7,Add a related call ID.
_models,L7,killswitch,Incident.get_inflection_context,get_inflection_context() -> dict,?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,loads,pydantic | sqlmodel,pure,no,5,Get inflection context from JSON (GAP-024).
_models,L7,killswitch,Incident.get_related_call_ids,get_related_call_ids() -> List[str],?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,loads,pydantic | sqlmodel,pure,no,5,Get list of related call IDs.
_models,L7,killswitch,Incident.resolve,"resolve(by: str, resolution_method: Optional[str])",?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,int | total_seconds | utc_now,pydantic | sqlmodel,pure,no,17,Mark incident as resolved.
_models,L7,killswitch,Incident.set_inflection_context,set_inflection_context(context: dict) -> None,?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,dumps,pydantic | sqlmodel,pure,no,3,Set inflection context as JSON (GAP-024).
_models,L7,killswitch,Incident.set_inflection_point,"set_inflection_point(step_index: Optional[int], timestamp: Optional[datetime], context: Optional[dict]) -> None",?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,set_inflection_context | utc_now,pydantic | sqlmodel,pure,no,23,Set inflection point metadata (GAP-024).
_models,L7,killswitch,IncidentEvent.get_data,"get_data() -> Dict[str, Any]",?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,loads,pydantic | sqlmodel,pure,no,5,Get event data from JSON.
_models,L7,killswitch,IncidentEvent.set_data,"set_data(data: Dict[str, Any])",?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,dumps,pydantic | sqlmodel,pure,no,3,Set event data as JSON.
_models,L7,killswitch,KillSwitchState.freeze,"freeze(by: str, reason: str, auto: bool, trigger: Optional[str])",?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,utc_now,pydantic | sqlmodel,pure,no,11,Freeze this entity.
_models,L7,killswitch,KillSwitchState.unfreeze,unfreeze(by: str),?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,utc_now,pydantic | sqlmodel,pure,no,6,Unfreeze this entity.
_models,L7,killswitch,ProxyCall.get_policy_decisions,"get_policy_decisions() -> List[Dict[str, Any]]",?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,loads,pydantic | sqlmodel,pure,no,5,Get policy decisions from JSON.
_models,L7,killswitch,ProxyCall.hash_request,hash_request(request_body: dict) -> str,?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,dumps | encode | hexdigest | sha256,pydantic | sqlmodel,pure,no,4,Generate deterministic hash of request for matching.
_models,L7,killswitch,ProxyCall.hash_response,hash_response(response_body: dict) -> str,?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,dumps | encode | hexdigest | sha256,pydantic | sqlmodel,pure,no,4,Generate deterministic hash of response.
_models,L7,killswitch,ProxyCall.set_policy_decisions,"set_policy_decisions(decisions: List[Dict[str, Any]])",?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,dumps,pydantic | sqlmodel,pure,no,3,Set policy decisions as JSON.
_models,L7,killswitch,generate_uuid,generate_uuid() -> str,?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,str | uuid4,pydantic | sqlmodel,pure,no,2,
_models,L7,killswitch,utc_now,utc_now() -> datetime,?:manager | ?:__init__ | ?:incidents | ?:guard | ?:v1_proxy | ?:ops | ?:v1_killswitch | ?:replay | L3:customer_killswitch_adapter | ?:incident_write_engine,now,pydantic | sqlmodel,pure,no,2,
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.allows_modifications,allows_modifications() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,is_onboarding | is_operational,,pure,no,3,Check if the knowledge plane configuration can be modified.
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.allows_new_runs,allows_new_runs() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,3,Check if new runs can use this knowledge plane.
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.allows_policy_binding,allows_policy_binding() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,6,Check if policies can be bound to this knowledge plane.
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.allows_queries,allows_queries() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,3,Check if queries against this knowledge plane are allowed.
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.is_failed,is_failed() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,3,Check if state is a failure state.
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.is_offboarding,is_offboarding() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,3,Check if state is in offboarding phase.
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.is_onboarding,is_onboarding() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,3,Check if state is in onboarding phase.
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.is_operational,is_operational() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,3,Check if state is operational (ACTIVE).
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.is_terminal,is_terminal() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,3,Check if state is terminal (no further transitions except FAILED).
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.requires_async_job,requires_async_job() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,7,Check if this state involves an async background job.
_models,L7,knowledge_lifecycle,KnowledgePlaneLifecycleState.requires_policy_gate,requires_policy_gate() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,7,Check if transition FROM this state requires policy gate (GAP-087).
_models,L7,knowledge_lifecycle,TransitionResult.__bool__,__bool__() -> bool,?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,,,pure,no,2,
_models,L7,knowledge_lifecycle,get_action_for_transition,"get_action_for_transition(from_state: KnowledgePlaneLifecycleState, to_state: KnowledgePlaneLifecycleState) -> Optional[str]",?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,items,,pure,no,9,"Get the action name for a given transition, if valid."
_models,L7,knowledge_lifecycle,get_next_offboarding_state,get_next_offboarding_state(current: KnowledgePlaneLifecycleState) -> Optional[KnowledgePlaneLifecycleState],?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,index | len,,pure,no,18,"Get the next state in the offboarding path, if applicable."
_models,L7,knowledge_lifecycle,get_next_onboarding_state,get_next_onboarding_state(current: KnowledgePlaneLifecycleState) -> Optional[KnowledgePlaneLifecycleState],?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,index | len,,pure,no,21,"Get the next state in the onboarding path, if applicable."
_models,L7,knowledge_lifecycle,get_transition_for_action,"get_transition_for_action(action: str, current_state: KnowledgePlaneLifecycleState) -> Optional[KnowledgePlaneLifecycleState]",?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,get_next_onboarding_state,,pure,no,18,"Get the target state for an action from current state, if valid."
_models,L7,knowledge_lifecycle,get_valid_transitions,get_valid_transitions(from_state: KnowledgePlaneLifecycleState) -> Set[KnowledgePlaneLifecycleState],?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,copy | get | set,,pure,no,5,Get all valid target states from a given state.
_models,L7,knowledge_lifecycle,is_valid_transition,"is_valid_transition(from_state: KnowledgePlaneLifecycleState, to_state: KnowledgePlaneLifecycleState) -> bool",?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,get | set,,pure,no,12,Check if a lifecycle transition is valid.
_models,L7,knowledge_lifecycle,validate_transition,"validate_transition(from_state: KnowledgePlaneLifecycleState, to_state: KnowledgePlaneLifecycleState) -> TransitionResult",?:lifecycle_worker | ?:knowledge_lifecycle_manager | ?:knowledge_sdk | ?:onboarding | ?:offboarding | ?:base | L4:lifecycle_stages_base | L4:onboarding | L4:offboarding | ?:test_async_job_coordination,TransitionResult | get_valid_transitions | is_valid_transition | requires_async_job | requires_policy_gate,,pure,no,40,Validate a lifecycle transition and return detailed result.
_models,L7,log_exports,generate_uuid,generate_uuid() -> str,?:logs_facade | L6:logs_domain_store,str | uuid4,sqlmodel,pure,no,3,Generate a UUID string.
_models,L7,log_exports,utc_now,utc_now() -> datetime,?:logs_facade | L6:logs_domain_store,now,sqlmodel,pure,no,3,Return current UTC time.
_models,L7,logs_records,generate_uuid,generate_uuid() -> str,?:pool | ?:runner | L7:__init__ | ?:logs_facade | ?:main | L6:logs_domain_store,str | uuid4,postgresql | sqlalchemy | sqlmodel,pure,no,3,Generate a UUID string.
_models,L7,logs_records,utc_now,utc_now() -> datetime,?:pool | ?:runner | L7:__init__ | ?:logs_facade | ?:main | L6:logs_domain_store,now,postgresql | sqlalchemy | sqlmodel,pure,no,3,Return current UTC time.
_models,L7,m10_recovery,SuggestionAction.matches_error,matches_error(error_code: str) -> bool,?:test_m10_recovery_enhanced,any | startswith | upper,orm | postgresql | sql | sqlalchemy,pure,no,6,Check if action applies to given error code.
_models,L7,m10_recovery,SuggestionAction.matches_skill,matches_skill(skill_id: str) -> bool,?:test_m10_recovery_enhanced,,orm | postgresql | sql | sqlalchemy,pure,no,6,Check if action applies to given skill.
_models,L7,m10_recovery,SuggestionAction.to_dict,"to_dict() -> Dict[str, Any]",?:test_m10_recovery_enhanced,isoformat,orm | postgresql | sql | sqlalchemy,pure,no,24,Serialize to dictionary.
_models,L7,m10_recovery,SuggestionInput.to_dict,"to_dict() -> Dict[str, Any]",?:test_m10_recovery_enhanced,isoformat,orm | postgresql | sql | sqlalchemy,pure,no,14,Serialize to dictionary.
_models,L7,m10_recovery,SuggestionProvenance.to_dict,"to_dict() -> Dict[str, Any]",?:test_m10_recovery_enhanced,isoformat,orm | postgresql | sql | sqlalchemy,pure,no,16,Serialize to dictionary.
_models,L7,monitor_config,MonitorConfig.allowed_rag_sources,allowed_rag_sources() -> list[str],?:test_monitor_enhancements | ?:test_inspection_constraints,loads,pydantic | sqlmodel,pure,no,5,Get allowed RAG sources as list.
_models,L7,monitor_config,MonitorConfig.allowed_rag_sources,allowed_rag_sources(value: list[str]) -> None,?:test_monitor_enhancements | ?:test_inspection_constraints,dumps,pydantic | sqlmodel,pure,no,3,Set allowed RAG sources from list.
_models,L7,monitor_config,MonitorConfig.enabled_metrics,enabled_metrics() -> list[MonitorMetric],?:test_monitor_enhancements | ?:test_inspection_constraints,append,pydantic | sqlmodel,pure,no,18,Get list of enabled monitor metrics.
_models,L7,monitor_config,MonitorConfig.is_metric_monitored,is_metric_monitored(metric: MonitorMetric) -> bool,?:test_monitor_enhancements | ?:test_inspection_constraints,,pydantic | sqlmodel,pure,no,3,Check if a specific metric is being monitored.
_models,L7,monitor_config,MonitorConfig.to_snapshot,to_snapshot() -> dict,?:test_monitor_enhancements | ?:test_inspection_constraints,,pydantic | sqlmodel,pure,no,14,Convert to snapshot dict for immutable storage.
_models,L7,override_authority,OverrideAuthority.allowed_roles,allowed_roles() -> list[str],?:test_override_authority,loads,pydantic | sqlmodel,pure,no,5,Get allowed roles as list.
_models,L7,override_authority,OverrideAuthority.allowed_roles,allowed_roles(value: list[str]) -> None,?:test_override_authority,dumps,pydantic | sqlmodel,pure,no,3,Set allowed roles from list.
_models,L7,override_authority,OverrideAuthority.apply_override,"apply_override(user_id: str, user_role: str, reason: str, duration_seconds: Optional[int]) -> tuple[bool, str]",?:test_override_authority,can_override | date | now | replace,pydantic | sqlmodel,pure,no,36,Apply an override to the policy.
_models,L7,override_authority,OverrideAuthority.can_override,"can_override(user_role: str) -> tuple[bool, str]",?:test_override_authority,,pydantic | sqlmodel,pure,no,17,Check if user with given role can override.
_models,L7,override_authority,OverrideAuthority.clear_override,clear_override() -> None,?:test_override_authority,now,pydantic | sqlmodel,pure,no,8,Clear the current override.
_models,L7,override_authority,OverrideAuthority.is_override_active,is_override_active() -> bool,?:test_override_authority,now,pydantic | sqlmodel,pure,no,7,Check if an override is currently active.
_models,L7,override_authority,OverrideAuthority.reset_daily_count,reset_daily_count() -> None,?:test_override_authority,,pydantic | sqlmodel,pure,no,3,Reset the daily override count (called by scheduler).
_models,L7,override_authority,OverrideRecord.create_record,"create_record(policy_id: str, tenant_id: str, override_by: str, override_role: str, reason: str, duration_seconds: int, run_id: Optional[str]) -> 'OverrideRecord'",?:test_override_authority,cls | now | replace,pydantic | sqlmodel,pure,no,23,Create an override record.
_models,L7,policy_control_plane,PolicyRule.retire,"retire(by: str, reason: str, superseded_by_id: Optional[str]) -> None",?:policy | ?:policy_limits_crud | L7:__init__ | ?:llm_threshold_service | ?:policies_facade | ?:cross_domain | ?:overview_facade | ?:policy_rules_service | ?:policy_limits_service | ?:simulation_driver,utc_now,postgresql | sqlalchemy | sqlmodel,pure,no,8,Retire this rule (PIN-412).
_models,L7,policy_control_plane,generate_uuid,generate_uuid() -> str,?:policy | ?:policy_limits_crud | L7:__init__ | ?:llm_threshold_service | ?:policies_facade | ?:cross_domain | ?:overview_facade | ?:policy_rules_service | ?:policy_limits_service | ?:simulation_driver,str | uuid4,postgresql | sqlalchemy | sqlmodel,pure,no,3,Generate a UUID string (PIN-412).
_models,L7,policy_control_plane,utc_now,utc_now() -> datetime,?:policy | ?:policy_limits_crud | L7:__init__ | ?:llm_threshold_service | ?:policies_facade | ?:cross_domain | ?:overview_facade | ?:policy_rules_service | ?:policy_limits_service | ?:simulation_driver,now,postgresql | sqlalchemy | sqlmodel,pure,no,3,Return current UTC time (PIN-412).
_models,L7,policy_precedence,PolicyPrecedence.to_snapshot,to_snapshot() -> dict,?:arbitrator | L6:arbitrator | ?:test_control_action_enhancements,,pydantic | sqlmodel,pure,no,9,Convert to snapshot dict for immutable storage.
_models,L7,policy_scope,PolicyScope.agent_ids,agent_ids() -> list[str],?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,loads,pydantic | sqlmodel,pure,no,5,Get agent IDs as list.
_models,L7,policy_scope,PolicyScope.agent_ids,agent_ids(value: list[str]) -> None,?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,dumps,pydantic | sqlmodel,pure,no,3,Set agent IDs from list.
_models,L7,policy_scope,PolicyScope.api_key_ids,api_key_ids() -> list[str],?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,loads,pydantic | sqlmodel,pure,no,5,Get API key IDs as list.
_models,L7,policy_scope,PolicyScope.api_key_ids,api_key_ids(value: list[str]) -> None,?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,dumps,pydantic | sqlmodel,pure,no,3,Set API key IDs from list.
_models,L7,policy_scope,PolicyScope.create_agent_scope,"create_agent_scope(policy_id: str, tenant_id: str, agent_ids: list[str], created_by: Optional[str]) -> 'PolicyScope'",?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,cls,pydantic | sqlmodel,pure,no,16,Factory method for AGENT scope.
_models,L7,policy_scope,PolicyScope.create_all_runs_scope,"create_all_runs_scope(policy_id: str, tenant_id: str, created_by: Optional[str]) -> 'PolicyScope'",?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,cls,pydantic | sqlmodel,pure,no,13,Factory method for ALL_RUNS scope.
_models,L7,policy_scope,PolicyScope.create_api_key_scope,"create_api_key_scope(policy_id: str, tenant_id: str, api_key_ids: list[str], created_by: Optional[str]) -> 'PolicyScope'",?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,cls,pydantic | sqlmodel,pure,no,16,Factory method for API_KEY scope.
_models,L7,policy_scope,PolicyScope.create_human_actor_scope,"create_human_actor_scope(policy_id: str, tenant_id: str, human_actor_ids: list[str], created_by: Optional[str]) -> 'PolicyScope'",?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,cls,pydantic | sqlmodel,pure,no,16,Factory method for HUMAN_ACTOR scope.
_models,L7,policy_scope,PolicyScope.human_actor_ids,human_actor_ids() -> list[str],?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,loads,pydantic | sqlmodel,pure,no,5,Get human actor IDs as list.
_models,L7,policy_scope,PolicyScope.human_actor_ids,human_actor_ids(value: list[str]) -> None,?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,dumps,pydantic | sqlmodel,pure,no,3,Set human actor IDs from list.
_models,L7,policy_scope,PolicyScope.matches,"matches(agent_id: Optional[str], api_key_id: Optional[str], human_actor_id: Optional[str]) -> bool",?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,ScopeType,pydantic | sqlmodel,pure,no,32,Check if this scope matches the given run context.
_models,L7,policy_scope,PolicyScope.to_snapshot,to_snapshot() -> dict,?:scope_resolver | L6:scope_resolver | ?:test_export_scope_resolution | ?:test_scope_selector,,pydantic | sqlmodel,pure,no,9,Convert to snapshot dict for immutable storage.
_models,L7,policy_snapshot,PolicySnapshot.create_snapshot,"create_snapshot(tenant_id: str, policies: list[dict[str, Any]], thresholds: dict[str, Any], policy_version: Optional[str]) -> 'PolicySnapshot'",?:prevention_engine | L7:__init__ | L5:prevention_engine | ?:test_threshold_snapshot_hash,cls | dumps | encode | hexdigest | len | sha256,pydantic | sqlmodel,pure,no,43,Create immutable snapshot with content hash.
_models,L7,policy_snapshot,PolicySnapshot.get_policies,"get_policies() -> list[dict[str, Any]]",?:prevention_engine | L7:__init__ | L5:prevention_engine | ?:test_threshold_snapshot_hash,loads,pydantic | sqlmodel,pure,no,3,Deserialize policies from JSON.
_models,L7,policy_snapshot,PolicySnapshot.get_threshold_hash,get_threshold_hash() -> str,?:prevention_engine | L7:__init__ | L5:prevention_engine | ?:test_threshold_snapshot_hash,encode | hexdigest | sha256,pydantic | sqlmodel,pure,no,6,Get or compute threshold hash (GAP-022).
_models,L7,policy_snapshot,PolicySnapshot.get_thresholds,"get_thresholds() -> dict[str, Any]",?:prevention_engine | L7:__init__ | L5:prevention_engine | ?:test_threshold_snapshot_hash,loads,pydantic | sqlmodel,pure,no,3,Deserialize thresholds from JSON.
_models,L7,policy_snapshot,PolicySnapshot.verify_integrity,verify_integrity() -> bool,?:prevention_engine | L7:__init__ | L5:prevention_engine | ?:test_threshold_snapshot_hash,encode | hexdigest | sha256,pydantic | sqlmodel,pure,no,5,Verify content hash matches stored data.
_models,L7,policy_snapshot,PolicySnapshot.verify_threshold_integrity,verify_threshold_integrity() -> bool,?:prevention_engine | L7:__init__ | L5:prevention_engine | ?:test_threshold_snapshot_hash,encode | hexdigest | sha256,pydantic | sqlmodel,pure,no,6,Verify threshold hash matches stored threshold data (GAP-022).
_models,L7,policy_snapshot,utc_now,utc_now() -> datetime,?:prevention_engine | L7:__init__ | L5:prevention_engine | ?:test_threshold_snapshot_hash,now,pydantic | sqlmodel,pure,no,3,Return timezone-aware UTC datetime.
_models,L7,retrieval_evidence,RetrievalEvidence.doc_count,doc_count() -> int,?:test_retrieval_evidence,len,postgresql | sqlalchemy | sqlmodel,pure,no,3,Number of documents returned.
_models,L7,retrieval_evidence,RetrievalEvidence.is_complete,is_complete() -> bool,?:test_retrieval_evidence,,postgresql | sqlalchemy | sqlmodel,pure,no,3,Check if the retrieval has completed.
_models,L7,retrieval_evidence,generate_uuid,generate_uuid() -> str,?:test_retrieval_evidence,str | uuid4,postgresql | sqlalchemy | sqlmodel,pure,no,3,Generate a new UUID string.
_models,L7,retrieval_evidence,utc_now,utc_now() -> datetime,?:test_retrieval_evidence,now,postgresql | sqlalchemy | sqlmodel,pure,no,3,Return current UTC time with timezone info.
_models,L7,run_lifecycle,get_lifecycle_state,get_lifecycle_state(status: RunStatus) -> RunLifecycleState,L7:__init__,,pydantic,pure,no,4,Map run status to lifecycle state.
_models,L7,tenant,APIKey.generate_key,"generate_key() -> tuple[str, str, str]",?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,encode | hexdigest | sha256 | token_urlsafe,onboarding_state | sqlmodel | tier_gating,pure,no,8,"Generate a new API key. Returns (full_key, prefix, hash)."
_models,L7,tenant,APIKey.hash_key,hash_key(key: str) -> str,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,encode | hexdigest | sha256,onboarding_state | sqlmodel | tier_gating,pure,no,3,Hash a key for comparison.
_models,L7,tenant,APIKey.is_valid,is_valid() -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,utc_now,onboarding_state | sqlmodel | tier_gating,pure,no,7,"Check if key is valid (not revoked, not expired)."
_models,L7,tenant,APIKey.record_usage,record_usage(),?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,utc_now,onboarding_state | sqlmodel | tier_gating,pure,no,4,Record that this key was used.
_models,L7,tenant,FounderAction.is_reversal,is_reversal() -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,3,Check if this action is a reversal action.
_models,L7,tenant,Invitation.generate_token,"generate_token() -> tuple[str, str]",?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,encode | hexdigest | sha256 | token_urlsafe,onboarding_state | sqlmodel | tier_gating,pure,no,5,"Generate invitation token. Returns (token, hash)."
_models,L7,tenant,Invitation.is_valid,is_valid() -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,utc_now,onboarding_state | sqlmodel | tier_gating,pure,no,7,Check if invitation is valid (pending and not expired).
_models,L7,tenant,Tenant.can_access_endpoint,can_access_endpoint(required_state: int) -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,7,Check if tenant's onboarding state allows access to an endpoint.
_models,L7,tenant,Tenant.can_create_run,"can_create_run() -> tuple[bool, str]",?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,7,Check if tenant can create a new run.
_models,L7,tenant,Tenant.can_use_tokens,"can_use_tokens(tokens: int) -> tuple[bool, str]",?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,5,Check if tenant can use given tokens.
_models,L7,tenant,Tenant.get_available_features,get_available_features() -> list[str],?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,get_tier_features,onboarding_state | sqlmodel | tier_gating,pure,no,5,Get list of all features available to this tenant.
_models,L7,tenant,Tenant.has_completed_onboarding,has_completed_onboarding() -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,5,Check if tenant has completed onboarding.
_models,L7,tenant,Tenant.has_feature,has_feature(feature: str) -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,check_tier_access,onboarding_state | sqlmodel | tier_gating,pure,no,10,Check if tenant has access to a feature based on their tier.
_models,L7,tenant,Tenant.increment_usage,increment_usage(tokens: int),?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,utc_now,onboarding_state | sqlmodel | tier_gating,pure,no,6,Increment usage counters.
_models,L7,tenant,Tenant.onboarding,onboarding() -> 'OnboardingState',?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,OnboardingState,onboarding_state | sqlmodel | tier_gating,pure,no,10,Get the tenant's onboarding state as an enum.
_models,L7,tenant,Tenant.retention_days,retention_days() -> int,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,get,onboarding_state | sqlmodel | tier_gating,pure,no,5,Get log retention days based on tier.
_models,L7,tenant,Tenant.tier,tier() -> 'TenantTier',?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,resolve_tier,onboarding_state | sqlmodel | tier_gating,pure,no,10,Get the tenant's tier from their plan.
_models,L7,tenant,Tenant.tier_marketing_name,tier_marketing_name() -> str,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,get,onboarding_state | sqlmodel | tier_gating,pure,no,5,Get the marketing name for the tenant's tier.
_models,L7,tenant,TenantMembership.can_change_roles,can_change_roles() -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,3,Check if this member can change other users' roles.
_models,L7,tenant,TenantMembership.can_manage_keys,can_manage_keys() -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,2,
_models,L7,tenant,TenantMembership.can_manage_users,can_manage_users() -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,3,Check if this member can invite/manage other users.
_models,L7,tenant,TenantMembership.can_run_workers,can_run_workers() -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,2,
_models,L7,tenant,TenantMembership.can_view_runs,can_view_runs() -> bool,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,,onboarding_state | sqlmodel | tier_gating,pure,no,2,
_models,L7,tenant,User.get_preferences,get_preferences() -> dict,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,loads,onboarding_state | sqlmodel | tier_gating,pure,no,9,"Parse preferences JSON, return empty dict if None."
_models,L7,tenant,User.set_preferences,set_preferences(prefs: dict) -> None,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,dumps,onboarding_state | sqlmodel | tier_gating,pure,no,4,Set preferences from dict.
_models,L7,tenant,generate_uuid,generate_uuid() -> str,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,str | uuid4,onboarding_state | sqlmodel | tier_gating,pure,no,2,
_models,L7,tenant,utc_now,utc_now() -> datetime,?:gateway_audit | ?:api_key_driver | ?:role_guard | ?:onboarding_gate | ?:onboarding_transitions | ?:tenant_auth | ?:predictions | ?:aos_accounts | ?:guard | ?:v1_proxy,utcnow,onboarding_state | sqlmodel | tier_gating,pure,no,7,Return current UTC time as a naive datetime (no timezone info).
_models,L7,threshold_signal,ThresholdSignal.acknowledge,acknowledge(user_id: str) -> None,?:alert_emitter | L4:alert_emitter | ?:test_control_action_enhancements | ?:test_limit_enhancements,now,pydantic | sqlmodel,pure,no,5,Acknowledge a NEAR signal.
_models,L7,threshold_signal,ThresholdSignal.create_breach_signal,"create_breach_signal(run_id: str, policy_id: str, tenant_id: str, metric: ThresholdMetric, current_value: float, threshold_value: float, action_taken: str, step_index: Optional[int]) -> 'ThresholdSignal'",?:alert_emitter | L4:alert_emitter | ?:test_control_action_enhancements | ?:test_limit_enhancements,cls,pydantic | sqlmodel,pure,no,25,Factory method for BREACH threshold signals.
_models,L7,threshold_signal,ThresholdSignal.create_near_signal,"create_near_signal(run_id: str, policy_id: str, tenant_id: str, metric: ThresholdMetric, current_value: float, threshold_value: float, step_index: Optional[int]) -> 'ThresholdSignal'",?:alert_emitter | L4:alert_emitter | ?:test_control_action_enhancements | ?:test_limit_enhancements,cls,pydantic | sqlmodel,pure,no,23,Factory method for NEAR threshold signals.
_models,L7,threshold_signal,ThresholdSignal.mark_alert_sent,mark_alert_sent(channels: list[str]) -> None,?:alert_emitter | L4:alert_emitter | ?:test_control_action_enhancements | ?:test_limit_enhancements,dumps | now,pydantic | sqlmodel,pure,no,7,Mark alert as sent via specified channels.
_models,L7,threshold_signal,ThresholdSignal.to_evidence,to_evidence() -> dict,?:alert_emitter | L4:alert_emitter | ?:test_control_action_enhancements | ?:test_limit_enhancements,isoformat,pydantic | sqlmodel,pure,no,13,Convert to evidence dict for export bundles.
account,L5,accounts_facade,AccountsFacade.__init__,__init__(driver: AccountsFacadeDriver | None) -> None,?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,get_accounts_facade_driver,__future__ | accounts_facade_driver | asyncio | result_types,pure,no,3,Initialize facade with optional driver injection.
account,L5,accounts_facade,AccountsFacade.accept_invitation,"async accept_invitation(session: AsyncSession, invitation_id: str, token: str) -> AcceptInvitationResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,AcceptInvitationResult | encode | fetch_invitation_by_id_and_token | fetch_membership | fetch_user_by_email | hexdigest | insert_membership | insert_user | now | sha256 | split | update_invitation_accepted | update_invitation_expired,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,67,Accept an invitation to join a tenant. Public endpoint.
account,L5,accounts_facade,AccountsFacade.create_support_ticket,"async create_support_ticket(session: AsyncSession, tenant_id: str, user_id: str) -> SupportTicketResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,SupportTicketResult | insert_support_ticket,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,35,Create a support ticket.
account,L5,accounts_facade,AccountsFacade.get_billing_invoices,"async get_billing_invoices(session: AsyncSession, tenant_id: str) -> InvoiceListResult | AccountsErrorResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,AccountsErrorResult | InvoiceListResult | fetch_tenant_detail | lower,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,31,Get billing invoice history.
account,L5,accounts_facade,AccountsFacade.get_billing_summary,"async get_billing_summary(session: AsyncSession, tenant_id: str) -> BillingSummaryResult | AccountsErrorResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,AccountsErrorResult | BillingSummaryResult | fetch_subscription | fetch_tenant_detail | upper,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,55,Get billing summary for the tenant.
account,L5,accounts_facade,AccountsFacade.get_profile,"async get_profile(session: AsyncSession, tenant_id: str, clerk_user_id: Optional[str]) -> ProfileResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,ProfileResult | fetch_profile | upper,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,21,Get current user profile.
account,L5,accounts_facade,AccountsFacade.get_project_detail,"async get_project_detail(session: AsyncSession, tenant_id: str, project_id: str) -> Optional[ProjectDetailResult]",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,ProjectDetailResult | fetch_tenant_detail | upper,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,36,Get project detail. User can only see their own tenant/project.
account,L5,accounts_facade,AccountsFacade.get_support_contact,get_support_contact() -> SupportContactResult,?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,SupportContactResult,__future__ | accounts_facade_driver | asyncio | result_types,pure,no,7,Get support contact information.
account,L5,accounts_facade,AccountsFacade.get_user_detail,"async get_user_detail(session: AsyncSession, tenant_id: str, user_id: str) -> Optional[UserDetailResult]",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,UserDetailResult | fetch_user_detail | upper,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,30,Get user detail. Tenant isolation enforced.
account,L5,accounts_facade,AccountsFacade.invite_user,"async invite_user(session: AsyncSession, tenant_id: str, caller_user_id: str) -> InvitationResult | AccountsErrorResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,AccountsErrorResult | InvitationResult | can_manage_users | encode | fetch_invitation_by_email | fetch_membership | hexdigest | insert_invitation | now | sha256 | timedelta | token_urlsafe,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,60,Invite a user to join the tenant. Requires owner or admin role.
account,L5,accounts_facade,AccountsFacade.list_invitations,"async list_invitations(session: AsyncSession, tenant_id: str, caller_user_id: str) -> InvitationListResult | AccountsErrorResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,AccountsErrorResult | InvitationListResult | InvitationResult | can_manage_users | fetch_invitations | fetch_membership | len,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,41,List invitations for the tenant. Requires owner or admin role.
account,L5,accounts_facade,AccountsFacade.list_projects,"async list_projects(session: AsyncSession, tenant_id: str) -> ProjectsListResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,ProjectSummaryResult | ProjectsListResult | count_tenants | fetch_tenants | len | upper,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,39,"List projects (tenants). In current architecture, Tenant = Project."
account,L5,accounts_facade,AccountsFacade.list_support_tickets,"async list_support_tickets(session: AsyncSession, tenant_id: str) -> SupportTicketListResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,SupportTicketListResult | SupportTicketResult | fetch_support_tickets | len,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,31,List support tickets for the tenant.
account,L5,accounts_facade,AccountsFacade.list_tenant_users,"async list_tenant_users(session: AsyncSession, tenant_id: str) -> TenantUsersListResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,TenantUserResult | TenantUsersListResult | fetch_tenant_memberships | len,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,22,List users in the current tenant.
account,L5,accounts_facade,AccountsFacade.list_users,"async list_users(session: AsyncSession, tenant_id: str) -> UsersListResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,UserSummaryResult | UsersListResult | count_users | fetch_users | len | upper,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,44,List users in the tenant.
account,L5,accounts_facade,AccountsFacade.remove_user,"async remove_user(session: AsyncSession, tenant_id: str, caller_user_id: str, target_user_id: str) -> dict[str, str] | AccountsErrorResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,AccountsErrorResult | can_manage_users | delete_membership | fetch_membership,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,52,Remove a user from the tenant. Requires owner or admin role.
account,L5,accounts_facade,AccountsFacade.update_profile,"async update_profile(session: AsyncSession, user_id: str) -> ProfileUpdateResult | AccountsErrorResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,AccountsErrorResult | ProfileUpdateResult | fetch_user_by_id | get | get_preferences | update_user_profile,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,37,Update current user's profile and preferences.
account,L5,accounts_facade,AccountsFacade.update_user_role,"async update_user_role(session: AsyncSession, tenant_id: str, caller_user_id: str, target_user_id: str, new_role: str) -> TenantUserResult | AccountsErrorResult",?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,AccountsErrorResult | TenantUserResult | can_change_roles | fetch_membership | fetch_membership_with_user | update_membership_role,__future__ | accounts_facade_driver | asyncio | result_types,pure,yes,64,Update a user's role in the tenant. Requires owner role.
account,L5,accounts_facade,get_accounts_facade,get_accounts_facade() -> AccountsFacade,?:aos_accounts | L4:account_handler | L4:account_bridge | L5:__init__,AccountsFacade,__future__ | accounts_facade_driver | asyncio | result_types,pure,no,6,Get the singleton AccountsFacade instance.
account,L5,billing_provider_engine,BillingProvider.get_billing_state,get_billing_state(tenant_id: str) -> BillingState,,,limits | plan | state,pure,no,11,Get the billing state for a tenant.
account,L5,billing_provider_engine,BillingProvider.get_limits,get_limits(plan: Plan) -> Limits,,,limits | plan | state,pure,no,13,Derive limits from a plan.
account,L5,billing_provider_engine,BillingProvider.get_plan,get_plan(tenant_id: str) -> Plan,,,limits | plan | state,pure,no,11,Get the plan for a tenant.
account,L5,billing_provider_engine,BillingProvider.is_limit_exceeded,"is_limit_exceeded(tenant_id: str, limit_name: str, current_value: float) -> bool",,,limits | plan | state,pure,no,15,Check if a specific limit is exceeded.
account,L5,billing_provider_engine,MockBillingProvider.__init__,__init__() -> None,,,limits | plan | state,pure,no,5,Initialize mock provider with in-memory state.
account,L5,billing_provider_engine,MockBillingProvider.get_billing_state,get_billing_state(tenant_id: str) -> BillingState,,default | get,limits | plan | state,pure,no,7,Get the billing state for a tenant.
account,L5,billing_provider_engine,MockBillingProvider.get_limits,get_limits(plan: Plan) -> Limits,,derive_limits,limits | plan | state,pure,no,7,Derive limits from a plan.
account,L5,billing_provider_engine,MockBillingProvider.get_plan,get_plan(tenant_id: str) -> Plan,,get,limits | plan | state,pure,no,7,Get the plan for a tenant.
account,L5,billing_provider_engine,MockBillingProvider.is_limit_exceeded,"is_limit_exceeded(tenant_id: str, limit_name: str, current_value: float) -> bool",,get_limits | get_plan | getattr,limits | plan | state,pure,no,25,Check if a specific limit is exceeded.
account,L5,billing_provider_engine,MockBillingProvider.reset,reset() -> None,,clear,limits | plan | state,pure,no,4,Reset all mock state (for testing).
account,L5,billing_provider_engine,MockBillingProvider.set_billing_state,"set_billing_state(tenant_id: str, state: BillingState) -> None",,info,limits | plan | state,pure,no,10,Set billing state for a tenant (mock/test only).
account,L5,billing_provider_engine,MockBillingProvider.set_plan,"set_plan(tenant_id: str, plan: Plan) -> None",,info,limits | plan | state,pure,no,10,Set plan for a tenant (mock/test only).
account,L5,billing_provider_engine,get_billing_provider,get_billing_provider() -> BillingProvider,,MockBillingProvider,limits | plan | state,pure,no,11,Get the billing provider instance.
account,L5,billing_provider_engine,set_billing_provider,set_billing_provider(provider: BillingProvider) -> None,,,limits | plan | state,pure,no,8,Set the billing provider instance.
account,L5,crm_validator_engine,ValidatorService.__init__,__init__(capability_registry: Optional[list[str]]),L4:__init__,frozenset,,pure,no,9,Initialize validator with optional capability registry.
account,L5,crm_validator_engine,ValidatorService._build_reason,"_build_reason(issue_type: IssueType, severity: Severity, action: RecommendedAction, confidence: Decimal) -> str",L4:__init__,Decimal | append | float | join,,pure,no,26,Build human-readable reason for verdict.
account,L5,crm_validator_engine,ValidatorService._calculate_confidence,"_calculate_confidence(source: str, type_confidence: Decimal, capabilities: list[str]) -> Decimal",L4:__init__,Decimal | _get_capability_confidence | _get_source_weight | max | min,,pure,no,26,Calculate overall confidence score.
account,L5,crm_validator_engine,ValidatorService._classify_issue_type,"_classify_issue_type(text: str) -> tuple[IssueType, Decimal, dict[str, Any]]",L4:__init__,Decimal | append | max | min,,pure,no,65,Classify issue type from text.
account,L5,crm_validator_engine,ValidatorService._classify_severity,"_classify_severity(text: str, issue_type: IssueType) -> tuple[Severity, Decimal]",L4:__init__,Decimal | min | sum,,pure,no,34,Classify severity from text and issue type.
account,L5,crm_validator_engine,ValidatorService._create_fallback_verdict,"_create_fallback_verdict(error_type: ValidatorErrorType, message: str) -> ValidatorVerdict",L4:__init__,Decimal | ValidatorVerdict | now,,pure,no,19,Create fallback verdict on error.
account,L5,crm_validator_engine,ValidatorService._determine_action,"_determine_action(issue_type: IssueType, severity: Severity, confidence: Decimal) -> RecommendedAction",L4:__init__,Decimal,,pure,no,37,Determine recommended action.
account,L5,crm_validator_engine,ValidatorService._do_validate,_do_validate(input: ValidatorInput) -> ValidatorVerdict,L4:__init__,ValidatorVerdict | _build_reason | _calculate_confidence | _classify_issue_type | _classify_severity | _determine_action | _extract_capabilities | _extract_text | _find_severity_indicators | _get_capability_confidence | _get_source_weight | float | list | lower | now,,pure,no,52,Internal validation logic.
account,L5,crm_validator_engine,ValidatorService._extract_capabilities,"_extract_capabilities(text: str, hints: Optional[list[str]]) -> list[str]",L4:__init__,add | escape | lower | search | set | sorted | update,,db_write,no,29,Extract affected capabilities from text.
account,L5,crm_validator_engine,ValidatorService._extract_text,"_extract_text(payload: dict[str, Any]) -> str",L4:__init__,append | isinstance | join | str,,pure,no,23,Extract searchable text from payload.
account,L5,crm_validator_engine,ValidatorService._find_severity_indicators,"_find_severity_indicators(text: str) -> dict[str, list[str]]",L4:__init__,,,pure,no,7,Find severity indicators in text for evidence.
account,L5,crm_validator_engine,ValidatorService._get_capability_confidence,_get_capability_confidence(capabilities: list[str]) -> Decimal,L4:__init__,Decimal | all | any,,pure,no,14,Get confidence modifier based on capability matches.
account,L5,crm_validator_engine,ValidatorService._get_source_weight,_get_source_weight(source: str) -> Decimal,L4:__init__,Decimal | get,,pure,no,14,Get confidence weight for source.
account,L5,crm_validator_engine,ValidatorService.validate,validate(input: ValidatorInput) -> ValidatorVerdict,L4:__init__,_create_fallback_verdict | _do_validate | str,,pure,no,24,Validate an issue and produce a verdict.
account,L5,email_verification_engine,EmailVerificationError.__init__,"__init__(message: str, error_code: str)",L5:__init__,__init__ | super,httpx | redis,pure,no,4,
account,L5,email_verification_engine,EmailVerificationService.__init__,__init__(redis_client: Optional[Redis]),L5:__init__,from_url,httpx | redis,pure,no,6,
account,L5,email_verification_engine,EmailVerificationService._attempts_key,_attempts_key(email: str) -> str,L5:__init__,encode | hexdigest | lower | sha256,httpx | redis,pure,no,4,Generate Redis key for attempt tracking.
account,L5,email_verification_engine,EmailVerificationService._cooldown_key,_cooldown_key(email: str) -> str,L5:__init__,encode | hexdigest | lower | sha256,httpx | redis,pure,no,4,Generate Redis key for cooldown tracking.
account,L5,email_verification_engine,EmailVerificationService._generate_otp,_generate_otp() -> str,L5:__init__,choice | join | range,httpx | redis,pure,no,3,Generate a cryptographically secure OTP.
account,L5,email_verification_engine,EmailVerificationService._otp_key,_otp_key(email: str) -> str,L5:__init__,encode | hexdigest | lower | sha256,httpx | redis,pure,no,4,Generate Redis key for OTP storage.
account,L5,email_verification_engine,EmailVerificationService._send_otp_email,"async _send_otp_email(email: str, otp: str, name: Optional[str])",L5:__init__,AsyncClient | EmailVerificationError | error | post | warning,httpx | redis,external_api,yes,74,Send OTP email via Resend.
account,L5,email_verification_engine,EmailVerificationService.send_otp,"async send_otp(email: str, name: Optional[str]) -> dict",L5:__init__,EmailVerificationError | _attempts_key | _cooldown_key | _generate_otp | _otp_key | _send_otp_email | delete | exists | info | lower | setex | strip | ttl,httpx | redis,db_write,yes,46,Generate and send OTP to email address.
account,L5,email_verification_engine,EmailVerificationService.verify_otp,"verify_otp(email: str, otp: str) -> VerificationResult",L5:__init__,VerificationResult | _attempts_key | _otp_key | delete | expire | get | incr | int | lower | max | split | strip,httpx | redis,db_write,no,58,Verify OTP code.
account,L5,email_verification_engine,get_email_verification_service,get_email_verification_service() -> EmailVerificationService,L5:__init__,EmailVerificationService,httpx | redis,pure,no,6,Get email verification service singleton.
account,L5,identity_resolver_engine,APIKeyIdentityResolver.provider,provider() -> IdentityProvider,,,iam_service | jwt,pure,no,2,
account,L5,identity_resolver_engine,APIKeyIdentityResolver.resolve,"async resolve(credential: str, tenant_id: Optional[str]) -> Optional[Identity]",,Identity | len,iam_service | jwt,pure,yes,19,Resolve identity from API key.
account,L5,identity_resolver_engine,ClerkIdentityResolver.__init__,__init__(clerk_secret_key: Optional[str]),,getenv,iam_service | jwt,pure,no,3,
account,L5,identity_resolver_engine,ClerkIdentityResolver.provider,provider() -> IdentityProvider,,,iam_service | jwt,pure,no,2,
account,L5,identity_resolver_engine,ClerkIdentityResolver.resolve,"async resolve(credential: str, tenant_id: Optional[str]) -> Optional[Identity]",,Identity | decode | get | set | warning,iam_service | jwt,pure,yes,28,Resolve identity from Clerk JWT.
account,L5,identity_resolver_engine,IdentityChain.resolve,"async resolve(credential: str, provider_hint: Optional[IdentityProvider], tenant_id: Optional[str]) -> Optional[Identity]",,debug | resolve,iam_service | jwt,pure,yes,36,Resolve identity using the resolver chain.
account,L5,identity_resolver_engine,IdentityResolver.provider,provider() -> IdentityProvider,,,iam_service | jwt,pure,no,3,Get the provider type.
account,L5,identity_resolver_engine,IdentityResolver.resolve,"async resolve(credential: str, tenant_id: Optional[str]) -> Optional[Identity]",,,iam_service | jwt,pure,yes,7,Resolve an identity from a credential.
account,L5,identity_resolver_engine,SystemIdentityResolver.provider,provider() -> IdentityProvider,,,iam_service | jwt,pure,no,2,
account,L5,identity_resolver_engine,SystemIdentityResolver.resolve,"async resolve(credential: str, tenant_id: Optional[str]) -> Optional[Identity]",,Identity,iam_service | jwt,pure,yes,13,Create a system identity.
account,L5,identity_resolver_engine,create_default_identity_chain,create_default_identity_chain() -> IdentityChain,,APIKeyIdentityResolver | ClerkIdentityResolver | IdentityChain | SystemIdentityResolver,iam_service | jwt,pure,no,9,Create the default identity resolver chain.
account,L5,notifications_facade,ChannelInfo.to_dict,"to_dict() -> Dict[str, Any]",L4:account_handler | L4:account_bridge | L5:__init__,,,pure,no,9,Convert to dictionary.
account,L5,notifications_facade,NotificationInfo.to_dict,"to_dict() -> Dict[str, Any]",L4:account_handler | L4:account_bridge | L5:__init__,,,pure,no,16,Convert to dictionary.
account,L5,notifications_facade,NotificationPreferences.to_dict,"to_dict() -> Dict[str, Any]",L4:account_handler | L4:account_bridge | L5:__init__,,,pure,no,8,Convert to dictionary.
account,L5,notifications_facade,NotificationsFacade.__init__,__init__(),L4:account_handler | L4:account_bridge | L5:__init__,ChannelInfo,,pure,no,44,Initialize facade.
account,L5,notifications_facade,NotificationsFacade.get_channel,async get_channel(channel_id: str) -> Optional[ChannelInfo],L4:account_handler | L4:account_bridge | L5:__init__,get,,pure,yes,11,Get a specific channel.
account,L5,notifications_facade,NotificationsFacade.get_notification,"async get_notification(notification_id: str, tenant_id: str) -> Optional[NotificationInfo]",L4:account_handler | L4:account_bridge | L5:__init__,get,,pure,yes,19,Get a specific notification.
account,L5,notifications_facade,NotificationsFacade.get_preferences,"async get_preferences(tenant_id: str, user_id: str) -> NotificationPreferences",L4:account_handler | L4:account_bridge | L5:__init__,NotificationPreferences,,pure,yes,25,Get notification preferences.
account,L5,notifications_facade,NotificationsFacade.list_channels,async list_channels() -> List[ChannelInfo],L4:account_handler | L4:account_bridge | L5:__init__,list | values,,pure,yes,8,List available notification channels.
account,L5,notifications_facade,NotificationsFacade.list_notifications,"async list_notifications(tenant_id: str, channel: Optional[str], status: Optional[str], recipient: Optional[str], limit: int, offset: int) -> List[NotificationInfo]",L4:account_handler | L4:account_bridge | L5:__init__,append | sort | values,,pure,yes,39,List notifications.
account,L5,notifications_facade,NotificationsFacade.mark_as_read,"async mark_as_read(notification_id: str, tenant_id: str) -> Optional[NotificationInfo]",L4:account_handler | L4:account_bridge | L5:__init__,get | isoformat | now,,pure,yes,22,Mark notification as read.
account,L5,notifications_facade,NotificationsFacade.send_notification,"async send_notification(tenant_id: str, channel: str, recipient: str, message: str, subject: Optional[str], priority: str, metadata: Optional[Dict[str, Any]]) -> NotificationInfo",L4:account_handler | L4:account_bridge | L5:__init__,NotificationInfo | get | info | isoformat | now | str | uuid4,,pure,yes,65,Send a notification.
account,L5,notifications_facade,NotificationsFacade.update_preferences,"async update_preferences(tenant_id: str, user_id: str, channels: Optional[Dict[str, bool]], priorities: Optional[Dict[str, List[str]]]) -> NotificationPreferences",L4:account_handler | L4:account_bridge | L5:__init__,get_preferences | update,,pure,yes,29,Update notification preferences.
account,L5,notifications_facade,get_notifications_facade,get_notifications_facade() -> NotificationsFacade,L4:account_handler | L4:account_bridge | L5:__init__,NotificationsFacade,,pure,no,14,Get the notifications facade instance.
account,L5,profile_engine,GovernanceConfig.to_dict,"to_dict() -> Dict[str, object]",,,,pure,no,14,Serialize for logging.
account,L5,profile_engine,GovernanceConfigError.__init__,"__init__(message: str, violations: List[str])",,__init__ | join | super,,pure,no,3,
account,L5,profile_engine,_get_bool_env,"_get_bool_env(name: str, default: bool) -> bool",,getenv | lower,,pure,no,6,Get boolean from environment variable.
account,L5,profile_engine,get_governance_config,get_governance_config() -> GovernanceConfig,,load_governance_config | validate_governance_config,,pure,no,14,Get the validated governance configuration singleton.
account,L5,profile_engine,get_governance_profile,get_governance_profile() -> GovernanceProfile,,GovernanceProfile | getenv | upper | warning,,pure,no,20,Get the current governance profile from environment.
account,L5,profile_engine,load_governance_config,load_governance_config() -> GovernanceConfig,,GovernanceConfig | _get_bool_env | get_governance_profile | info | to_dict,,pure,no,48,Load complete governance configuration.
account,L5,profile_engine,reset_governance_config,reset_governance_config() -> None,,,,pure,no,4,Reset the singleton (for testing).
account,L5,profile_engine,validate_governance_at_startup,validate_governance_at_startup() -> None,,get_governance_config | info,,pure,no,22,Validate governance configuration at application startup.
account,L5,profile_engine,validate_governance_config,validate_governance_config(config: Optional[GovernanceConfig]) -> List[str],,GovernanceConfigError | all | append | error | get | info | len | load_governance_config | warning,,pure,no,76,Validate governance configuration for invalid combinations.
account,L5,tenant_engine,QuotaExceededError.__init__,"__init__(quota_name: str, limit: int, current: int)",L4:account_bridge | L5:__init__,__init__ | super,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,5,
account,L5,tenant_engine,TenantEngine.__init__,"__init__(session: Session, driver: TenantDriver | None)",L4:account_bridge | L5:__init__,get_tenant_driver,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,3,
account,L5,tenant_engine,TenantEngine._maybe_reset_daily_counter,_maybe_reset_daily_counter(tenant: Tenant) -> None,L4:account_bridge | L5:__init__,date | update_tenant_usage | utc_now,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,15,Reset daily run counter if new day (temporal logic).
account,L5,tenant_engine,TenantEngine.check_run_quota,"check_run_quota(tenant_id: str) -> Tuple[bool, str]",L4:account_bridge | L5:__init__,_maybe_reset_daily_counter | count_running_runs | fetch_tenant_by_id,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,33,Check if tenant can create a new run.
account,L5,tenant_engine,TenantEngine.check_token_quota,"check_token_quota(tenant_id: str, tokens_needed: int) -> Tuple[bool, str]",L4:account_bridge | L5:__init__,fetch_tenant_by_id,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,20,Check if tenant has token budget for operation.
account,L5,tenant_engine,TenantEngine.complete_run,"complete_run(run_id: str, success: bool, output_json: Optional[str], replay_token_json: Optional[str], total_tokens: int, total_latency_ms: int, stages_completed: int, recoveries: int, policy_violations: int, cost_cents: int, error: Optional[str]) -> WorkerRun",L4:account_bridge | L5:__init__,TenantEngineError | fetch_run_by_id | fetch_tenant_by_id | record_usage | update_run_completed | update_tenant_usage,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,64,Mark a run as completed and record usage.
account,L5,tenant_engine,TenantEngine.create_api_key,"create_api_key(tenant_id: str, name: str, user_id: Optional[str], permissions: Optional[List[str]], allowed_workers: Optional[List[str]], expires_in_days: Optional[int], rate_limit_rpm: Optional[int], max_concurrent_runs: Optional[int]) -> Tuple[str, APIKey]",L4:account_bridge | L5:__init__,QuotaExceededError | TenantEngineError | count_active_api_keys | fetch_tenant_by_id | generate_key | info | insert_api_key | insert_audit_log | timedelta | utc_now,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,69,Create a new API key for a tenant.
account,L5,tenant_engine,TenantEngine.create_membership_with_default,"create_membership_with_default(tenant: Tenant, user_id: str, role: str, set_as_default: bool) -> TenantMembership",L4:account_bridge | L5:__init__,info | insert_membership,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,25,Create a tenant membership and optionally set as user's default tenant.
account,L5,tenant_engine,TenantEngine.create_run,"create_run(tenant_id: str, worker_id: str, task: str, api_key_id: Optional[str], user_id: Optional[str], input_json: Optional[str]) -> WorkerRun",L4:account_bridge | L5:__init__,QuotaExceededError | check_run_quota | increment_usage | insert_run,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,29,Create a new worker run (with quota check).
account,L5,tenant_engine,TenantEngine.create_tenant,"create_tenant(name: str, slug: str, clerk_org_id: Optional[str], plan: str, billing_email: Optional[str]) -> Tenant",L4:account_bridge | L5:__init__,TenantEngineError | fetch_tenant_by_slug | get | info | insert_tenant,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,33,Create a new tenant with plan quotas.
account,L5,tenant_engine,TenantEngine.get_tenant,get_tenant(tenant_id: str) -> Optional[Tenant],L4:account_bridge | L5:__init__,fetch_tenant_by_id,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,3,Get tenant by ID.
account,L5,tenant_engine,TenantEngine.get_tenant_by_slug,get_tenant_by_slug(slug: str) -> Optional[Tenant],L4:account_bridge | L5:__init__,fetch_tenant_by_slug,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,3,Get tenant by slug.
account,L5,tenant_engine,TenantEngine.get_usage_summary,"get_usage_summary(tenant_id: str, start_date: Optional[Any], end_date: Optional[Any]) -> dict",L4:account_bridge | L5:__init__,fetch_usage_records | isoformat | len | replace | utc_now,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,31,Get usage summary for a tenant.
account,L5,tenant_engine,TenantEngine.increment_usage,"increment_usage(tenant_id: str, tokens: int) -> None",L4:account_bridge | L5:__init__,_maybe_reset_daily_counter | fetch_tenant_by_id | increment_tenant_usage,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,11,Increment usage counters for a tenant.
account,L5,tenant_engine,TenantEngine.list_api_keys,"list_api_keys(tenant_id: str, include_revoked: bool) -> List[APIKey]",L4:account_bridge | L5:__init__,fetch_api_keys,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,3,List API keys for a tenant.
account,L5,tenant_engine,TenantEngine.list_runs,"list_runs(tenant_id: str, limit: int, offset: int, status: Optional[str], worker_id: Optional[str]) -> List[WorkerRun]",L4:account_bridge | L5:__init__,fetch_runs,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,16,List runs for a tenant.
account,L5,tenant_engine,TenantEngine.record_usage,"record_usage(tenant_id: str, meter_name: str, amount: int, unit: str, worker_id: Optional[str], api_key_id: Optional[str], metadata: Optional[dict]) -> None",L4:account_bridge | L5:__init__,insert_usage_record,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,20,Record a usage event for billing.
account,L5,tenant_engine,TenantEngine.revoke_api_key,"revoke_api_key(key_id: str, reason: str, user_id: Optional[str]) -> APIKey",L4:account_bridge | L5:__init__,TenantEngineError | fetch_api_key_by_id | info | insert_audit_log | update_api_key_revoked,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,28,Revoke an API key.
account,L5,tenant_engine,TenantEngine.suspend,"suspend(tenant_id: str, reason: str) -> Tenant",L4:account_bridge | L5:__init__,TenantEngineError | fetch_tenant_by_id | update_tenant_status | warning,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,11,Suspend a tenant.
account,L5,tenant_engine,TenantEngine.update_plan,"update_plan(tenant_id: str, plan: str) -> Tenant",L4:account_bridge | L5:__init__,TenantEngineError | fetch_tenant_by_id | get | info | update_tenant_plan,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,22,Update tenant plan and quotas.
account,L5,tenant_engine,get_tenant_engine,get_tenant_engine(session: Session) -> TenantEngine,L4:account_bridge | L5:__init__,TenantEngine,__future__ | sqlmodel | tenant | tenant_driver | time,pure,no,3,Get a TenantEngine instance.
account,L5,user_write_engine,UserWriteService.__init__,__init__(session: 'Session'),L5:__init__,get_user_write_driver,sqlmodel | tenant | user_write_driver,pure,no,2,
account,L5,user_write_engine,UserWriteService.create_user,"create_user(email: str, clerk_user_id: str, name: Optional[str], avatar_url: Optional[str], status: str) -> 'User'",L5:__init__,create_user,sqlmodel | tenant | user_write_driver,pure,no,16,Delegate to driver.
account,L5,user_write_engine,UserWriteService.update_user_login,update_user_login(user: 'User') -> 'User',L5:__init__,update_user_login,sqlmodel | tenant | user_write_driver,pure,no,3,Delegate to driver.
account,L5,user_write_engine,UserWriteService.user_to_dict,user_to_dict(user: 'User') -> Dict,L5:__init__,user_to_dict,sqlmodel | tenant | user_write_driver,pure,no,3,Delegate to driver.
account,L6,accounts_facade_driver,AccountsFacadeDriver.count_tenants,"async count_tenants(session: AsyncSession, tenant_id: str) -> int",L6:__init__ | L5:accounts_facade,count | execute | scalar | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,14,Count tenants.
account,L6,accounts_facade_driver,AccountsFacadeDriver.count_users,"async count_users(session: AsyncSession, tenant_id: str) -> int",L6:__init__ | L5:accounts_facade,count | execute | join | scalar | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,21,Count users in a tenant.
account,L6,accounts_facade_driver,AccountsFacadeDriver.delete_membership,"async delete_membership(session: AsyncSession, membership: TenantMembership) -> bool",L6:__init__ | L5:accounts_facade,delete,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,9,Delete a membership.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_invitation_by_email,"async fetch_invitation_by_email(session: AsyncSession, tenant_id: str, email: str, status: str) -> Optional[Invitation]",L6:__init__ | L5:accounts_facade,execute | scalar_one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,15,Fetch pending invitation by email.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_invitation_by_id_and_token,"async fetch_invitation_by_id_and_token(session: AsyncSession, invitation_id: str, token_hash: str) -> Optional[Invitation]",L6:__init__ | L5:accounts_facade,execute | scalar_one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,14,Fetch invitation by ID and token hash.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_invitations,"async fetch_invitations(session: AsyncSession, tenant_id: str) -> list[InvitationSnapshot]",L6:__init__ | L5:accounts_facade,InvitationSnapshot | all | desc | execute | order_by | scalars | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,31,Fetch invitations for tenant.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_membership,"async fetch_membership(session: AsyncSession, tenant_id: str, user_id: str) -> Optional[TenantMembership]",L6:__init__ | L5:accounts_facade,execute | scalar_one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,13,Fetch a specific membership (returns ORM model for mutations).
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_membership_with_user,"async fetch_membership_with_user(session: AsyncSession, tenant_id: str, user_id: str) -> Optional[tuple[TenantMembership, User]]",L6:__init__ | L5:accounts_facade,execute | join | one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,18,Fetch membership with user data.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_profile,"async fetch_profile(session: AsyncSession, tenant_id: str, clerk_user_id: Optional[str]) -> ProfileSnapshot",L6:__init__ | L5:accounts_facade,ProfileSnapshot | execute | now | scalar_one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,56,Fetch user profile.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_subscription,"async fetch_subscription(session: AsyncSession, tenant_id: str) -> Optional[SubscriptionSnapshot]",L6:__init__ | L5:accounts_facade,SubscriptionSnapshot | desc | execute | limit | order_by | scalar_one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,25,Fetch latest subscription for tenant.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_support_tickets,"async fetch_support_tickets(session: AsyncSession, tenant_id: str) -> list[TicketSnapshot]",L6:__init__ | L5:accounts_facade,TicketSnapshot | all | desc | execute | order_by | scalars | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,32,Fetch support tickets for tenant.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_tenant,"async fetch_tenant(session: AsyncSession, tenant_id: str) -> Optional[TenantSnapshot]",L6:__init__ | L5:accounts_facade,TenantSnapshot | execute | scalar_one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,22,Fetch a tenant by ID.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_tenant_detail,"async fetch_tenant_detail(session: AsyncSession, tenant_id: str) -> Optional[TenantDetailSnapshot]",L6:__init__ | L5:accounts_facade,TenantDetailSnapshot | execute | has_completed_onboarding | scalar_one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,32,Fetch detailed tenant data.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_tenant_memberships,"async fetch_tenant_memberships(session: AsyncSession, tenant_id: str) -> list[MembershipSnapshot]",L6:__init__ | L5:accounts_facade,MembershipSnapshot | all | execute | join | order_by | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,26,Fetch all memberships for a tenant.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_tenants,"async fetch_tenants(session: AsyncSession, tenant_id: str) -> list[TenantSnapshot]",L6:__init__ | L5:accounts_facade,TenantSnapshot | all | execute | limit | offset | scalars | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,31,Fetch tenants (filtered by tenant_id for isolation).
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_user_by_email,"async fetch_user_by_email(session: AsyncSession, email: str) -> Optional[User]",L6:__init__ | L5:accounts_facade,execute | scalar_one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,9,Fetch user by email.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_user_by_id,"async fetch_user_by_id(session: AsyncSession, user_id: str) -> Optional[User]",L6:__init__ | L5:accounts_facade,execute | scalar_one_or_none | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,9,Fetch user by ID (returns ORM model for mutations).
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_user_detail,"async fetch_user_detail(session: AsyncSession, tenant_id: str, user_id: str) -> Optional[UserDetailSnapshot]",L6:__init__ | L5:accounts_facade,UserDetailSnapshot | can_manage_keys | can_run_workers | can_view_runs | execute | first | join | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,41,Fetch detailed user data with membership.
account,L6,accounts_facade_driver,AccountsFacadeDriver.fetch_users,"async fetch_users(session: AsyncSession, tenant_id: str) -> list[UserSnapshot]",L6:__init__ | L5:accounts_facade,UserSnapshot | all | execute | join | limit | offset | order_by | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,47,Fetch users in a tenant.
account,L6,accounts_facade_driver,AccountsFacadeDriver.insert_invitation,"async insert_invitation(session: AsyncSession, tenant_id: str, email: str, role: str, token_hash: str, invited_by: str, expires_at: datetime) -> InvitationSnapshot",L6:__init__ | L5:accounts_facade,Invitation | InvitationSnapshot | add | generate_uuid | refresh | utc_now,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,39,Insert a new invitation.
account,L6,accounts_facade_driver,AccountsFacadeDriver.insert_membership,"async insert_membership(session: AsyncSession, tenant_id: str, user_id: str, role: str) -> TenantMembership",L6:__init__ | L5:accounts_facade,TenantMembership | add | generate_uuid | utc_now,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,18,Insert a new membership.
account,L6,accounts_facade_driver,AccountsFacadeDriver.insert_support_ticket,"async insert_support_ticket(session: AsyncSession, tenant_id: str, user_id: str) -> TicketSnapshot",L6:__init__ | L5:accounts_facade,SupportTicket | TicketSnapshot | add | generate_uuid | refresh | utc_now,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,42,Insert a new support ticket.
account,L6,accounts_facade_driver,AccountsFacadeDriver.insert_user,"async insert_user(session: AsyncSession, email: str, name: str) -> User",L6:__init__ | L5:accounts_facade,User | add | flush | generate_uuid | utc_now,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,18,Insert a new user.
account,L6,accounts_facade_driver,AccountsFacadeDriver.update_invitation_accepted,"async update_invitation_accepted(session: AsyncSession, invitation: Invitation) -> InvitationSnapshot",L6:__init__ | L5:accounts_facade,InvitationSnapshot | utc_now,__future__ | asyncio | sqlalchemy | tenant,pure,yes,22,Mark invitation as accepted.
account,L6,accounts_facade_driver,AccountsFacadeDriver.update_invitation_expired,"async update_invitation_expired(session: AsyncSession, invitation: Invitation) -> None",L6:__init__ | L5:accounts_facade,,__future__ | asyncio | sqlalchemy | tenant,pure,yes,7,Mark invitation as expired.
account,L6,accounts_facade_driver,AccountsFacadeDriver.update_membership_role,"async update_membership_role(session: AsyncSession, membership: TenantMembership, new_role: str) -> MembershipSnapshot",L6:__init__ | L5:accounts_facade,MembershipSnapshot | execute | scalar_one | select | where,__future__ | asyncio | sqlalchemy | tenant,db_write,yes,22,Update membership role.
account,L6,accounts_facade_driver,AccountsFacadeDriver.update_user_profile,"async update_user_profile(session: AsyncSession, user: User) -> User",L6:__init__ | L5:accounts_facade,get_preferences | refresh | set_preferences | update | utc_now,__future__ | asyncio | sqlalchemy | tenant,pure,yes,25,Update user profile fields.
account,L6,accounts_facade_driver,get_accounts_facade_driver,get_accounts_facade_driver() -> AccountsFacadeDriver,L6:__init__ | L5:accounts_facade,AccountsFacadeDriver,__future__ | asyncio | sqlalchemy | tenant,pure,no,6,Get the singleton AccountsFacadeDriver instance.
account,L6,tenant_driver,TenantDriver.__init__,__init__(session: Session),L6:__init__ | L5:tenant_engine,,__future__ | sqlmodel | tenant | time,pure,no,2,
account,L6,tenant_driver,TenantDriver.count_active_api_keys,count_active_api_keys(tenant_id: str) -> int,L6:__init__ | L5:tenant_engine,count | exec | one | select | where,__future__ | sqlmodel | tenant | time,pure,no,9,Count active API keys for tenant.
account,L6,tenant_driver,TenantDriver.count_running_runs,count_running_runs(tenant_id: str) -> int,L6:__init__ | L5:tenant_engine,cast | count | exec | in_ | one | select | where,__future__ | sqlmodel | tenant | time,pure,no,9,Count queued or running runs for tenant.
account,L6,tenant_driver,TenantDriver.fetch_api_key_by_id,fetch_api_key_by_id(key_id: str) -> Optional[APIKey],L6:__init__ | L5:tenant_engine,get,__future__ | sqlmodel | tenant | time,pure,no,3,Fetch API key by ID.
account,L6,tenant_driver,TenantDriver.fetch_api_keys,"fetch_api_keys(tenant_id: str, include_revoked: bool) -> List[APIKey]",L6:__init__ | L5:tenant_engine,exec | list | select | where,__future__ | sqlmodel | tenant | time,pure,no,10,Fetch API keys for tenant.
account,L6,tenant_driver,TenantDriver.fetch_run_by_id,fetch_run_by_id(run_id: str) -> Optional[WorkerRun],L6:__init__ | L5:tenant_engine,get,__future__ | sqlmodel | tenant | time,pure,no,3,Fetch run by ID.
account,L6,tenant_driver,TenantDriver.fetch_runs,"fetch_runs(tenant_id: str, limit: int, offset: int, status: Optional[str], worker_id: Optional[str]) -> List[WorkerRun]",L6:__init__ | L5:tenant_engine,cast | desc | exec | limit | list | offset | order_by | select | where,__future__ | sqlmodel | tenant | time,pure,no,16,Fetch runs for tenant.
account,L6,tenant_driver,TenantDriver.fetch_tenant_by_id,fetch_tenant_by_id(tenant_id: str) -> Optional[Tenant],L6:__init__ | L5:tenant_engine,get,__future__ | sqlmodel | tenant | time,pure,no,3,Fetch tenant by ID (returns ORM model for mutations).
account,L6,tenant_driver,TenantDriver.fetch_tenant_by_slug,fetch_tenant_by_slug(slug: str) -> Optional[Tenant],L6:__init__ | L5:tenant_engine,exec | first | select | where,__future__ | sqlmodel | tenant | time,pure,no,3,Fetch tenant by slug.
account,L6,tenant_driver,TenantDriver.fetch_tenant_snapshot,fetch_tenant_snapshot(tenant_id: str) -> Optional[TenantCoreSnapshot],L6:__init__ | L5:tenant_engine,TenantCoreSnapshot | fetch_tenant_by_id,__future__ | sqlmodel | tenant | time,pure,no,23,Fetch tenant as snapshot.
account,L6,tenant_driver,TenantDriver.fetch_usage_records,"fetch_usage_records(tenant_id: str, start_date: datetime, end_date: datetime) -> List[UsageRecord]",L6:__init__ | L5:tenant_engine,exec | list | select | where,__future__ | sqlmodel | tenant | time,pure,no,13,Fetch usage records for period.
account,L6,tenant_driver,TenantDriver.increment_tenant_usage,"increment_tenant_usage(tenant: Tenant, tokens: int) -> None",L6:__init__ | L5:tenant_engine,add | increment_usage,__future__ | sqlmodel | tenant | time,db_write,no,4,Increment usage counters.
account,L6,tenant_driver,TenantDriver.insert_api_key,"insert_api_key(tenant_id: str, name: str, key_prefix: str, key_hash: str, user_id: Optional[str], permissions: Optional[List[str]], allowed_workers: Optional[List[str]], expires_at: Optional[datetime], rate_limit_rpm: Optional[int], max_concurrent_runs: Optional[int]) -> APIKey",L6:__init__ | L5:tenant_engine,APIKey | add | dumps | flush | refresh,__future__ | sqlmodel | tenant | time,db_write,no,30,Insert a new API key.
account,L6,tenant_driver,TenantDriver.insert_audit_log,"insert_audit_log(action: str, resource_type: str, tenant_id: Optional[str], user_id: Optional[str], api_key_id: Optional[str], resource_id: Optional[str], old_value: Optional[dict], new_value: Optional[dict], ip_address: Optional[str], user_agent: Optional[str], request_id: Optional[str]) -> None",L6:__init__ | L5:tenant_engine,AuditLog | add | dumps,__future__ | sqlmodel | tenant | time,db_write,no,29,Insert an audit log entry.
account,L6,tenant_driver,TenantDriver.insert_membership,"insert_membership(tenant_id: str, user_id: str, role: str, set_as_default: bool) -> TenantMembership",L6:__init__ | L5:tenant_engine,TenantMembership | add | get,__future__ | sqlmodel | tenant | time,db_write,no,23,Insert a new membership.
account,L6,tenant_driver,TenantDriver.insert_run,"insert_run(tenant_id: str, worker_id: str, task: str, api_key_id: Optional[str], user_id: Optional[str], input_json: Optional[str]) -> WorkerRun",L6:__init__ | L5:tenant_engine,WorkerRun | add | flush | refresh,__future__ | sqlmodel | tenant | time,db_write,no,23,Insert a new worker run.
account,L6,tenant_driver,TenantDriver.insert_tenant,"insert_tenant(name: str, slug: str, plan: str, max_workers: int, max_runs_per_day: int, max_concurrent_runs: int, max_tokens_per_month: int, max_api_keys: int, clerk_org_id: Optional[str], billing_email: Optional[str]) -> Tenant",L6:__init__ | L5:tenant_engine,Tenant | add | flush | refresh,__future__ | sqlmodel | tenant | time,db_write,no,30,Insert a new tenant.
account,L6,tenant_driver,TenantDriver.insert_usage_record,"insert_usage_record(tenant_id: str, meter_name: str, amount: int, unit: str, worker_id: Optional[str], api_key_id: Optional[str], metadata: Optional[dict]) -> UsageRecord",L6:__init__ | L5:tenant_engine,UsageRecord | add | dumps | replace | timedelta | utc_now,__future__ | sqlmodel | tenant | time,db_write,no,29,Insert a usage record.
account,L6,tenant_driver,TenantDriver.update_api_key_revoked,"update_api_key_revoked(api_key: APIKey, reason: str) -> APIKey",L6:__init__ | L5:tenant_engine,add | utc_now,__future__ | sqlmodel | tenant | time,db_write,no,12,Mark API key as revoked.
account,L6,tenant_driver,TenantDriver.update_run_completed,"update_run_completed(run: WorkerRun, success: bool, output_json: Optional[str], replay_token_json: Optional[str], total_tokens: int, total_latency_ms: int, stages_completed: int, recoveries: int, policy_violations: int, cost_cents: int, error: Optional[str]) -> WorkerRun",L6:__init__ | L5:tenant_engine,add | utc_now,__future__ | sqlmodel | tenant | time,db_write,no,30,Update run as completed.
account,L6,tenant_driver,TenantDriver.update_tenant_plan,"update_tenant_plan(tenant: Tenant, plan: str, max_workers: int, max_runs_per_day: int, max_concurrent_runs: int, max_tokens_per_month: int, max_api_keys: int) -> Tenant",L6:__init__ | L5:tenant_engine,add | utc_now,__future__ | sqlmodel | tenant | time,db_write,no,21,Update tenant plan and quotas.
account,L6,tenant_driver,TenantDriver.update_tenant_status,"update_tenant_status(tenant: Tenant, status: str, suspended_reason: Optional[str]) -> Tenant",L6:__init__ | L5:tenant_engine,add | utc_now,__future__ | sqlmodel | tenant | time,db_write,no,14,Update tenant status.
account,L6,tenant_driver,TenantDriver.update_tenant_usage,"update_tenant_usage(tenant: Tenant, runs_today: Optional[int], runs_this_month: Optional[int], tokens_this_month: Optional[int], last_run_reset_at: Optional[datetime]) -> Tenant",L6:__init__ | L5:tenant_engine,add,__future__ | sqlmodel | tenant | time,db_write,no,20,Update tenant usage counters.
account,L6,tenant_driver,get_tenant_driver,get_tenant_driver(session: Session) -> TenantDriver,L6:__init__ | L5:tenant_engine,TenantDriver,__future__ | sqlmodel | tenant | time,pure,no,3,Get a TenantDriver instance.
account,L6,user_write_driver,UserWriteDriver.__init__,__init__(session: Session),L6:__init__ | L5:user_write_engine,,sqlmodel | tenant | time,pure,no,2,
account,L6,user_write_driver,UserWriteDriver.create_user,"create_user(email: str, clerk_user_id: str, name: Optional[str], avatar_url: Optional[str], status: str) -> User",L6:__init__ | L5:user_write_engine,User | add | flush | refresh,sqlmodel | tenant | time,db_write,no,32,Create a new user and persist.
account,L6,user_write_driver,UserWriteDriver.update_user_login,update_user_login(user: User) -> User,L6:__init__ | L5:user_write_engine,add | flush | refresh | utc_now,sqlmodel | tenant | time,db_write,no,17,Update user's last_login_at timestamp and persist.
account,L6,user_write_driver,UserWriteDriver.user_to_dict,user_to_dict(user: User) -> Dict,L6:__init__ | L5:user_write_engine,,sqlmodel | tenant | time,pure,no,15,Convert user to dict for response.
account,L6,user_write_driver,get_user_write_driver,get_user_write_driver(session: Session) -> UserWriteDriver,L6:__init__ | L5:user_write_engine,UserWriteDriver,sqlmodel | tenant | time,pure,no,3,Factory function to get UserWriteDriver instance.
activity,L5,activity_enums,SeverityLevel.from_risk_level,from_risk_level(risk_level: str) -> 'SeverityLevel',L5:activity_facade,,,pure,no,7,Convert risk level string to severity level.
activity,L5,activity_enums,SeverityLevel.from_score,from_score(score: float) -> 'SeverityLevel',L5:activity_facade,,,pure,no,7,Convert numeric severity score (0.0-1.0) to level.
activity,L5,activity_facade,ActivityFacade.__init__,__init__() -> None,?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,3,Initialize facade.
activity,L5,activity_facade,ActivityFacade._compute_severity,"_compute_severity(row: dict[str, Any]) -> str",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,from_risk_level | get | upper,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,9,Compute severity level from run data.
activity,L5,activity_facade,ActivityFacade._compute_signal_summary,"_compute_signal_summary(row: dict[str, Any], signal_type: str) -> str",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,get,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,5,Compute signal summary from run data.
activity,L5,activity_facade,ActivityFacade._compute_signal_type,"_compute_signal_type(row: dict[str, Any]) -> str",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,get,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,25,Compute signal type from run data.
activity,L5,activity_facade,ActivityFacade._get_attention_service,_get_attention_service(session: AsyncSession) -> AttentionRankingService,?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,AttentionRankingService,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,3,Get attention ranking service for this session.
activity,L5,activity_facade,ActivityFacade._get_cost_service,_get_cost_service(session: AsyncSession) -> CostAnalysisService,?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,CostAnalysisService,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,3,Get cost analysis service for this session.
activity,L5,activity_facade,ActivityFacade._get_driver,_get_driver(session: AsyncSession) -> ActivityReadDriver,?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,get_activity_read_driver,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,3,Get activity read driver for this session.
activity,L5,activity_facade,ActivityFacade._get_feedback_service,_get_feedback_service(session: AsyncSession) -> SignalFeedbackService,?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,SignalFeedbackService,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,3,Get signal feedback service for this session.
activity,L5,activity_facade,ActivityFacade._get_pattern_service,_get_pattern_service(session: AsyncSession) -> PatternDetectionService,?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,PatternDetectionService,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,3,Get pattern detection service for this session.
activity,L5,activity_facade,ActivityFacade._get_runs_with_policy_context,"async _get_runs_with_policy_context(session: AsyncSession, tenant_id: str, state: str) -> dict[str, Any]",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,PolicyContextResult | RunSummaryV2Result | _get_driver | append | count_runs | fetch_runs_with_policy_context | float | get | join | len | lower,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,92,Internal helper to get runs with policy context.
activity,L5,activity_facade,ActivityFacade.acknowledge_signal,"async acknowledge_signal(session: AsyncSession, tenant_id: str, signal_id: str, acknowledged_by: str | None) -> AcknowledgeResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,_get_feedback_service | acknowledge_signal,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,25,Acknowledge a signal.
activity,L5,activity_facade,ActivityFacade.get_attention_queue,"async get_attention_queue(session: AsyncSession, tenant_id: str) -> AttentionQueueResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,_get_attention_service | get_attention_queue,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,28,Get attention ranking (SIG-O5).
activity,L5,activity_facade,ActivityFacade.get_completed_runs,"async get_completed_runs(session: AsyncSession, tenant_id: str) -> CompletedRunsResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,RunsResult | _get_runs_with_policy_context,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,46,Get completed runs with policy context (V2).
activity,L5,activity_facade,ActivityFacade.get_cost_analysis,"async get_cost_analysis(session: AsyncSession, tenant_id: str) -> CostAnalysisResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,_get_cost_service | analyze_costs,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,31,Analyze cost anomalies (SIG-O4).
activity,L5,activity_facade,ActivityFacade.get_live_runs,"async get_live_runs(session: AsyncSession, tenant_id: str) -> LiveRunsResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,RunsResult | _get_runs_with_policy_context,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,46,Get live runs with policy context (V2).
activity,L5,activity_facade,ActivityFacade.get_metrics,"async get_metrics(session: AsyncSession, tenant_id: str) -> MetricsResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,MetricsResult | _get_driver | append | fetch_metrics | join,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,63,Get activity metrics (V2).
activity,L5,activity_facade,ActivityFacade.get_patterns,"async get_patterns(session: AsyncSession, tenant_id: str) -> PatternDetectionResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,_get_pattern_service | detect_patterns,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,31,Detect instability patterns (SIG-O3).
activity,L5,activity_facade,ActivityFacade.get_risk_signals,"async get_risk_signals(session: AsyncSession, tenant_id: str) -> RiskSignalsResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,RiskSignalsResult | get_metrics,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,32,Get risk signal aggregates.
activity,L5,activity_facade,ActivityFacade.get_run_detail,"async get_run_detail(session: AsyncSession, tenant_id: str, run_id: str) -> RunDetailResult | None",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,RunDetailResult | _get_driver | fetch_run_detail | float | get,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,52,Get run detail (O3).
activity,L5,activity_facade,ActivityFacade.get_run_evidence,"async get_run_evidence(session: AsyncSession, tenant_id: str, run_id: str) -> RunEvidenceResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,RunEvidenceResult,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,21,Get run evidence context (O4).
activity,L5,activity_facade,ActivityFacade.get_run_proof,"async get_run_proof(session: AsyncSession, tenant_id: str, run_id: str, include_payloads: bool) -> RunProofResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,RunProofResult,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,31,Get run integrity proof (O5).
activity,L5,activity_facade,ActivityFacade.get_runs,"async get_runs(session: AsyncSession, tenant_id: str) -> RunListResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,RunListResult | RunSummaryResult | _get_driver | append | count_runs | fetch_runs | float | isoformat | join | len | lower,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,181,List runs with filters.
activity,L5,activity_facade,ActivityFacade.get_signals,"async get_signals(session: AsyncSession, tenant_id: str) -> SignalsResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,PolicyContextResult | SignalProjectionResult | SignalsResult | _compute_severity | _compute_signal_summary | _compute_signal_type | _get_driver | append | compute_signal_fingerprint_from_row | count_runs | dict | fetch_at_risk_runs | float | get | items,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,96,Get synthesized attention signals (V2).
activity,L5,activity_facade,ActivityFacade.get_status_summary,"async get_status_summary(session: AsyncSession, tenant_id: str, project_id: str | None) -> StatusSummaryResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,StatusCount | StatusSummaryResult | _get_driver | append | fetch_status_summary | join | sum,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,36,Get summary by status (COMP-O3).
activity,L5,activity_facade,ActivityFacade.get_threshold_signals,"async get_threshold_signals(session: AsyncSession, tenant_id: str) -> ThresholdSignalsResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,PolicyContextResult | ThresholdSignalResult | ThresholdSignalsResult | _get_driver | append | count_runs | fetch_threshold_signals | float | get | items | join,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,84,Get threshold proximity signals (V2).
activity,L5,activity_facade,ActivityFacade.suppress_signal,"async suppress_signal(session: AsyncSession, tenant_id: str, signal_id: str, suppressed_by: str | None, duration_hours: int, reason: str | None) -> SuppressResult",?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,_get_feedback_service | suppress_signal,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,yes,31,Suppress a signal.
activity,L5,activity_facade,get_activity_facade,get_activity_facade() -> ActivityFacade,?:activity | L4:activity_handler | L4:activity_bridge | L3:customer_activity_adapter | L5:__init__,ActivityFacade,__future__ | activity_enums | activity_read_driver | asyncio | attention_ranking_engine | cost_analysis_engine | pattern_detection_engine | signal_feedback_engine | signal_identity,pure,no,6,Get the singleton ActivityFacade instance.
activity,L5,attention_ranking_engine,AttentionRankingService.__init__,__init__() -> None,L5:activity_facade,,time,pure,no,2,
activity,L5,attention_ranking_engine,AttentionRankingService.compute_attention_score,"async compute_attention_score(signal_type: str, severity: float, recency_hours: float, pattern_frequency: int) -> float",L5:activity_facade,max | min,time,pure,yes,14,Compute attention score for a signal.
activity,L5,attention_ranking_engine,AttentionRankingService.get_attention_queue,async get_attention_queue(tenant_id: str) -> AttentionQueueResult,L5:activity_facade,AttentionQueueResult | utc_now,time,pure,yes,15,Get prioritized attention queue for tenant.
activity,L5,cost_analysis_engine,CostAnalysisService.__init__,__init__() -> None,L5:activity_facade,,time,pure,no,2,
activity,L5,cost_analysis_engine,CostAnalysisService.analyze_costs,async analyze_costs(tenant_id: str) -> CostAnalysisResult,L5:activity_facade,CostAnalysisResult | utc_now,time,pure,yes,15,Analyze costs and detect anomalies.
activity,L5,cost_analysis_engine,CostAnalysisService.get_cost_breakdown,"async get_cost_breakdown(tenant_id: str) -> dict[str, float]",L5:activity_facade,,time,pure,yes,9,Get cost breakdown by dimension.
activity,L5,cus_telemetry_engine,CusTelemetryEngine.__init__,__init__(driver: Any),?:cus_telemetry_service | L4:activity_handler,,,pure,no,2,
activity,L5,cus_telemetry_engine,CusTelemetryEngine.get_usage,"async get_usage(tenant_id: str, **kwargs) -> List[Dict[str, Any]]",?:cus_telemetry_service | L4:activity_handler,NotImplementedError,,pure,yes,2,
activity,L5,cus_telemetry_engine,CusTelemetryEngine.get_usage_summary,"async get_usage_summary(tenant_id: str, **kwargs) -> Dict[str, Any]",?:cus_telemetry_service | L4:activity_handler,NotImplementedError,,pure,yes,2,
activity,L5,cus_telemetry_engine,CusTelemetryEngine.ingest_batch,"async ingest_batch(tenant_id: str, default_integration_id: Optional[str], records: List[Any]) -> BatchIngestResult",?:cus_telemetry_service | L4:activity_handler,NotImplementedError,,pure,yes,2,
activity,L5,cus_telemetry_engine,CusTelemetryEngine.ingest_usage,"async ingest_usage(tenant_id: str, integration_id: str, payload: Any) -> IngestResult",?:cus_telemetry_service | L4:activity_handler,NotImplementedError,,pure,yes,2,
activity,L5,cus_telemetry_engine,get_cus_telemetry_engine,get_cus_telemetry_engine() -> CusTelemetryEngine,?:cus_telemetry_service | L4:activity_handler,CusTelemetryEngine,,pure,no,3,Stub factory  returns disconnected engine.
activity,L5,cus_telemetry_engine,get_cus_telemetry_service,get_cus_telemetry_service() -> CusTelemetryService,?:cus_telemetry_service | L4:activity_handler,get_cus_telemetry_engine,,pure,no,11,Get the CusTelemetryService instance.
activity,L5,pattern_detection_engine,PatternDetectionService.__init__,__init__() -> None,L5:activity_facade,,time,pure,no,2,
activity,L5,pattern_detection_engine,PatternDetectionService.detect_patterns,async detect_patterns(tenant_id: str) -> PatternDetectionResult,L5:activity_facade,PatternDetectionResult | utc_now,time,pure,yes,16,Detect patterns in recent activity.
activity,L5,pattern_detection_engine,PatternDetectionService.get_pattern_detail,"async get_pattern_detail(tenant_id: str, pattern_id: str) -> Optional[DetectedPattern]",L5:activity_facade,,time,pure,yes,7,Get details of a specific pattern.
activity,L5,signal_feedback_engine,SignalFeedbackService.__init__,__init__() -> None,L4:activity_handler | L5:activity_facade,,time,pure,no,2,
activity,L5,signal_feedback_engine,SignalFeedbackService.acknowledge_signal,"async acknowledge_signal(tenant_id: str, signal_id: str) -> AcknowledgeResult",L4:activity_handler | L5:activity_facade,AcknowledgeResult | utc_now,time,pure,yes,15,Acknowledge a signal.
activity,L5,signal_feedback_engine,SignalFeedbackService.get_bulk_signal_feedback,"async get_bulk_signal_feedback(tenant_id: str, signal_ids: list[str]) -> dict[str, SignalFeedbackStatus]",L4:activity_handler | L5:activity_facade,,time,pure,yes,14,Get feedback status for multiple signals in bulk.
activity,L5,signal_feedback_engine,SignalFeedbackService.get_signal_feedback_status,"async get_signal_feedback_status(tenant_id: str, signal_id: str) -> dict[str, bool]",L4:activity_handler | L5:activity_facade,,time,pure,yes,10,Get current feedback status for a signal.
activity,L5,signal_feedback_engine,SignalFeedbackService.suppress_signal,"async suppress_signal(tenant_id: str, signal_id: str) -> SuppressResult",L4:activity_handler | L5:activity_facade,SuppressResult | timedelta | utc_now,time,pure,yes,23,Suppress a signal for a duration.
activity,L5,signal_identity,compute_signal_fingerprint,"compute_signal_fingerprint(signal_type: str, dimension: str, source: str, tenant_id: str) -> str",?:activity | ?:__init__ | ?:activity_facade | L4:activity_handler | L5:activity_facade | ?:test_signal_feedback,compute_signal_fingerprint_from_row,,pure,no,24,Compute a stable fingerprint for signal identity fields.
activity,L5,signal_identity,compute_signal_fingerprint_from_row,"compute_signal_fingerprint_from_row(row: dict[str, Any]) -> str",?:activity | ?:__init__ | ?:activity_facade | L4:activity_handler | L5:activity_facade | ?:test_signal_feedback,dumps | encode | get | hexdigest | sha256,,pure,no,28,Compute a stable fingerprint for a signal row.
activity,L6,activity_read_driver,ActivityReadDriver.__init__,__init__(session: AsyncSession),L5:activity_facade,,asyncio | sqlalchemy,pure,no,3,Initialize driver with async session.
activity,L6,activity_read_driver,ActivityReadDriver.count_runs,"async count_runs(where_sql: str, params: dict[str, Any]) -> int",L5:activity_facade,execute | scalar | text,asyncio | sqlalchemy,db_write,yes,18,Count runs matching filters.
activity,L6,activity_read_driver,ActivityReadDriver.fetch_at_risk_runs,"async fetch_at_risk_runs(where_sql: str, params: dict[str, Any], limit: int, offset: int) -> list[dict[str, Any]]",L5:activity_facade,all | dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,42,Fetch at-risk runs for signal synthesis.
activity,L6,activity_read_driver,ActivityReadDriver.fetch_metrics,"async fetch_metrics(where_sql: str, params: dict[str, Any]) -> dict[str, Any]",L5:activity_facade,dict | execute | first | mappings | text,asyncio | sqlalchemy,db_write,yes,36,Fetch aggregated metrics.
activity,L6,activity_read_driver,ActivityReadDriver.fetch_run_detail,"async fetch_run_detail(tenant_id: str, run_id: str) -> dict[str, Any] | None",L5:activity_facade,dict | execute | first | mappings | text,asyncio | sqlalchemy,db_write,yes,25,Fetch single run detail.
activity,L6,activity_read_driver,ActivityReadDriver.fetch_runs,"async fetch_runs(where_sql: str, params: dict[str, Any], sort_by: str, sort_dir: str, limit: int, offset: int) -> list[dict[str, Any]]",L5:activity_facade,all | dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,38,Fetch runs matching filters.
activity,L6,activity_read_driver,ActivityReadDriver.fetch_runs_with_policy_context,"async fetch_runs_with_policy_context(where_sql: str, params: dict[str, Any], sort_by: str, sort_dir: str, limit: int, offset: int) -> list[dict[str, Any]]",L5:activity_facade,all | dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,49,Fetch runs with policy context columns.
activity,L6,activity_read_driver,ActivityReadDriver.fetch_status_summary,"async fetch_status_summary(where_sql: str, params: dict[str, Any]) -> list[dict[str, Any]]",L5:activity_facade,all | dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,24,Fetch runs grouped by status.
activity,L6,activity_read_driver,ActivityReadDriver.fetch_threshold_signals,"async fetch_threshold_signals(where_sql: str, params: dict[str, Any], limit: int, offset: int) -> list[dict[str, Any]]",L5:activity_facade,all | dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,41,Fetch threshold proximity signals.
activity,L6,activity_read_driver,get_activity_read_driver,get_activity_read_driver(session: AsyncSession) -> ActivityReadDriver,L5:activity_facade,ActivityReadDriver,asyncio | sqlalchemy,pure,no,3,Get an ActivityReadDriver instance.
activity,L6,orphan_recovery_driver,detect_orphaned_runs,"async detect_orphaned_runs(session: AsyncSession, threshold_minutes: int) -> list[WorkerRun]",,all | asc | execute | in_ | list | order_by | scalars | select | timedelta | utcnow | where,asyncio | db | infra | sqlalchemy | tenant,db_write,yes,23,Detect runs that appear to be orphaned.
activity,L6,orphan_recovery_driver,get_crash_recovery_summary,async get_crash_recovery_summary() -> dict,,all | count | desc | execute | get_async_session | isoformat | limit | order_by | scalar | scalars | select | select_from | where,asyncio | db | infra | sqlalchemy | tenant,db_write,yes,35,Get a summary of crashed runs for operator visibility.
activity,L6,orphan_recovery_driver,mark_run_as_crashed,"async mark_run_as_crashed(session: AsyncSession, run: WorkerRun, reason: str) -> bool",,error | execute | info | isoformat | str | update | utcnow | values | where,asyncio | db | infra | sqlalchemy | tenant,db_write,yes,44,Mark a run as crashed.
activity,L6,orphan_recovery_driver,recover_orphaned_runs,async recover_orphaned_runs(threshold_minutes: Optional[int]) -> dict,,append | detect_orphaned_runs | error | get_async_session | info | len | mark_run_as_crashed | str | warning,asyncio | db | infra | sqlalchemy | tenant,pure,yes,86,Main recovery function - called on startup.
activity,L6,run_signal_driver,RunSignalDriver.__init__,__init__(session: Any),L4:signal_coordinator,,sqlalchemy,pure,no,8,Initialize with a sync SQLAlchemy Session.
activity,L6,run_signal_driver,RunSignalDriver.get_risk_level,get_risk_level(run_id: str) -> int,L4:signal_coordinator,execute | fetchone | str | text | warning,sqlalchemy,db_write,no,35,Get current risk level for a run.
activity,L6,run_signal_driver,RunSignalDriver.update_risk_level,"update_risk_level(run_id: str, signals: List[Any]) -> None",L4:signal_coordinator,debug | error | execute | get | hasattr | info | len | max | str | text,sqlalchemy,db_write,no,63,Update risk level for a run based on threshold signals.
analytics,L5,ai_console_panel_engine,AIConsolePanelEngine.__init__,"__init__(spec_loader: Optional[PanelSpecLoader], signal_collector: Optional[PanelSignalCollector], metrics_emitter: Optional[PanelMetricsEmitter], api_base_url: Optional[str])",?:__init__,PanelDependencyResolver | PanelSlotEvaluator | PanelVerificationEngine | create_consistency_checker | create_response_assembler | create_signal_collector | get | get_panel_metrics_emitter | get_panel_spec_loader | info | len | load,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,no,30,
analytics,L5,ai_console_panel_engine,AIConsolePanelEngine._create_short_circuit_response,"_create_short_circuit_response(panel_id: str, panel_spec, blocking_upstream: str, params: Dict[str, Any], start_time: float) -> Dict[str, Any]",?:__init__,assemble | check | evaluate_missing | perf_counter | values,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,no,30,Create response when short-circuiting due to upstream failure.
analytics,L5,ai_console_panel_engine,AIConsolePanelEngine._evaluate_panel_slots,"async _evaluate_panel_slots(panel_spec, params: Dict[str, Any]) -> List[PanelSlotResult]",?:__init__,append | check_determinism_rule | collect_for_slot | determine_authority | determine_state | error | evaluate | evaluate_missing | get | items | str | verify_inputs | warning,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,yes,70,Evaluate all slots in a panel.
analytics,L5,ai_console_panel_engine,AIConsolePanelEngine.close,async close(),?:__init__,close,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,yes,3,Clean up resources.
analytics,L5,ai_console_panel_engine,AIConsolePanelEngine.evaluate_all_panels,"async evaluate_all_panels(params: Optional[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]",?:__init__,evaluate_panel | get_all_tiers,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,yes,22,Evaluate all panels in dependency order.
analytics,L5,ai_console_panel_engine,AIConsolePanelEngine.evaluate_panel,"async evaluate_panel(panel_id: str, params: Optional[Dict[str, Any]]) -> Dict[str, Any]",?:__init__,_create_short_circuit_response | _evaluate_panel_slots | any | assemble | assemble_error | can_short_circuit | check | debug | error | get | info | items | measure_evaluation | perf_counter | record_error,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,yes,82,Evaluate a panel and return spec-compliant response.
analytics,L5,ai_console_panel_engine,AIConsolePanelEngine.get_panel_ids,async get_panel_ids() -> List[str],?:__init__,keys | list,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,yes,3,Get all registered panel IDs.
analytics,L5,ai_console_panel_engine,AIConsolePanelEngine.get_panel_spec,"async get_panel_spec(panel_id: str) -> Optional[Dict[str, Any]]",?:__init__,get | keys | len | list,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,yes,16,Get spec for a specific panel.
analytics,L5,ai_console_panel_engine,create_panel_engine,async create_panel_engine(api_base_url: Optional[str]) -> AIConsolePanelEngine,?:__init__,AIConsolePanelEngine,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,yes,5,Create and initialize panel engine.
analytics,L5,ai_console_panel_engine,get_panel_engine,async get_panel_engine() -> AIConsolePanelEngine,?:__init__,create_panel_engine,panel_consistency_checker | panel_dependency_resolver | panel_metrics_emitter | panel_response_assembler | panel_signal_collector | panel_slot_evaluator | panel_spec_loader | panel_types | panel_verification_engine,pure,yes,6,Get singleton panel engine.
analytics,L5,analytics_facade,AnalyticsFacade.__init__,__init__() -> None,?:analytics | L4:analytics_handler | L5s:query_types,,__future__ | analytics_read_driver | asyncio | query_types,pure,no,3,Initialize facade.
analytics,L5,analytics_facade,AnalyticsFacade._calculate_freshness,_calculate_freshness(series: list[UsageDataPointResult]) -> int,?:analytics | L4:analytics_handler | L5s:query_types,fromisoformat | int | now | replace | total_seconds,__future__ | analytics_read_driver | asyncio | query_types,pure,no,13,Calculate data freshness in seconds.
analytics,L5,analytics_facade,AnalyticsFacade._calculate_freshness_from_cost,_calculate_freshness_from_cost(series: list[CostDataPointResult]) -> int,?:analytics | L4:analytics_handler | L5s:query_types,fromisoformat | int | now | replace | total_seconds,__future__ | analytics_read_driver | asyncio | query_types,pure,no,13,Calculate data freshness in seconds from cost series.
analytics,L5,analytics_facade,AnalyticsFacade.get_cost_statistics,"async get_cost_statistics(session: AsyncSession, tenant_id: str, from_ts: datetime, to_ts: datetime) -> CostStatisticsResult",?:analytics | L4:analytics_handler | L5s:query_types,CostByFeatureResult | CostByModelResult | CostDataPointResult | CostStatisticsResult | CostTotalsResult | SignalSourceResult | TimeWindowResult | _calculate_freshness_from_cost | append | fetch_cost_by_feature | fetch_cost_by_model | fetch_cost_spend | get | max | round,__future__ | analytics_read_driver | asyncio | query_types,pure,yes,115,Get cost statistics for the specified time window.
analytics,L5,analytics_facade,AnalyticsFacade.get_status,get_status() -> AnalyticsStatusResult,?:analytics | L4:analytics_handler | L5s:query_types,AnalyticsStatusResult | TopicStatusResult,__future__ | analytics_read_driver | asyncio | query_types,pure,no,23,Get analytics domain capability status.
analytics,L5,analytics_facade,AnalyticsFacade.get_usage_statistics,"async get_usage_statistics(session: AsyncSession, tenant_id: str, from_ts: datetime, to_ts: datetime) -> UsageStatisticsResult",?:analytics | L4:analytics_handler | L5s:query_types,SignalSourceResult | TimeWindowResult | UsageDataPointResult | UsageStatisticsResult | UsageTotalsResult | _calculate_freshness | append | fetch_cost_metrics | fetch_llm_usage | fetch_worker_execution | get | keys | max | sorted | split,__future__ | analytics_read_driver | asyncio | query_types,pure,yes,117,Get usage statistics for the specified time window.
analytics,L5,analytics_facade,SignalAdapter.fetch_cost_by_feature,"async fetch_cost_by_feature(session: AsyncSession, tenant_id: str, from_ts: datetime, to_ts: datetime) -> dict[str, Any]",?:analytics | L4:analytics_handler | L5s:query_types,fetch_cost_by_feature | get_analytics_read_driver | str | warning,__future__ | analytics_read_driver | asyncio | query_types,pure,yes,14,Fetch cost breakdown by feature tag from cost_records table.
analytics,L5,analytics_facade,SignalAdapter.fetch_cost_by_model,"async fetch_cost_by_model(session: AsyncSession, tenant_id: str, from_ts: datetime, to_ts: datetime) -> dict[str, Any]",?:analytics | L4:analytics_handler | L5s:query_types,fetch_cost_by_model | get_analytics_read_driver | str | warning,__future__ | analytics_read_driver | asyncio | query_types,pure,yes,14,Fetch cost breakdown by model from cost_records table.
analytics,L5,analytics_facade,SignalAdapter.fetch_cost_metrics,"async fetch_cost_metrics(session: AsyncSession, tenant_id: str, from_ts: datetime, to_ts: datetime, resolution: ResolutionType) -> dict[str, Any]",?:analytics | L4:analytics_handler | L5s:query_types,fetch_cost_metrics | get_analytics_read_driver | str | warning,__future__ | analytics_read_driver | asyncio | query_types,pure,yes,16,Fetch cost metrics from cost_records table.
analytics,L5,analytics_facade,SignalAdapter.fetch_cost_spend,"async fetch_cost_spend(session: AsyncSession, tenant_id: str, from_ts: datetime, to_ts: datetime, resolution: ResolutionType) -> dict[str, Any]",?:analytics | L4:analytics_handler | L5s:query_types,fetch_cost_spend | get_analytics_read_driver | str | warning,__future__ | analytics_read_driver | asyncio | query_types,pure,yes,16,Fetch cost spend data from cost_records table.
analytics,L5,analytics_facade,SignalAdapter.fetch_llm_usage,"async fetch_llm_usage(session: AsyncSession, tenant_id: str, from_ts: datetime, to_ts: datetime, resolution: ResolutionType) -> dict[str, Any]",?:analytics | L4:analytics_handler | L5s:query_types,fetch_llm_usage | get_analytics_read_driver | str | warning,__future__ | analytics_read_driver | asyncio | query_types,pure,yes,16,Fetch LLM usage from runs table.
analytics,L5,analytics_facade,SignalAdapter.fetch_worker_execution,"async fetch_worker_execution(session: AsyncSession, tenant_id: str, from_ts: datetime, to_ts: datetime, resolution: ResolutionType) -> dict[str, Any]",?:analytics | L4:analytics_handler | L5s:query_types,fetch_worker_execution | get_analytics_read_driver | str | warning,__future__ | analytics_read_driver | asyncio | query_types,pure,yes,16,Fetch worker execution metrics from aos_traces table.
analytics,L5,analytics_facade,get_analytics_facade,get_analytics_facade() -> AnalyticsFacade,?:analytics | L4:analytics_handler | L5s:query_types,AnalyticsFacade,__future__ | analytics_read_driver | asyncio | query_types,pure,no,6,Get the singleton AnalyticsFacade instance.
analytics,L5,canary_engine,CanaryRunner.__init__,__init__(config: Optional[CanaryRunConfig]),,CanaryRunConfig | CostSimV2Adapter | CostSimulator | Path | get_config | mkdir,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,no,25,Initialize canary runner.
analytics,L5,canary_engine,CanaryRunner._approximate_kl_divergence,"_approximate_kl_divergence(p: List[int], q: List[int], bins: int) -> float",,int | log | max | min | round | sum | zip,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,no,55,Approximate KL divergence between two cost distributions.
analytics,L5,canary_engine,CanaryRunner._calculate_metrics,"_calculate_metrics(comparisons: List[ComparisonResult]) -> Dict[str, Any]",,_approximate_kl_divergence | abs | float | int | len | sorted | sum,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,no,46,Calculate aggregate metrics from comparisons.
analytics,L5,canary_engine,CanaryRunner._compare_with_golden,"async _compare_with_golden(samples: List[CanarySample], comparisons: List[ComparisonResult]) -> Dict[str, Any]",,,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,yes,11,Compare results against golden reference dataset.
analytics,L5,canary_engine,CanaryRunner._evaluate_results,"_evaluate_results(metrics: Dict[str, Any], golden_comparison: Optional[Dict[str, Any]]) -> Tuple[bool, List[str]]",,append | len,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,no,27,Evaluate results and determine pass/fail.
analytics,L5,canary_engine,CanaryRunner._generate_synthetic_samples,_generate_synthetic_samples() -> List[CanarySample],,CanarySample | append,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,no,48,Generate synthetic test samples.
analytics,L5,canary_engine,CanaryRunner._load_samples,async _load_samples() -> List[CanarySample],,CanarySample | _generate_synthetic_samples | append | error | get | get_decompressed_input | get_provenance_logger | now | query | timedelta | warning,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,yes,41,Load samples from recent provenance logs.
analytics,L5,canary_engine,CanaryRunner._run_internal,"async _run_internal(run_id: str, start_time: datetime, samples: Optional[List[CanarySample]]) -> CanaryReport",,CanaryReport | _calculate_metrics | _compare_with_golden | _evaluate_results | _load_samples | _run_single | _save_artifacts | append | error | exists | gather | get_circuit_breaker | info | isinstance | len,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,yes,128,Internal canary run logic (called after leader election).
analytics,L5,canary_engine,CanaryRunner._run_single,"async _run_single(sample: CanarySample) -> Tuple[ComparisonResult, Optional[DiffResult]]",,CostSimV2Adapter | DiffResult | compute_output_hash | simulate_with_comparison,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,yes,27,Run comparison for a single sample.
analytics,L5,canary_engine,CanaryRunner._save_artifacts,"async _save_artifacts(report: CanaryReport, comparisons: List[ComparisonResult], diffs: List[DiffResult]) -> List[str]",,append | dump | dumps | error | open | str | strftime | to_dict | write,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,file_io,yes,36,Save canary run artifacts.
analytics,L5,canary_engine,CanaryRunner.run,async run(samples: Optional[List[CanarySample]]) -> CanaryReport,,CanaryReport | _run_internal | info | leader_election | now | str | uuid4,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,yes,48,Run canary validation.
analytics,L5,canary_engine,run_canary,"async run_canary(sample_count: int, drift_threshold: float) -> CanaryReport",,CanaryRunConfig | CanaryRunner | run,__future__ | circuit_breaker | circuit_breaker_async | config | leader | models | provenance | simulate | v2_adapter,pure,yes,20,Convenience function to run canary.
analytics,L5,config_engine,CostSimConfig.from_env,from_env() -> 'CostSimConfig',,cls | float | getenv | int | lower,__future__ | subprocess,pure,no,26,Load configuration from environment variables.
analytics,L5,config_engine,get_commit_sha,get_commit_sha() -> str,,getenv | run | strip,__future__ | subprocess,pure,no,16,Get current git commit SHA.
analytics,L5,config_engine,get_config,get_config() -> CostSimConfig,,from_env,__future__ | subprocess,pure,no,6,Get the global CostSim configuration.
analytics,L5,config_engine,is_v2_disabled_by_drift,is_v2_disabled_by_drift() -> bool,,exists | get_config,__future__ | subprocess,pure,no,4,Check if V2 was auto-disabled due to drift.
analytics,L5,config_engine,is_v2_sandbox_enabled,is_v2_sandbox_enabled() -> bool,,exists | get_config | warning,__future__ | subprocess,pure,no,19,Check if V2 sandbox is enabled.
analytics,L5,coordinator_engine,CoordinationError.__init__,"__init__(message: str, envelope_id: str)",,__init__ | super,audit_persistence | envelope | infra | sqlmodel,pure,no,4,
analytics,L5,coordinator_engine,CoordinationManager.__init__,"__init__(db: Optional[Session], emit_traces: bool, tenant_id: Optional[str]) -> None",,,audit_persistence | envelope | infra | sqlmodel,pure,no,33,Initialize CoordinationManager.
analytics,L5,coordinator_engine,CoordinationManager._emit_audit_record,"_emit_audit_record(envelope: Envelope, decision: CoordinationDecisionType, reason: str, conflicting_envelope_id: Optional[str], preempting_envelope_id: Optional[str]) -> CoordinationAuditRecord",,CoordinationAuditRecord | append | info | now | persist_audit_record | str | uuid4,audit_persistence | envelope | infra | sqlmodel,pure,no,67,Emit a coordination audit record (I-C4-7).
analytics,L5,coordinator_engine,CoordinationManager._find_preemption_targets,_find_preemption_targets(incoming: Envelope) -> List[Envelope],,append | has_higher_priority | values,audit_persistence | envelope | infra | sqlmodel,pure,no,23,Find envelopes that would be preempted by the incoming envelope.
analytics,L5,coordinator_engine,CoordinationManager._get_parameter_key,_get_parameter_key(envelope: Envelope) -> str,,,audit_persistence | envelope | infra | sqlmodel,pure,no,3,Get the canonical key for parameter indexing.
analytics,L5,coordinator_engine,CoordinationManager._revert_envelope,"_revert_envelope(envelope: Envelope, reason: RevertReason, preempting_envelope_id: Optional[str]) -> None",,_emit_audit_record | _get_parameter_key | info | now,audit_persistence | envelope | infra | sqlmodel,pure,no,41,Revert a single envelope.
analytics,L5,coordinator_engine,CoordinationManager.active_envelope_count,active_envelope_count() -> int,,len,audit_persistence | envelope | infra | sqlmodel,pure,no,3,Get count of currently active envelopes.
analytics,L5,coordinator_engine,CoordinationManager.apply,"apply(envelope: Envelope) -> Tuple[bool, Optional[List[str]]]",,CoordinationError | _emit_audit_record | _find_preemption_targets | _get_parameter_key | _revert_envelope | append | check_allowed | info | len | now,audit_persistence | envelope | infra | sqlmodel,pure,no,63,Apply an envelope after coordination check.
analytics,L5,coordinator_engine,CoordinationManager.check_allowed,check_allowed(envelope: Envelope) -> CoordinationDecision,,CoordinationDecision | _emit_audit_record | _find_preemption_targets | _get_parameter_key,audit_persistence | envelope | infra | sqlmodel,pure,no,74,Check if an envelope is allowed to apply (I-C4-1).
analytics,L5,coordinator_engine,CoordinationManager.expire_envelope,expire_envelope(envelope_id: str) -> bool,,_get_parameter_key | info | now,audit_persistence | envelope | infra | sqlmodel,pure,no,32,Mark an envelope as expired (timebox ended).
analytics,L5,coordinator_engine,CoordinationManager.get_active_envelopes,get_active_envelopes() -> List[Envelope],,list | values,audit_persistence | envelope | infra | sqlmodel,pure,no,3,Get list of currently active envelopes (read-only copy).
analytics,L5,coordinator_engine,CoordinationManager.get_audit_trail,get_audit_trail() -> List[CoordinationAuditRecord],,list,audit_persistence | envelope | infra | sqlmodel,pure,no,3,Get audit trail (read-only copy).
analytics,L5,coordinator_engine,CoordinationManager.get_coordination_stats,get_coordination_stats() -> Dict,,get_envelopes_by_class | keys | len | list,audit_persistence | envelope | infra | sqlmodel,pure,no,18,Get current coordination statistics.
analytics,L5,coordinator_engine,CoordinationManager.get_envelope_for_parameter,"get_envelope_for_parameter(subsystem: str, parameter: str) -> Optional[Envelope]",,get,audit_persistence | envelope | infra | sqlmodel,pure,no,20,Get the active envelope controlling a specific parameter.
analytics,L5,coordinator_engine,CoordinationManager.get_envelopes_by_class,get_envelopes_by_class(envelope_class: EnvelopeClass) -> List[Envelope],,values,audit_persistence | envelope | infra | sqlmodel,pure,no,11,Get all active envelopes of a specific class.
analytics,L5,coordinator_engine,CoordinationManager.is_kill_switch_active,is_kill_switch_active() -> bool,,,audit_persistence | envelope | infra | sqlmodel,pure,no,3,Check if kill-switch is currently active.
analytics,L5,coordinator_engine,CoordinationManager.kill_switch,kill_switch() -> List[str],,_revert_envelope | append | len | list | values | warning,audit_persistence | envelope | infra | sqlmodel,pure,no,31,Activate kill-switch: revert ALL active envelopes atomically (I-C4-6).
analytics,L5,coordinator_engine,CoordinationManager.reset_kill_switch,reset_kill_switch() -> None,,warning,audit_persistence | envelope | infra | sqlmodel,pure,no,9,Reset kill-switch state (for testing/recovery).
analytics,L5,coordinator_engine,CoordinationManager.revert,"revert(envelope_id: str, reason: RevertReason) -> bool",,_revert_envelope | warning,audit_persistence | envelope | infra | sqlmodel,pure,no,21,Explicitly revert a single envelope.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector.__init__,__init__(session),L4:anomaly_incident_coordinator | L5:detection_facade,get_cost_anomaly_driver | get_cost_anomaly_read_driver | today,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,no,12,
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector._check_budget_threshold,"_check_budget_threshold(budget_type: str, entity_id: Optional[str], period: str, current_cents: float, limit_cents: int, warn_threshold_pct: int) -> Optional[DetectedAnomaly]",L4:anomaly_incident_coordinator | L5:detection_facade,DetectedAnomaly | float | title,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,no,47,Check if budget threshold is breached.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector._derive_cause,"_derive_cause(tenant_id: str, entity_type: str, entity_id: str) -> DerivedCause",L4:anomaly_incident_coordinator | L5:detection_facade,combine | fetch_feature_concentration | fetch_prompt_comparison | fetch_request_comparison | fetch_retry_comparison | time | timedelta,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,no,78,Derive the cause of a cost anomaly.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector._detect_entity_spikes,"async _detect_entity_spikes(tenant_id: str, entity_type: str, column_name: str, lookback_days: int) -> List[DetectedAnomaly]",L4:anomaly_incident_coordinator | L5:detection_facade,DetectedAnomaly | _derive_cause | _format_spike_message | _record_breach_and_get_consecutive_count | _reset_breach_history | append | classify_severity | combine | fetch_entity_baseline | fetch_entity_today_spend | get | items | time | timedelta,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,yes,87,Detect spikes for a specific entity type (user or feature).
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector._detect_tenant_spike,"async _detect_tenant_spike(tenant_id: str, lookback_days: int) -> List[DetectedAnomaly]",L4:anomaly_incident_coordinator | L5:detection_facade,DetectedAnomaly | _derive_cause | _format_spike_message | _record_breach_and_get_consecutive_count | _reset_breach_history | append | classify_severity | combine | fetch_tenant_baseline | fetch_tenant_today_spend | time | timedelta,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,yes,75,Detect tenant-level absolute spikes.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector._format_spike_message,"_format_spike_message(entity_type: str, entity_id: str, deviation_pct: float, breach_count: int) -> str",L4:anomaly_incident_coordinator | L5:detection_facade,title,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,no,13,Format human-readable spike message.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector._record_breach_and_get_consecutive_count,"_record_breach_and_get_consecutive_count(tenant_id: str, entity_type: str, entity_id: str, breach_type: str, deviation_pct: float, current_value: float, baseline_value: float) -> int",L4:anomaly_incident_coordinator | L5:detection_facade,fetch_breach_exists_today | fetch_consecutive_breaches | insert_breach_history | utc_now | uuid4,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,no,49,Record a breach and return the consecutive breach count.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector._reset_breach_history,"_reset_breach_history(tenant_id: str, entity_type: str, entity_id: str, breach_type: str) -> None",L4:anomaly_incident_coordinator | L5:detection_facade,,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,no,11,Reset breach history when entity is no longer breaching.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector._reset_drift_tracking,"_reset_drift_tracking(tenant_id: str, entity_type: str, entity_id: str) -> None",L4:anomaly_incident_coordinator | L5:detection_facade,reset_drift_tracking | utc_now,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,no,17,Mark drift tracking as inactive.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector._update_drift_tracking,"_update_drift_tracking(tenant_id: str, entity_type: str, entity_id: str, rolling_avg: float, baseline_avg: float, drift_pct: float) -> int",L4:anomaly_incident_coordinator | L5:detection_facade,fetch_drift_tracking | insert_drift_tracking | int | timedelta | update_drift_tracking | utc_now | uuid4,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,no,61,Update drift tracking and return days count.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector.detect_absolute_spikes,"async detect_absolute_spikes(tenant_id: str, lookback_days: int) -> List[DetectedAnomaly]",L4:anomaly_incident_coordinator | L5:detection_facade,_detect_entity_spikes | _detect_tenant_spike | extend,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,yes,29,Detect absolute spikes with consecutive interval logic.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector.detect_all,async detect_all(tenant_id: str) -> List[DetectedAnomaly],L4:anomaly_incident_coordinator | L5:detection_facade,detect_absolute_spikes | detect_budget_issues | detect_sustained_drift | extend,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,yes,17,Run all anomaly detection checks for a tenant.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector.detect_budget_issues,async detect_budget_issues(tenant_id: str) -> List[DetectedAnomaly],L4:anomaly_incident_coordinator | L5:detection_facade,_check_budget_threshold | append | combine | fetch_active_budgets | fetch_daily_spend | fetch_monthly_spend | replace | time,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,yes,63,Detect budget warnings and exceeded budgets.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector.detect_sustained_drift,async detect_sustained_drift(tenant_id: str) -> List[DetectedAnomaly],L4:anomaly_incident_coordinator | L5:detection_facade,DetectedAnomaly | _derive_cause | _reset_drift_tracking | _update_drift_tracking | append | classify_severity | fetch_baseline_avg | fetch_rolling_avg | timedelta,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,yes,84,Detect sustained drift anomalies.
analytics,L5,cost_anomaly_detector_engine,CostAnomalyDetector.persist_anomalies,"async persist_anomalies(tenant_id: str, anomalies: List[DetectedAnomaly]) -> List[CostAnomaly]",L4:anomaly_incident_coordinator | L5:detection_facade,CostAnomaly | append | combine | find_existing_anomaly | flush_and_refresh | info | len | persist_anomaly | time,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,yes,58,Persist detected anomalies to database.
analytics,L5,cost_anomaly_detector_engine,_run_anomaly_detection_with_facts,"async _run_anomaly_detection_with_facts(session, tenant_id: str) -> dict",L4:anomaly_incident_coordinator | L5:detection_facade,CostAnomalyFact | append | float | info | int | run_anomaly_detection | str,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,yes,68,Run anomaly detection and emit CostAnomalyFact for HIGH anomalies.
analytics,L5,cost_anomaly_detector_engine,classify_severity,classify_severity(deviation_pct: float) -> AnomalySeverity,L4:anomaly_incident_coordinator | L5:detection_facade,,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,no,20,Classify severity based on percentage deviation.
analytics,L5,cost_anomaly_detector_engine,run_anomaly_detection,"async run_anomaly_detection(session, tenant_id: str) -> List[CostAnomaly]",L4:anomaly_incident_coordinator | L5:detection_facade,CostAnomalyDetector | debug | detect_all | info | len | persist_anomalies,__future__ | anomaly_types | cost_anomaly_driver | cost_anomaly_read_driver | db,pure,yes,14,Run anomaly detection and persist results.
analytics,L5,cost_anomaly_schemas,CostAnomalyReadProtocol.fetch_active_budgets,fetch_active_budgets(tenant_id: str) -> list,,,__future__ | db,pure,no,3,Fetch all active budgets for a tenant.
analytics,L5,cost_anomaly_schemas,CostAnomalyReadProtocol.find_existing_anomaly,"find_existing_anomaly(tenant_id: str, anomaly_type: str, entity_type: str, entity_id: Optional[str], since: datetime) -> Optional[CostAnomaly]",,,__future__ | db,pure,no,10,Find existing unresolved anomaly for deduplication.
analytics,L5,cost_anomaly_schemas,CostAnomalyReadProtocol.flush_and_refresh,flush_and_refresh(anomalies: List[CostAnomaly]) -> None,,,__future__ | db,pure,no,3,Flush to get generated IDs and refresh.
analytics,L5,cost_anomaly_schemas,CostAnomalyReadProtocol.persist_anomaly,persist_anomaly(anomaly: CostAnomaly) -> None,,,__future__ | db,pure,no,3,Add or update an anomaly record.
analytics,L5,cost_model_engine,calculate_cumulative_risk,"calculate_cumulative_risk(risks: List[Dict[str, float]]) -> float",?:v2_adapter,values,,pure,no,17,Calculate cumulative risk from individual risk factors (L4 domain function).
analytics,L5,cost_model_engine,check_feasibility,"check_feasibility(estimated_cost_cents: int, budget_cents: int, permission_gaps: List[str], cumulative_risk: float, risk_threshold: float) -> FeasibilityResult",?:v2_adapter,FeasibilityResult | len,,pure,no,44,Check if a plan is feasible (L4 domain function).
analytics,L5,cost_model_engine,classify_drift,"classify_drift(v1_cost_cents: int, v2_cost_cents: int, v1_feasible: bool, v2_feasible: bool) -> DriftAnalysis",?:v2_adapter,DriftAnalysis | abs | max | min | round,,pure,no,56,Classify drift between V1 and V2 simulation results (L4 domain function).
analytics,L5,cost_model_engine,estimate_step_cost,"estimate_step_cost(step_index: int, skill_id: str, params: Dict[str, Any]) -> StepCostEstimate",?:v2_adapter,StepCostEstimate | get | get_skill_coefficients | len | min | split | startswith | str,,pure,no,93,Estimate cost and latency for a single step (L4 domain function).
analytics,L5,cost_model_engine,get_skill_coefficients,"get_skill_coefficients(skill_id: str) -> Dict[str, float]",?:v2_adapter,get,,pure,no,11,Get cost model coefficients for a skill (L4 domain function).
analytics,L5,cost_model_engine,is_significant_risk,is_significant_risk(probability: float) -> bool,?:v2_adapter,,,pure,no,3,Check if a risk factor is significant enough to report (L4 domain function).
analytics,L5,cost_snapshot_schemas,CostSnapshot.create,"create(tenant_id: str, snapshot_type: SnapshotType, period_start: datetime, period_end: datetime) -> 'CostSnapshot'",L6:cost_snapshots_driver | L5:cost_snapshots_engine,cls | encode | hexdigest | isoformat | sha256,__future__,pure,no,17,Create a new snapshot in pending status.
analytics,L5,cost_snapshot_schemas,CostSnapshot.to_dict,"to_dict() -> dict[str, Any]",L6:cost_snapshots_driver | L5:cost_snapshots_engine,isinstance | isoformat,__future__,pure,no,15,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.aggregate_cost_records,"async aggregate_cost_records(tenant_id: str, snapshot_id: str, period_start: datetime, period_end: datetime) -> list[SnapshotAggregate]",L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,7,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.compute_baselines,"async compute_baselines(tenant_id: str, window_days: int) -> list[dict[str, Any]]",L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,5,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.get_aggregates_with_baseline,"async get_aggregates_with_baseline(snapshot_id: str) -> list[dict[str, Any]]",L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,3,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.get_current_baseline,"async get_current_baseline(tenant_id: str, entity_type: EntityType, entity_id: str | None, window_days: int) -> SnapshotBaseline | None",L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,7,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.get_snapshot,async get_snapshot(snapshot_id: str) -> CostSnapshot | None,L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,1,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.insert_aggregate,async insert_aggregate(aggregate: SnapshotAggregate) -> None,L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,1,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.insert_anomaly,"async insert_anomaly(anomaly_id: str, tenant_id: str, anomaly_type: str, severity: str | None, entity_type: str, entity_id: str | None, current_value_cents: float, expected_value_cents: float, deviation_pct: float, threshold_pct: float, message: str, snapshot_id: str, detected_at: datetime) -> None",L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,16,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.insert_baseline,async insert_baseline(baseline: SnapshotBaseline) -> None,L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,1,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.insert_evaluation,async insert_evaluation(evaluation: AnomalyEvaluation) -> None,L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,1,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.insert_snapshot,async insert_snapshot(snapshot: CostSnapshot) -> None,L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,1,
analytics,L5,cost_snapshot_schemas,CostSnapshotsDriverProtocol.update_snapshot,async update_snapshot(snapshot: CostSnapshot) -> None,L6:cost_snapshots_driver | L5:cost_snapshots_engine,,__future__,pure,yes,1,
analytics,L5,cost_snapshot_schemas,SnapshotAggregate.create,"create(snapshot_id: str, tenant_id: str, entity_type: EntityType, entity_id: str | None, total_cost_cents: float, request_count: int, total_input_tokens: int, total_output_tokens: int) -> 'SnapshotAggregate'",L6:cost_snapshots_driver | L5:cost_snapshots_engine,cls | encode | hexdigest | sha256,__future__,pure,no,28,
analytics,L5,cost_snapshot_schemas,SnapshotBaseline.create,"create(tenant_id: str, entity_type: EntityType, entity_id: str | None, window_days: int, avg_daily_cost_cents: float, avg_daily_requests: float, samples_count: int, stddev: float | None, max_cost: float | None, min_cost: float | None, last_snapshot_id: str | None) -> 'SnapshotBaseline'",L6:cost_snapshots_driver | L5:cost_snapshots_engine,cls | date | encode | hexdigest | isoformat | now | sha256 | timedelta,__future__,pure,no,34,
analytics,L5,cost_snapshots_engine,BaselineComputer.__init__,__init__(driver: CostSnapshotsDriverProtocol),,,__future__ | cost_snapshot_schemas,pure,no,2,
analytics,L5,cost_snapshots_engine,BaselineComputer.compute_baselines,"async compute_baselines(tenant_id: str, window_days: int) -> list[SnapshotBaseline]",,EntityType | append | compute_baselines | create | float | insert_baseline | int,__future__ | cost_snapshot_schemas,pure,yes,28,Compute baselines for all entities from historical snapshots.
analytics,L5,cost_snapshots_engine,SnapshotAnomalyDetector.__init__,__init__(driver: CostSnapshotsDriverProtocol),,,__future__ | cost_snapshot_schemas,pure,no,2,
analytics,L5,cost_snapshots_engine,SnapshotAnomalyDetector.evaluate_snapshot,"async evaluate_snapshot(snapshot_id: str, threshold_pct: float) -> list[AnomalyEvaluation]",,AnomalyEvaluation | EntityType | append | encode | get | get_aggregates_with_baseline | get_snapshot | hexdigest | info | insert_anomaly | insert_evaluation | sha256 | warning,__future__ | cost_snapshot_schemas,pure,yes,81,Evaluate all aggregates in a snapshot for anomalies.
analytics,L5,cost_snapshots_engine,SnapshotComputer.__init__,__init__(driver: CostSnapshotsDriverProtocol),,,__future__ | cost_snapshot_schemas,pure,no,2,
analytics,L5,cost_snapshots_engine,SnapshotComputer._compute_snapshot,"async _compute_snapshot(tenant_id: str, snapshot_type: SnapshotType, period_start: datetime, period_end: datetime) -> CostSnapshot",,aggregate_cost_records | create | error | get_current_baseline | info | insert_aggregate | insert_snapshot | int | now | str | sum | time | update_snapshot,__future__ | cost_snapshot_schemas,pure,yes,63,Internal snapshot computation.
analytics,L5,cost_snapshots_engine,SnapshotComputer.compute_daily_snapshot,"async compute_daily_snapshot(tenant_id: str, date: datetime | None) -> CostSnapshot",,_compute_snapshot | now | replace | timedelta,__future__ | cost_snapshot_schemas,pure,yes,21,Compute daily snapshot for a tenant.
analytics,L5,cost_snapshots_engine,SnapshotComputer.compute_hourly_snapshot,"async compute_hourly_snapshot(tenant_id: str, hour: datetime | None) -> CostSnapshot",,_compute_snapshot | now | replace | timedelta,__future__ | cost_snapshot_schemas,pure,yes,19,Compute hourly snapshot for a tenant.
analytics,L5,cost_snapshots_engine,run_daily_snapshot_and_baseline_job,"async run_daily_snapshot_and_baseline_job(driver: CostSnapshotsDriverProtocol, tenant_ids: list[str]) -> dict",,BaselineComputer | SnapshotAnomalyDetector | SnapshotComputer | append | compute_baselines | compute_daily_snapshot | evaluate_snapshot | len,__future__ | cost_snapshot_schemas,pure,yes,36,Run daily snapshot and baseline computation for multiple tenants.
analytics,L5,cost_snapshots_engine,run_hourly_snapshot_job,"async run_hourly_snapshot_job(driver: CostSnapshotsDriverProtocol, tenant_ids: list[str]) -> dict",,SnapshotComputer | append | compute_hourly_snapshot | str,__future__ | cost_snapshot_schemas,pure,yes,20,Run hourly snapshot job for multiple tenants.
analytics,L5,cost_write_engine,CostWriteEngine.__init__,__init__(session: 'Session'),,get_cost_write_driver,cost_write_driver | db | sqlmodel,pure,no,2,
analytics,L5,cost_write_engine,CostWriteEngine.create_cost_record,"create_cost_record(tenant_id: str, user_id: Optional[str], feature_tag: Optional[str], request_id: Optional[str], workflow_id: Optional[str], skill_id: Optional[str], model: str, input_tokens: int, output_tokens: int, cost_cents: int) -> 'CostRecord'",,create_cost_record,cost_write_driver | db | sqlmodel,pure,no,26,Delegate to driver.
analytics,L5,cost_write_engine,CostWriteEngine.create_feature_tag,"create_feature_tag(tenant_id: str, tag: str, display_name: str, description: Optional[str], budget_cents: Optional[int]) -> 'FeatureTag'",,create_feature_tag,cost_write_driver | db | sqlmodel,pure,no,16,Delegate to driver.
analytics,L5,cost_write_engine,CostWriteEngine.create_or_update_budget,"create_or_update_budget(existing_budget: Optional['CostBudget'], tenant_id: str, budget_type: str, entity_id: Optional[str], daily_limit_cents: Optional[int], monthly_limit_cents: Optional[int], warn_threshold_pct: int, hard_limit_enabled: bool) -> 'CostBudget'",,create_or_update_budget,cost_write_driver | db | sqlmodel,pure,no,22,Delegate to driver.
analytics,L5,cost_write_engine,CostWriteEngine.update_feature_tag,"update_feature_tag(feature_tag: 'FeatureTag', display_name: Optional[str], description: Optional[str], budget_cents: Optional[int], is_active: Optional[bool]) -> 'FeatureTag'",,update_feature_tag,cost_write_driver | db | sqlmodel,pure,no,16,Delegate to driver.
analytics,L5,costsim_models_engine,CanaryReport.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,__future__,pure,no,19,Convert to dictionary.
analytics,L5,costsim_models_engine,ComparisonResult.to_dict,"to_dict() -> Dict[str, Any]",,,__future__,pure,no,17,Convert to dictionary.
analytics,L5,costsim_models_engine,DiffResult.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,__future__,pure,no,13,Convert to dictionary.
analytics,L5,costsim_models_engine,DivergenceReport.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,__future__,pure,no,15,Convert to dictionary.
analytics,L5,costsim_models_engine,V2SimulationResult.compute_output_hash,compute_output_hash() -> str,,dumps | encode | hexdigest | sha256,__future__,pure,no,12,Compute deterministic hash of output.
analytics,L5,costsim_models_engine,V2SimulationResult.to_dict,"to_dict() -> Dict[str, Any]",,,__future__,pure,no,16,Convert to dictionary.
analytics,L5,costsim_models_engine,ValidationResult.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,__future__,pure,no,15,Convert to dictionary.
analytics,L5,datasets_engine,DatasetValidator.__init__,__init__(),,_build_datasets,__future__ | models | random | v2_adapter,pure,no,3,Initialize validator with built-in datasets.
analytics,L5,datasets_engine,DatasetValidator._build_datasets,"_build_datasets() -> Dict[str, ReferenceDataset]",,_build_high_variance_dataset | _build_historical_dataset | _build_low_variance_dataset | _build_mixed_city_dataset | _build_noise_injected_dataset,__future__ | models | random | v2_adapter,pure,no,9,Build all reference datasets.
analytics,L5,datasets_engine,DatasetValidator._build_high_variance_dataset,_build_high_variance_dataset() -> ReferenceDataset,,DatasetSample | ReferenceDataset | append | choice | enumerate | randint | range,__future__ | models | random | v2_adapter,pure,no,81,Build high variance dataset.
analytics,L5,datasets_engine,DatasetValidator._build_historical_dataset,_build_historical_dataset() -> ReferenceDataset,,DatasetSample | ReferenceDataset | append,__future__ | models | random | v2_adapter,pure,no,100,Build historical dataset.
analytics,L5,datasets_engine,DatasetValidator._build_low_variance_dataset,_build_low_variance_dataset() -> ReferenceDataset,,DatasetSample | ReferenceDataset | append | range,__future__ | models | random | v2_adapter,pure,no,77,Build low variance dataset.
analytics,L5,datasets_engine,DatasetValidator._build_mixed_city_dataset,_build_mixed_city_dataset() -> ReferenceDataset,,DatasetSample | ReferenceDataset | append | range,__future__ | models | random | v2_adapter,pure,no,84,Build mixed city dataset.
analytics,L5,datasets_engine,DatasetValidator._build_noise_injected_dataset,_build_noise_injected_dataset() -> ReferenceDataset,,DatasetSample | ReferenceDataset | append | range,__future__ | models | random | v2_adapter,pure,no,117,Build noise-injected dataset.
analytics,L5,datasets_engine,DatasetValidator._calculate_drift_score,"_calculate_drift_score(mean_error: float, median_error: float, std_deviation: float, outlier_pct: float, thresholds: Dict[str, float]) -> float",,min,__future__ | models | random | v2_adapter,pure,no,19,Calculate overall drift score.
analytics,L5,datasets_engine,DatasetValidator.get_dataset,get_dataset(dataset_id: str) -> Optional[ReferenceDataset],,get,__future__ | models | random | v2_adapter,pure,no,3,Get a dataset by ID.
analytics,L5,datasets_engine,DatasetValidator.list_datasets,"list_datasets() -> List[Dict[str, Any]]",,to_dict | values,__future__ | models | random | v2_adapter,pure,no,3,List all available datasets.
analytics,L5,datasets_engine,DatasetValidator.validate_all,"async validate_all() -> Dict[str, ValidationResult]",,validate_dataset,__future__ | models | random | v2_adapter,pure,yes,6,Validate V2 against all datasets.
analytics,L5,datasets_engine,DatasetValidator.validate_dataset,async validate_dataset(dataset_id: str) -> ValidationResult,,CostSimV2Adapter | ValidationResult | ValueError | _calculate_drift_score | abs | append | error | get | len | round | simulate | sorted | sqrt | sum,__future__ | models | random | v2_adapter,pure,yes,95,Validate V2 against a reference dataset.
analytics,L5,datasets_engine,ReferenceDataset.to_dict,"to_dict() -> Dict[str, Any]",,len,__future__ | models | random | v2_adapter,pure,no,9,Convert to dictionary.
analytics,L5,datasets_engine,get_dataset_validator,get_dataset_validator() -> DatasetValidator,,DatasetValidator,__future__ | models | random | v2_adapter,pure,no,6,Get the global dataset validator.
analytics,L5,datasets_engine,validate_all_datasets,"async validate_all_datasets() -> Dict[str, ValidationResult]",,get_dataset_validator | validate_all,__future__ | models | random | v2_adapter,pure,yes,4,Convenience function to validate all datasets.
analytics,L5,datasets_engine,validate_dataset,async validate_dataset(dataset_id: str) -> ValidationResult,,get_dataset_validator | validate_dataset,__future__ | models | random | v2_adapter,pure,yes,4,Convenience function to validate a dataset.
analytics,L5,detection_facade,AnomalyInfo.to_dict,"to_dict() -> Dict[str, Any]",L4:analytics_handler | ?:anomaly_severity,,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,no,21,Convert to dictionary.
analytics,L5,detection_facade,DetectionFacade.__init__,__init__(),L4:analytics_handler | ?:anomaly_severity,,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,no,7,Initialize facade with lazy-loaded services.
analytics,L5,detection_facade,DetectionFacade._run_cost_detection,"async _run_cost_detection(tenant_id: str, session) -> DetectionResult",L4:analytics_handler | ?:anomaly_severity,AnomalyInfo | DetectionResult | detect_and_ingest | error | get | get_anomaly_incident_coordinator | hasattr | isoformat | len | now | str,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,yes,90,Run cost anomaly detection.
analytics,L5,detection_facade,DetectionFacade.acknowledge_anomaly,"async acknowledge_anomaly(anomaly_id: str, tenant_id: str, actor: Optional[str]) -> Optional[AnomalyInfo]",L4:analytics_handler | ?:anomaly_severity,get | isoformat | now,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,yes,27,Acknowledge an anomaly (mark as seen but not resolved).
analytics,L5,detection_facade,DetectionFacade.cost_detector,cost_detector(),L4:analytics_handler | ?:anomaly_severity,warning,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,no,10,Lazy-load CostAnomalyDetector.
analytics,L5,detection_facade,DetectionFacade.get_anomaly,"async get_anomaly(anomaly_id: str, tenant_id: str) -> Optional[AnomalyInfo]",L4:analytics_handler | ?:anomaly_severity,get,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,yes,19,Get a specific anomaly.
analytics,L5,detection_facade,DetectionFacade.get_detection_status,get_detection_status() -> DetectionStatusInfo,L4:analytics_handler | ?:anomaly_severity,DetectionStatusInfo | isoformat,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,no,30,Get detection engine status.
analytics,L5,detection_facade,DetectionFacade.list_anomalies,"async list_anomalies(tenant_id: str, detection_type: Optional[str], severity: Optional[str], status: Optional[str], limit: int, offset: int) -> List[AnomalyInfo]",L4:analytics_handler | ?:anomaly_severity,append | debug | sort | values,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,yes,46,List anomalies for a tenant.
analytics,L5,detection_facade,DetectionFacade.resolve_anomaly,"async resolve_anomaly(anomaly_id: str, tenant_id: str, resolution: str, notes: Optional[str], actor: Optional[str]) -> Optional[AnomalyInfo]",L4:analytics_handler | ?:anomaly_severity,get | info | isoformat | now,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,yes,43,Resolve an anomaly.
analytics,L5,detection_facade,DetectionFacade.run_detection,"async run_detection(tenant_id: str, detection_type: str, session) -> DetectionResult",L4:analytics_handler | ?:anomaly_severity,DetectionResult | _run_cost_detection | error | info | isoformat | now | str,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,yes,56,Run anomaly detection on demand.
analytics,L5,detection_facade,DetectionResult.to_dict,"to_dict() -> Dict[str, Any]",L4:analytics_handler | ?:anomaly_severity,,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,no,12,Convert to dictionary.
analytics,L5,detection_facade,DetectionStatusInfo.to_dict,"to_dict() -> Dict[str, Any]",L4:analytics_handler | ?:anomaly_severity,,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,no,8,Convert to dictionary.
analytics,L5,detection_facade,get_detection_facade,get_detection_facade() -> DetectionFacade,L4:analytics_handler | ?:anomaly_severity,DetectionFacade,anomaly_incident_coordinator | cost_anomaly_detector_engine,pure,no,14,Get the detection facade instance.
analytics,L5,divergence_engine,DivergenceAnalyzer.__init__,"__init__(outlier_threshold: float, major_drift_threshold: float)",,,__future__ | config | models | provenance,pure,no,14,Initialize divergence analyzer.
analytics,L5,divergence_engine,DivergenceAnalyzer._calculate_kl_divergence,"_calculate_kl_divergence(p: List[int], q: List[int], bins: int) -> float",,int | log | max | min | sum | zip,__future__ | config | models | provenance,pure,no,50,Calculate KL divergence between two distributions.
analytics,L5,divergence_engine,DivergenceAnalyzer._calculate_metrics,"_calculate_metrics(samples: List[DivergenceSample]) -> Dict[str, Any]",,_calculate_kl_divergence | abs | float | int | len | round | sorted | sum,__future__ | config | models | provenance,pure,no,46,Calculate divergence metrics from samples.
analytics,L5,divergence_engine,DivergenceAnalyzer._load_samples,"async _load_samples(start_date: datetime, end_date: datetime, tenant_id: Optional[str], max_samples: int) -> List[DivergenceSample]",,_parse_provenance_log | append | error | get_provenance_logger | query | warning,__future__ | config | models | provenance,pure,yes,31,Load samples from provenance logs.
analytics,L5,divergence_engine,DivergenceAnalyzer._parse_provenance_log,_parse_provenance_log(log: ProvenanceLog) -> Optional[DivergenceSample],,DivergenceSample | get | get_decompressed_output | warning,__future__ | config | models | provenance,pure,no,40,Parse a provenance log into a divergence sample.
analytics,L5,divergence_engine,DivergenceAnalyzer.generate_report,"async generate_report(start_date: Optional[datetime], end_date: Optional[datetime], tenant_id: Optional[str], max_samples: int) -> DivergenceReport",,DivergenceReport | _calculate_metrics | _load_samples | get_config | isoformat | len | now | timedelta,__future__ | config | models | provenance,pure,yes,79,Generate a divergence report for the specified time range.
analytics,L5,divergence_engine,generate_divergence_report,"async generate_divergence_report(start_date: Optional[datetime], end_date: Optional[datetime], tenant_id: Optional[str]) -> DivergenceReport",,DivergenceAnalyzer | generate_report,__future__ | config | models | provenance,pure,yes,22,Convenience function to generate a divergence report.
analytics,L5,envelope_engine,EnvelopeValidationError.__init__,"__init__(rule_id: str, message: str)",,__init__ | super,,pure,no,4,
analytics,L5,envelope_engine,calculate_bounded_value,"calculate_bounded_value(baseline: float, bounds: EnvelopeBounds, prediction_confidence: float) -> float",,min,,pure,no,47,Calculate the bounded value based on prediction confidence.
analytics,L5,envelope_engine,create_audit_record,"create_audit_record(envelope: Envelope, baseline_value: float) -> EnvelopeAuditRecord",,EnvelopeAuditRecord | now,,pure,no,14,Create an audit record for envelope application.
analytics,L5,envelope_engine,get_envelope_priority,get_envelope_priority(envelope_class: EnvelopeClass) -> int,,,,pure,no,3,Get the priority of an envelope class (lower number = higher priority).
analytics,L5,envelope_engine,has_higher_priority,"has_higher_priority(class_a: EnvelopeClass, class_b: EnvelopeClass) -> bool",,get_envelope_priority,,pure,no,3,Check if class_a has higher priority than class_b.
analytics,L5,envelope_engine,validate_envelope,validate_envelope(envelope: Envelope) -> None,,EnvelopeValidationError | info | issubset | set,,pure,no,74,Validate envelope against hard gate rules (V1-V5 + CI-C4-1).
analytics,L5,metrics_engine,CostSimMetrics.__init__,__init__(prefix: str),,_init_metrics | warning,__future__ | config | prometheus_client,pure,no,14,Initialize metrics.
analytics,L5,metrics_engine,CostSimMetrics._init_metrics,_init_metrics() -> None,,Counter | Gauge | Histogram | Info | get_config | info,__future__ | config | prometheus_client,pure,no,140,Initialize Prometheus metrics.
analytics,L5,metrics_engine,CostSimMetrics.record_alert_send_failure,"record_alert_send_failure(alert_type: str, error_type: str) -> None",,inc | labels,__future__ | config | prometheus_client,pure,no,19,Record alert send failure.
analytics,L5,metrics_engine,CostSimMetrics.record_auto_recovery,record_auto_recovery() -> None,,inc,__future__ | config | prometheus_client,pure,no,6,Record auto-recovery event after TTL expiry.
analytics,L5,metrics_engine,CostSimMetrics.record_canary_run,record_canary_run(status: str) -> None,,inc | labels,__future__ | config | prometheus_client,pure,no,11,Record canary run completion.
analytics,L5,metrics_engine,CostSimMetrics.record_cb_disabled,"record_cb_disabled(reason: str, severity: str) -> None",,inc | labels,__future__ | config | prometheus_client,pure,no,16,Record circuit breaker disable event.
analytics,L5,metrics_engine,CostSimMetrics.record_cb_enabled,record_cb_enabled(reason: str) -> None,,inc | labels,__future__ | config | prometheus_client,pure,no,11,Record circuit breaker enable (recovery) event.
analytics,L5,metrics_engine,CostSimMetrics.record_cb_incident,"record_cb_incident(severity: str, resolved: bool) -> None",,inc | labels | lower | str,__future__ | config | prometheus_client,pure,no,19,Record circuit breaker incident.
analytics,L5,metrics_engine,CostSimMetrics.record_cost_delta,"record_cost_delta(delta_cents: int, tenant_id: str) -> None",,abs | labels | observe,__future__ | config | prometheus_client,pure,no,16,Record cost delta.
analytics,L5,metrics_engine,CostSimMetrics.record_drift,"record_drift(drift_score: float, verdict: str, tenant_id: str) -> None",,inc | labels | observe,__future__ | config | prometheus_client,pure,no,22,Record drift observation.
analytics,L5,metrics_engine,CostSimMetrics.record_provenance_log,record_provenance_log(tenant_id: str) -> None,,inc | labels,__future__ | config | prometheus_client,pure,no,6,Record provenance log entry.
analytics,L5,metrics_engine,CostSimMetrics.record_schema_error,"record_schema_error(error_type: str, tenant_id: str) -> None",,inc | labels,__future__ | config | prometheus_client,pure,no,19,Record schema validation error.
analytics,L5,metrics_engine,CostSimMetrics.record_simulation,"record_simulation(status: str, tenant_id: str) -> None",,inc | labels,__future__ | config | prometheus_client,pure,no,19,Record simulation completion.
analytics,L5,metrics_engine,CostSimMetrics.record_simulation_duration,"record_simulation_duration(duration_ms: int, tenant_id: str) -> None",,labels | observe,__future__ | config | prometheus_client,pure,no,16,Record simulation duration.
analytics,L5,metrics_engine,CostSimMetrics.set_alert_queue_depth,set_alert_queue_depth(depth: int) -> None,,set,__future__ | config | prometheus_client,pure,no,11,Set current alert queue depth.
analytics,L5,metrics_engine,CostSimMetrics.set_circuit_breaker_state,"set_circuit_breaker_state(is_open: bool, tenant_id: str) -> None",,labels | set,__future__ | config | prometheus_client,pure,no,16,Set circuit breaker state.
analytics,L5,metrics_engine,CostSimMetrics.set_consecutive_failures,set_consecutive_failures(count: int) -> None,,set,__future__ | config | prometheus_client,pure,no,11,Set current consecutive failure count.
analytics,L5,metrics_engine,CostSimMetrics.set_kl_divergence,"set_kl_divergence(kl_divergence: float, tenant_id: str) -> None",,labels | set,__future__ | config | prometheus_client,pure,no,16,Set latest KL divergence.
analytics,L5,metrics_engine,get_alert_rules,get_alert_rules() -> str,,,__future__ | config | prometheus_client,pure,no,3,Get Prometheus alert rules YAML.
analytics,L5,metrics_engine,get_metrics,get_metrics() -> CostSimMetrics,,CostSimMetrics,__future__ | config | prometheus_client,pure,no,6,Get the global CostSim metrics instance.
analytics,L5,pattern_detection_engine,compute_error_signature,compute_error_signature(error: str) -> str,L5:activity_facade,encode | hexdigest | lower | sha256 | strip | sub,db | feedback | pattern_detection_driver | time,pure,no,21,Compute a stable signature for an error message.
analytics,L5,pattern_detection_engine,detect_cost_spikes,"async detect_cost_spikes(driver: PatternDetectionDriver, tenant_id: Optional[UUID], spike_threshold_percent: float, min_runs: int) -> list[dict]",L5:activity_facade,append | fetch_completed_runs_with_costs | info | items | len | round | str | sum,db | feedback | pattern_detection_driver | time,pure,yes,74,Detect abnormal cost increases.
analytics,L5,pattern_detection_engine,detect_failure_patterns,"async detect_failure_patterns(driver: PatternDetectionDriver, tenant_id: Optional[UUID], threshold: int, window_hours: int) -> list[dict]",L5:activity_facade,append | compute_error_signature | fetch_failed_runs | info | items | len | str | timedelta | utc_now,db | feedback | pattern_detection_driver | time,pure,yes,61,Detect repeated failure patterns.
analytics,L5,pattern_detection_engine,emit_feedback,"async emit_feedback(driver: PatternDetectionDriver, feedback: PatternFeedbackCreate) -> dict",L5:activity_facade,UUID | info | insert_feedback | isinstance | len | str | utc_now,db | feedback | pattern_detection_driver | time,pure,yes,46,Emit a feedback record.
analytics,L5,pattern_detection_engine,get_feedback_summary,"async get_feedback_summary(tenant_id: Optional[UUID], acknowledged: Optional[bool], limit: int) -> dict",L5:activity_facade,fetch_feedback_records | get | get_async_session | get_pattern_detection_driver | isoformat | len | str,db | feedback | pattern_detection_driver | time,pure,yes,44,Get feedback summary for ops visibility.
analytics,L5,pattern_detection_engine,run_pattern_detection,async run_pattern_detection(tenant_id: Optional[UUID]) -> dict,L5:activity_facade,PatternFeedbackCreate | append | detect_cost_spikes | detect_failure_patterns | emit_feedback | error | get_async_session | get_pattern_detection_driver | str,db | feedback | pattern_detection_driver | time,pure,yes,81,Run full pattern detection cycle.
analytics,L5,prediction_engine,emit_prediction,"async emit_prediction(driver: 'PredictionDriver', tenant_id: str, prediction_type: str, subject_type: str, subject_id: str, confidence_score: float, prediction_value: dict, contributing_factors: list, notes: Optional[str], valid_until: Optional['datetime']) -> 'PredictionEvent'",,info | insert_prediction | str | timedelta | utc_now,db | prediction | prediction_driver | time,pure,yes,53,Emit a prediction event.
analytics,L5,prediction_engine,get_prediction_summary,"async get_prediction_summary(tenant_id: Optional[UUID], prediction_type: Optional[str], include_expired: bool, limit: int) -> dict",,fetch_predictions | get | get_async_session | get_prediction_driver | isoformat | len | str | utc_now,db | prediction | prediction_driver | time,pure,yes,54,Get prediction summary for ops visibility.
analytics,L5,prediction_engine,predict_cost_overrun,"async predict_cost_overrun(driver: 'PredictionDriver', tenant_id: Optional[UUID], worker_id: Optional[str]) -> list[dict]",,append | fetch_cost_runs | info | items | len | min | round | sorted | str | sum | timedelta | utc_now,db | prediction | prediction_driver | time,pure,yes,97,Predict likelihood of cost overrun for upcoming runs.
analytics,L5,prediction_engine,predict_failure_likelihood,"async predict_failure_likelihood(driver: 'PredictionDriver', tenant_id: Optional[UUID], worker_id: Optional[str]) -> list[dict]",,append | fetch_failed_runs | fetch_failure_patterns | fetch_run_totals | get | info | items | len | list | min | round | set | str | timedelta | utc_now,db | prediction | prediction_driver | time,pure,yes,97,Predict likelihood of failure for upcoming runs.
analytics,L5,prediction_engine,run_prediction_cycle,async run_prediction_cycle(tenant_id: Optional[UUID]) -> dict,,append | emit_prediction | error | get_async_session | get_prediction_driver | predict_cost_overrun | predict_failure_likelihood | str,db | prediction | prediction_driver | time,pure,yes,79,Run full prediction cycle.
analytics,L5,provenance_engine,ProvenanceLog.from_dict,"from_dict(data: Dict[str, Any]) -> 'ProvenanceLog'",,cls | fromisoformat | get,__future__ | config | gzip,pure,no,19,Create from dictionary.
analytics,L5,provenance_engine,ProvenanceLog.get_decompressed_input,"get_decompressed_input() -> Dict[str, Any]",,b64decode | decompress | loads,__future__ | config | gzip,pure,no,7,Get decompressed input JSON.
analytics,L5,provenance_engine,ProvenanceLog.get_decompressed_output,"get_decompressed_output() -> Dict[str, Any]",,b64decode | decompress | loads,__future__ | config | gzip,pure,no,7,Get decompressed output JSON.
analytics,L5,provenance_engine,ProvenanceLog.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,__future__ | config | gzip,pure,no,19,Convert to dictionary for storage.
analytics,L5,provenance_engine,ProvenanceLogger.__init__,"__init__(storage_path: Optional[str], db_enabled: bool, file_enabled: bool)",,Lock | Path | get_config | mkdir,__future__ | config | gzip,pure,no,28,Initialize provenance logger.
analytics,L5,provenance_engine,ProvenanceLogger._flush,async _flush() -> None,,_write_to_db | _write_to_file | clear | copy,__future__ | config | gzip,pure,yes,15,Flush buffer to storage.
analytics,L5,provenance_engine,ProvenanceLogger._store,async _store(log_entry: ProvenanceLog) -> None,,_flush | append | len,__future__ | config | gzip,pure,yes,7,Store a provenance log entry.
analytics,L5,provenance_engine,ProvenanceLogger._write_to_db,async _write_to_db(entries: List[ProvenanceLog]) -> None,,,__future__ | config | gzip,pure,yes,5,Write entries to database.
analytics,L5,provenance_engine,ProvenanceLogger._write_to_file,async _write_to_file(entries: List[ProvenanceLog]) -> None,,append | dumps | error | items | open | strftime | to_dict | write,__future__ | config | gzip,file_io,yes,20,Write entries to file storage.
analytics,L5,provenance_engine,ProvenanceLogger.close,async close() -> None,,_flush,__future__ | config | gzip,pure,yes,4,Flush remaining entries and close.
analytics,L5,provenance_engine,ProvenanceLogger.log,"async log(input_data: Any, output_data: Any, runtime_ms: int, status: str, tenant_id: Optional[str], run_id: Optional[str]) -> ProvenanceLog",,ProvenanceLog | _store | compress_json | compute_hash | dumps | get_commit_sha | get_config | now | str | uuid4,__future__ | config | gzip,pure,yes,62,Log a provenance entry.
analytics,L5,provenance_engine,ProvenanceLogger.query,"async query(start_date: Optional[datetime], end_date: Optional[datetime], input_hash: Optional[str], status: Optional[str], tenant_id: Optional[str], limit: int) -> List[ProvenanceLog]",,append | error | from_dict | glob | len | loads | open | sorted,__future__ | config | gzip,file_io,yes,57,Query provenance logs.
analytics,L5,provenance_engine,compress_json,compress_json(data: Any) -> str,,b64encode | compress | decode | dumps | encode,__future__ | config | gzip,pure,no,5,Compress JSON data to base64-encoded gzip.
analytics,L5,provenance_engine,compute_hash,compute_hash(data: Any) -> str,,dumps | encode | hexdigest | isinstance | sha256 | str,__future__ | config | gzip,pure,no,7,Compute SHA256 hash of data.
analytics,L5,provenance_engine,get_provenance_logger,get_provenance_logger() -> ProvenanceLogger,,ProvenanceLogger,__future__ | config | gzip,pure,no,6,Get the global provenance logger.
analytics,L5,s1_retry_backoff_engine,create_s1_envelope,"create_s1_envelope(baseline_value: float, reference_id: str) -> Envelope",,Envelope | EnvelopeBaseline | EnvelopeBounds | EnvelopeScope | EnvelopeTimebox | EnvelopeTrigger,envelope,pure,no,50,Create a fresh S1 envelope instance with specified baseline.
analytics,L5,sandbox_engine,CostSimSandbox.__init__,"__init__(budget_cents: int, allowed_skills: Optional[List[str]], risk_threshold: float, tenant_id: Optional[str], run_id: Optional[str])",,CostSimulator,__future__ | circuit_breaker_async | config | models | simulate | v2_adapter,pure,no,33,Initialize sandbox router.
analytics,L5,sandbox_engine,CostSimSandbox._get_v2_adapter,_get_v2_adapter() -> CostSimV2Adapter,,CostSimV2Adapter,__future__ | circuit_breaker_async | config | models | simulate | v2_adapter,pure,no,12,Get or create V2 adapter.
analytics,L5,sandbox_engine,CostSimSandbox._log_comparison,_log_comparison(comparison: ComparisonResult) -> None,,debug | error | info | warning,__future__ | circuit_breaker_async | config | models | simulate | v2_adapter,pure,no,28,Log comparison result for monitoring.
analytics,L5,sandbox_engine,CostSimSandbox.simulate,"async simulate(plan: List[Dict[str, Any]]) -> SandboxResult",,SandboxResult | _get_v2_adapter | _log_comparison | error | is_v2_disabled | is_v2_sandbox_enabled | report_drift | simulate | simulate_with_comparison | str | warning,__future__ | circuit_breaker_async | config | models | simulate | v2_adapter,pure,yes,73,Run simulation through sandbox.
analytics,L5,sandbox_engine,SandboxResult.production_result,production_result() -> SimulationResult,,,__future__ | circuit_breaker_async | config | models | simulate | v2_adapter,pure,no,3,Get the production result (always V1).
analytics,L5,sandbox_engine,get_sandbox,"get_sandbox(budget_cents: int, tenant_id: Optional[str]) -> CostSimSandbox",,CostSimSandbox,__future__ | circuit_breaker_async | config | models | simulate | v2_adapter,pure,no,31,Get a sandbox instance.
analytics,L5,sandbox_engine,simulate_with_sandbox,"async simulate_with_sandbox(plan: List[Dict[str, Any]], budget_cents: int, allowed_skills: Optional[List[str]], tenant_id: Optional[str], run_id: Optional[str]) -> SandboxResult",,CostSimSandbox | simulate,__future__ | circuit_breaker_async | config | models | simulate | v2_adapter,pure,yes,27,Convenience function for sandbox simulation.
analytics,L6,analytics_read_driver,AnalyticsReadDriver.__init__,__init__(session: AsyncSession),L5:analytics_facade,,asyncio | sqlalchemy,pure,no,3,Initialize driver with async session.
analytics,L6,analytics_read_driver,AnalyticsReadDriver.fetch_cost_by_feature,"async fetch_cost_by_feature(tenant_id: str, from_ts: datetime, to_ts: datetime) -> list[dict[str, Any]]",L5:analytics_facade,execute | fetchall | float | text,asyncio | sqlalchemy,db_write,yes,48,Fetch cost breakdown by feature tag from cost_records table.
analytics,L6,analytics_read_driver,AnalyticsReadDriver.fetch_cost_by_model,"async fetch_cost_by_model(tenant_id: str, from_ts: datetime, to_ts: datetime) -> list[dict[str, Any]]",L5:analytics_facade,execute | fetchall | float | text,asyncio | sqlalchemy,db_write,yes,52,Fetch cost breakdown by model from cost_records table.
analytics,L6,analytics_read_driver,AnalyticsReadDriver.fetch_cost_metrics,"async fetch_cost_metrics(tenant_id: str, from_ts: datetime, to_ts: datetime, time_trunc: str) -> list[dict[str, Any]]",L5:analytics_facade,execute | fetchall | isoformat | text,asyncio | sqlalchemy,db_write,yes,51,Fetch cost metrics from cost_records table.
analytics,L6,analytics_read_driver,AnalyticsReadDriver.fetch_cost_spend,"async fetch_cost_spend(tenant_id: str, from_ts: datetime, to_ts: datetime, time_trunc: str) -> list[dict[str, Any]]",L5:analytics_facade,execute | fetchall | float | isoformat | text,asyncio | sqlalchemy,db_write,yes,55,Fetch cost spend data from cost_records table.
analytics,L6,analytics_read_driver,AnalyticsReadDriver.fetch_llm_usage,"async fetch_llm_usage(tenant_id: str, from_ts: datetime, to_ts: datetime, time_trunc: str) -> list[dict[str, Any]]",L5:analytics_facade,execute | fetchall | isoformat | text,asyncio | sqlalchemy,db_write,yes,51,Fetch LLM usage from runs table.
analytics,L6,analytics_read_driver,AnalyticsReadDriver.fetch_worker_execution,"async fetch_worker_execution(tenant_id: str, from_ts: datetime, to_ts: datetime, time_trunc: str) -> list[dict[str, Any]]",L5:analytics_facade,execute | fetchall | isoformat | text,asyncio | sqlalchemy,db_write,yes,49,Fetch worker execution metrics from aos_traces table.
analytics,L6,analytics_read_driver,get_analytics_read_driver,get_analytics_read_driver(session: AsyncSession) -> AnalyticsReadDriver,L5:analytics_facade,AnalyticsReadDriver,asyncio | sqlalchemy,pure,no,3,Get an AnalyticsReadDriver instance.
analytics,L6,coordination_audit_driver,_now_utc,_now_utc() -> datetime,,now,infra | sqlmodel,pure,no,3,Get current UTC timestamp.
analytics,L6,coordination_audit_driver,persist_audit_record,"persist_audit_record(db: Session, audit_id: str, envelope_id: str, envelope_class: str, decision: str, reason: str, decision_timestamp: datetime, conflicting_envelope_id: Optional[str], preempting_envelope_id: Optional[str], active_envelopes_count: int, tenant_id: Optional[str], emit_traces: bool) -> bool",,CoordinationAuditRecordDB | UUID | add | debug | error | isinstance | rollback | str,infra | sqlmodel,db_write,no,90,Persist a coordination audit record to the database.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.__init__,__init__(session: Session),L6:__init__ | L5:cost_anomaly_detector_engine,,sqlalchemy | sqlmodel,pure,no,3,Initialize with database session.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_baseline_avg,"fetch_baseline_avg(tenant_id: str, baseline_start: date, baseline_end: date) -> float",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,42,Fetch 21-day baseline average cost (excluding rolling period).
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_breach_exists_today,"fetch_breach_exists_today(tenant_id: str, entity_type: str, entity_id: str, breach_type: str, today: date) -> bool",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,42,Check if breach already recorded for today.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_consecutive_breaches,"fetch_consecutive_breaches(tenant_id: str, entity_type: str, entity_id: str, breach_type: str, today: date) -> int",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,53,Count consecutive breaches ending on today.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_daily_spend,"fetch_daily_spend(tenant_id: str, today_start: datetime, budget_type: Optional[str], entity_id: Optional[str]) -> float",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,42,Fetch daily spend for budget checking.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_drift_tracking,"fetch_drift_tracking(tenant_id: str, entity_type: str, entity_id: str) -> Optional[Tuple[str, int, date, date]]",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,37,Fetch active drift tracking record.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_entity_baseline,"fetch_entity_baseline(tenant_id: str, column_name: str, baseline_start: date, baseline_end: date) -> Dict[str, float]",L6:__init__ | L5:cost_anomaly_detector_engine,all | execute | text,sqlalchemy | sqlmodel,db_write,no,44,Fetch baseline daily averages per entity (user or feature).
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_entity_today_spend,"fetch_entity_today_spend(tenant_id: str, column_name: str, today_start: datetime) -> Dict[str, float]",L6:__init__ | L5:cost_anomaly_detector_engine,all | execute | text,sqlalchemy | sqlmodel,db_write,no,37,Fetch today's spend per entity (user or feature).
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_feature_concentration,"fetch_feature_concentration(tenant_id: str, today_start: datetime) -> Tuple[Optional[float], Optional[float]]",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,39,Fetch feature cost concentration for today.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_monthly_spend,"fetch_monthly_spend(tenant_id: str, month_start: date, budget_type: Optional[str], entity_id: Optional[str]) -> float",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,42,Fetch monthly spend for budget checking.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_prompt_comparison,"fetch_prompt_comparison(tenant_id: str, today_start: datetime, yesterday_start: datetime, entity_type: Optional[str], entity_id: Optional[str]) -> Tuple[Optional[float], Optional[float]]",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,51,Fetch average prompt token comparison between today and yesterday.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_request_comparison,"fetch_request_comparison(tenant_id: str, today_start: datetime, yesterday_start: datetime, entity_type: Optional[str], entity_id: Optional[str]) -> Tuple[Optional[int], Optional[int]]",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,51,Fetch request count comparison between today and yesterday.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_retry_comparison,"fetch_retry_comparison(tenant_id: str, today_start: datetime, yesterday_start: datetime, entity_type: Optional[str], entity_id: Optional[str]) -> Tuple[Optional[float], Optional[float]]",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,53,Fetch retry ratio comparison between today and yesterday.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_rolling_avg,"fetch_rolling_avg(tenant_id: str, rolling_start: date, rolling_end: date) -> float",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,36,Fetch 7-day rolling average cost.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_tenant_baseline,"fetch_tenant_baseline(tenant_id: str, baseline_start: date, baseline_end: date) -> Optional[float]",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,36,Fetch tenant-level baseline daily average.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.fetch_tenant_today_spend,"fetch_tenant_today_spend(tenant_id: str, today_start: datetime) -> float",L6:__init__ | L5:cost_anomaly_detector_engine,execute | first | text,sqlalchemy | sqlmodel,db_write,no,32,Fetch tenant-level today's spend.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.insert_breach_history,"insert_breach_history(breach_id: str, tenant_id: str, entity_type: str, entity_id: str, breach_type: str, breach_date: date, deviation_pct: float, current_value: float, baseline_value: float, created_at: datetime) -> None",L6:__init__ | L5:cost_anomaly_detector_engine,execute | text,sqlalchemy | sqlmodel,db_write,no,54,Insert or update breach history record.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.insert_drift_tracking,"insert_drift_tracking(drift_id: str, tenant_id: str, entity_type: str, entity_id: str, rolling_avg: float, baseline_avg: float, drift_pct: float, today: date, created_at: datetime) -> None",L6:__init__ | L5:cost_anomaly_detector_engine,execute | text,sqlalchemy | sqlmodel,db_write,no,49,Insert new drift tracking record.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.reset_drift_tracking,"reset_drift_tracking(tenant_id: str, entity_type: str, entity_id: str, updated_at: datetime) -> None",L6:__init__ | L5:cost_anomaly_detector_engine,execute | text,sqlalchemy | sqlmodel,db_write,no,34,Mark drift tracking as inactive.
analytics,L6,cost_anomaly_driver,CostAnomalyDriver.update_drift_tracking,"update_drift_tracking(drift_id: str, rolling_avg: float, baseline_avg: float, drift_pct: float, drift_days_count: int, today: date, updated_at: datetime) -> None",L6:__init__ | L5:cost_anomaly_detector_engine,execute | text,sqlalchemy | sqlmodel,db_write,no,45,Update existing drift tracking record.
analytics,L6,cost_anomaly_driver,get_cost_anomaly_driver,get_cost_anomaly_driver(session: Session) -> CostAnomalyDriver,L6:__init__ | L5:cost_anomaly_detector_engine,CostAnomalyDriver,sqlalchemy | sqlmodel,pure,no,3,Factory function to get CostAnomalyDriver instance.
analytics,L6,cost_anomaly_read_driver,CostAnomalyReadDriver.__init__,__init__(session: Session),L5:cost_anomaly_detector_engine,,db | sqlmodel,pure,no,2,
analytics,L6,cost_anomaly_read_driver,CostAnomalyReadDriver.fetch_active_budgets,fetch_active_budgets(tenant_id: str) -> list,L5:cost_anomaly_detector_engine,all | exec | select | where,db | sqlmodel,pure,no,15,Fetch all active budgets for a tenant.
analytics,L6,cost_anomaly_read_driver,CostAnomalyReadDriver.find_existing_anomaly,"find_existing_anomaly(tenant_id: str, anomaly_type: str, entity_type: str, entity_id: Optional[str], since: datetime) -> Optional[CostAnomaly]",L5:cost_anomaly_detector_engine,exec | first | select | where,db | sqlmodel,pure,no,30,Find an existing unresolved anomaly for deduplication.
analytics,L6,cost_anomaly_read_driver,CostAnomalyReadDriver.flush_and_refresh,flush_and_refresh(anomalies: List[CostAnomaly]) -> None,L5:cost_anomaly_detector_engine,flush | refresh,db | sqlmodel,db_write,no,5,Flush to get generated IDs and refresh objects.
analytics,L6,cost_anomaly_read_driver,CostAnomalyReadDriver.persist_anomaly,persist_anomaly(anomaly: CostAnomaly) -> None,L5:cost_anomaly_detector_engine,add,db | sqlmodel,db_write,no,3,Add or update an anomaly record (no commit  L4 owns transaction).
analytics,L6,cost_anomaly_read_driver,get_cost_anomaly_read_driver,get_cost_anomaly_read_driver(session: Session) -> CostAnomalyReadDriver,L5:cost_anomaly_detector_engine,CostAnomalyReadDriver,db | sqlmodel,pure,no,3,Factory for CostAnomalyReadDriver.
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.__init__,__init__(session: AsyncSession),L5:cost_snapshots_engine,,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,pure,no,2,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.aggregate_cost_records,"async aggregate_cost_records(tenant_id: str, snapshot_id: str, period_start: datetime, period_end: datetime) -> list[SnapshotAggregate]",L5:cost_snapshots_engine,append | create | execute | fetchall | fetchone | float | int | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,93,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.compute_baselines,"async compute_baselines(tenant_id: str, window_days: int) -> list[dict[str, Any]]",L5:cost_snapshots_engine,append | execute | fetchall | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,34,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.get_aggregates_with_baseline,"async get_aggregates_with_baseline(snapshot_id: str) -> list[dict[str, Any]]",L5:cost_snapshots_engine,append | execute | fetchall | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,17,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.get_current_baseline,"async get_current_baseline(tenant_id: str, entity_type: EntityType, entity_id: str | None, window_days: int) -> SnapshotBaseline | None",L5:cost_snapshots_engine,EntityType | SnapshotBaseline | execute | fetchone | isinstance | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,46,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.get_snapshot,async get_snapshot(snapshot_id: str) -> CostSnapshot | None,L5:cost_snapshots_engine,CostSnapshot | SnapshotStatus | SnapshotType | execute | fetchone | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,14,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.insert_aggregate,async insert_aggregate(aggregate: SnapshotAggregate) -> None,L5:cost_snapshots_engine,execute | isinstance | now | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,43,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.insert_anomaly,"async insert_anomaly(anomaly_id: str, tenant_id: str, anomaly_type: str, severity: str | None, entity_type: str, entity_id: str | None, current_value_cents: float, expected_value_cents: float, deviation_pct: float, threshold_pct: float, message: str, snapshot_id: str, detected_at: datetime) -> None",L5:cost_snapshots_engine,execute | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,36,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.insert_baseline,async insert_baseline(baseline: SnapshotBaseline) -> None,L5:cost_snapshots_engine,execute | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,41,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.insert_evaluation,async insert_evaluation(evaluation: AnomalyEvaluation) -> None,L5:cost_snapshots_engine,execute | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,27,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.insert_snapshot,async insert_snapshot(snapshot: CostSnapshot) -> None,L5:cost_snapshots_engine,execute | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,25,
analytics,L6,cost_snapshots_driver,CostSnapshotsDriver.update_snapshot,async update_snapshot(snapshot: CostSnapshot) -> None,L5:cost_snapshots_engine,execute | text,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,db_write,yes,21,
analytics,L6,cost_snapshots_driver,get_cost_snapshots_driver,get_cost_snapshots_driver(session: AsyncSession) -> CostSnapshotsDriver,L5:cost_snapshots_engine,CostSnapshotsDriver,__future__ | asyncio | cost_snapshot_schemas | sqlalchemy,pure,no,3,Factory function to get CostSnapshotsDriver instance.
analytics,L6,cost_write_driver,CostWriteDriver.__init__,__init__(session: Session),L5:cost_write_engine,,db | sqlmodel | time,pure,no,2,
analytics,L6,cost_write_driver,CostWriteDriver.create_cost_record,"create_cost_record(tenant_id: str, user_id: Optional[str], feature_tag: Optional[str], request_id: Optional[str], workflow_id: Optional[str], skill_id: Optional[str], model: str, input_tokens: int, output_tokens: int, cost_cents: int) -> CostRecord",L5:cost_write_engine,CostRecord | add,db | sqlmodel | time,db_write,no,46,Create a new cost record and persist.
analytics,L6,cost_write_driver,CostWriteDriver.create_feature_tag,"create_feature_tag(tenant_id: str, tag: str, display_name: str, description: Optional[str], budget_cents: Optional[int]) -> FeatureTag",L5:cost_write_engine,FeatureTag | add | flush | refresh,db | sqlmodel | time,db_write,no,32,Create a new feature tag and persist.
analytics,L6,cost_write_driver,CostWriteDriver.create_or_update_budget,"create_or_update_budget(existing_budget: Optional[CostBudget], tenant_id: str, budget_type: str, entity_id: Optional[str], daily_limit_cents: Optional[int], monthly_limit_cents: Optional[int], warn_threshold_pct: int, hard_limit_enabled: bool) -> CostBudget",L5:cost_write_engine,CostBudget | add | flush | refresh | utc_now,db | sqlmodel | time,db_write,no,49,Create a new budget or update existing one and persist.
analytics,L6,cost_write_driver,CostWriteDriver.update_feature_tag,"update_feature_tag(feature_tag: FeatureTag, display_name: Optional[str], description: Optional[str], budget_cents: Optional[int], is_active: Optional[bool]) -> FeatureTag",L5:cost_write_engine,add | flush | refresh | utc_now,db | sqlmodel | time,db_write,no,36,Update a feature tag and persist.
analytics,L6,cost_write_driver,get_cost_write_driver,get_cost_write_driver(session: Session) -> CostWriteDriver,L5:cost_write_engine,CostWriteDriver,db | sqlmodel | time,pure,no,3,Factory function to get CostWriteDriver instance.
analytics,L6,leader_driver,LeaderContext.__aenter__,async __aenter__() -> bool,,AsyncSessionLocal | error | try_acquire_leader_lock | wait_for | warning,__future__ | asyncio | db_async | sqlalchemy,pure,yes,24,Enter context and attempt to acquire leadership.
analytics,L6,leader_driver,LeaderContext.__aexit__,"async __aexit__(exc_type, _exc_val, _exc_tb) -> None",,close | error,__future__ | asyncio | db_async | sqlalchemy,pure,yes,11,Exit context and release leadership.
analytics,L6,leader_driver,LeaderContext.__init__,"__init__(lock_id: int, session: Optional[AsyncSession], timeout_seconds: float)",,,__future__ | asyncio | db_async | sqlalchemy,pure,no,19,Initialize leader context.
analytics,L6,leader_driver,LeaderContext.is_leader,is_leader() -> bool,,,__future__ | asyncio | db_async | sqlalchemy,pure,no,3,Check if we currently hold leadership.
analytics,L6,leader_driver,is_lock_held,"async is_lock_held(session: AsyncSession, lock_id: int) -> bool",,execute | fetchone | text,__future__ | asyncio | db_async | sqlalchemy,db_write,yes,31,Check if a lock is currently held by any session.
analytics,L6,leader_driver,leader_election,"async leader_election(lock_id: int, timeout_seconds: float) -> AsyncGenerator[bool, None]",,AsyncSessionLocal | close | error | try_acquire_leader_lock | wait_for | warning,__future__ | asyncio | db_async | sqlalchemy,pure,yes,47,Context manager for leader election.
analytics,L6,leader_driver,release_leader_lock,"async release_leader_lock(session: AsyncSession, lock_id: int) -> bool",,debug | execute | fetchone | info | text,__future__ | asyncio | db_async | sqlalchemy,db_write,yes,30,Explicitly release an advisory lock.
analytics,L6,leader_driver,try_acquire_leader_lock,"async try_acquire_leader_lock(session: AsyncSession, lock_id: int) -> bool",,debug | execute | fetchone | info | text,__future__ | asyncio | db_async | sqlalchemy,db_write,yes,30,Try to acquire an advisory lock (non-blocking).
analytics,L6,leader_driver,with_alert_worker_lock,"async with_alert_worker_lock(callback, *args, **kwargs)",,with_leader_lock,__future__ | asyncio | db_async | sqlalchemy,pure,yes,3,Execute callback with alert worker lock.
analytics,L6,leader_driver,with_archiver_lock,"async with_archiver_lock(callback, *args, **kwargs)",,with_leader_lock,__future__ | asyncio | db_async | sqlalchemy,pure,yes,3,Execute callback with provenance archiver lock.
analytics,L6,leader_driver,with_canary_lock,"async with_canary_lock(callback, *args, **kwargs)",,with_leader_lock,__future__ | asyncio | db_async | sqlalchemy,pure,yes,3,Execute callback with canary runner lock.
analytics,L6,leader_driver,with_leader_lock,"async with_leader_lock(lock_id: int, callback, *args, **kwargs)",,callback | leader_election,__future__ | asyncio | db_async | sqlalchemy,pure,yes,34,Execute callback only if we can acquire leadership.
analytics,L6,pattern_detection_driver,PatternDetectionDriver.__init__,__init__(session: AsyncSession),L5:pattern_detection_engine,,asyncio | feedback | sqlalchemy | tenant,pure,no,3,Initialize driver with async session.
analytics,L6,pattern_detection_driver,PatternDetectionDriver.fetch_completed_runs_with_costs,async fetch_completed_runs_with_costs(tenant_id: Optional[UUID]) -> list[WorkerRun],L5:pattern_detection_engine,all | desc | execute | isnot | list | order_by | scalars | select | where,asyncio | feedback | sqlalchemy | tenant,db_write,yes,26,Fetch completed runs that have cost data.
analytics,L6,pattern_detection_driver,PatternDetectionDriver.fetch_failed_runs,"async fetch_failed_runs(window_start: datetime, tenant_id: Optional[UUID]) -> list[WorkerRun]",L5:pattern_detection_engine,all | execute | isnot | list | scalars | select | where,asyncio | feedback | sqlalchemy | tenant,db_write,yes,27,Fetch failed runs within a time window.
analytics,L6,pattern_detection_driver,PatternDetectionDriver.fetch_feedback_records,"async fetch_feedback_records(tenant_id: Optional[UUID], acknowledged: Optional[bool], limit: int) -> list[PatternFeedback]",L5:pattern_detection_engine,all | desc | execute | limit | list | order_by | scalars | select | where,asyncio | feedback | sqlalchemy | tenant,db_write,yes,28,Fetch pattern feedback records.
analytics,L6,pattern_detection_driver,PatternDetectionDriver.insert_feedback,"async insert_feedback(tenant_id: UUID, pattern_type: str, severity: str, description: str, signature: str, provenance: list[str], occurrence_count: int, time_window_minutes: int, threshold_used: str, extra_data: Optional[dict[str, Any]], detected_at: datetime, created_at: datetime) -> PatternFeedback",L5:pattern_detection_engine,PatternFeedback | add | flush,asyncio | feedback | sqlalchemy | tenant,db_write,yes,54,Insert a pattern feedback record.
analytics,L6,pattern_detection_driver,get_pattern_detection_driver,get_pattern_detection_driver(session: AsyncSession) -> PatternDetectionDriver,L5:pattern_detection_engine,PatternDetectionDriver,asyncio | feedback | sqlalchemy | tenant,pure,no,3,Get a PatternDetectionDriver instance.
analytics,L6,prediction_driver,PredictionDriver.__init__,__init__(session: AsyncSession),L6:__init__ | L5:prediction_engine,,asyncio | feedback | prediction | sqlalchemy | tenant,pure,no,3,Initialize with async database session.
analytics,L6,prediction_driver,PredictionDriver.fetch_cost_runs,"async fetch_cost_runs(since: datetime, tenant_id: Optional[UUID], worker_id: Optional[str], limit: int) -> List[WorkerRun]",L6:__init__ | L5:prediction_engine,all | desc | execute | isnot | limit | list | order_by | scalars | select | where,asyncio | feedback | prediction | sqlalchemy | tenant,db_write,yes,36,Fetch completed runs with cost data.
analytics,L6,prediction_driver,PredictionDriver.fetch_failed_runs,"async fetch_failed_runs(since: datetime, tenant_id: Optional[UUID], worker_id: Optional[str], limit: int) -> List[WorkerRun]",L6:__init__ | L5:prediction_engine,all | desc | execute | limit | list | order_by | scalars | select | where,asyncio | feedback | prediction | sqlalchemy | tenant,db_write,yes,34,Fetch failed worker runs.
analytics,L6,prediction_driver,PredictionDriver.fetch_failure_patterns,"async fetch_failure_patterns(tenant_id: Optional[UUID], limit: int) -> List[PatternFeedback]",L6:__init__ | L5:prediction_engine,all | desc | execute | limit | list | order_by | scalars | select | str | where,asyncio | feedback | prediction | sqlalchemy | tenant,db_write,yes,27,Fetch failure pattern feedback records.
analytics,L6,prediction_driver,PredictionDriver.fetch_predictions,"async fetch_predictions(tenant_id: Optional[UUID], prediction_type: Optional[str], valid_after: Optional[datetime], limit: int) -> List[PredictionEvent]",L6:__init__ | L5:prediction_engine,all | desc | execute | is_ | limit | list | order_by | scalars | select | str | where,asyncio | feedback | prediction | sqlalchemy | tenant,db_write,yes,35,Fetch prediction events.
analytics,L6,prediction_driver,PredictionDriver.fetch_run_totals,"async fetch_run_totals(since: datetime, tenant_id: Optional[UUID]) -> Dict[str, int]",L6:__init__ | L5:prediction_engine,count | execute | group_by | label | select | str | where,asyncio | feedback | prediction | sqlalchemy | tenant,db_write,yes,26,Fetch total run counts grouped by worker.
analytics,L6,prediction_driver,PredictionDriver.insert_prediction,"async insert_prediction(tenant_id: str, prediction_type: str, subject_type: str, subject_id: str, confidence_score: float, prediction_value: Dict[str, Any], contributing_factors: List[Dict[str, Any]], valid_until: datetime, created_at: datetime, notes: Optional[str]) -> PredictionEvent",L6:__init__ | L5:prediction_engine,PredictionEvent | add | flush,asyncio | feedback | prediction | sqlalchemy | tenant,db_write,yes,49,Insert a new prediction event.
analytics,L6,prediction_driver,get_prediction_driver,get_prediction_driver(session: AsyncSession) -> PredictionDriver,L6:__init__ | L5:prediction_engine,PredictionDriver,asyncio | feedback | prediction | sqlalchemy | tenant,pure,no,3,Factory function to get PredictionDriver instance.
analytics,L6,provenance_driver,backfill_v1_baseline,"async backfill_v1_baseline(records: List[Dict[str, Any]], batch_size: int) -> Dict[str, int]",,check_duplicate | error | get | info | len | range | write_provenance,__future__ | asyncio | costsim_cb | db_async | sqlalchemy,pure,yes,46,Backfill V1 baseline records from historical data.
analytics,L6,provenance_driver,check_duplicate,async check_duplicate(input_hash: str) -> bool,,async_session_context | execute | first | limit | scalars | select | where,__future__ | asyncio | costsim_cb | db_async | sqlalchemy,db_write,yes,15,Check if a record with this input hash already exists.
analytics,L6,provenance_driver,compute_input_hash,"compute_input_hash(payload: Dict[str, Any]) -> str",,dumps | encode | hexdigest | sha256,__future__ | asyncio | costsim_cb | db_async | sqlalchemy,pure,no,13,Compute deterministic hash of input payload.
analytics,L6,provenance_driver,count_provenance,"async count_provenance(tenant_id: Optional[str], variant_slug: Optional[str], start_date: Optional[datetime], end_date: Optional[datetime]) -> int",,and_ | append | async_session_context | count | execute | scalar | select | select_from | where,__future__ | asyncio | costsim_cb | db_async | sqlalchemy,db_write,yes,37,Count provenance records matching filters.
analytics,L6,provenance_driver,get_drift_stats,"async get_drift_stats(start_date: Optional[datetime], end_date: Optional[datetime]) -> Dict[str, Any]",,and_ | append | async_session_context | avg | count | execute | fetchone | float | isnot | label | max | min | select | stddev | where,__future__ | asyncio | costsim_cb | db_async | sqlalchemy,db_write,yes,52,Get drift statistics between V1 and V2 costs.
analytics,L6,provenance_driver,query_provenance,"async query_provenance(tenant_id: Optional[str], variant_slug: Optional[str], source: Optional[str], input_hash: Optional[str], start_date: Optional[datetime], end_date: Optional[datetime], limit: int, offset: int) -> List[Dict[str, Any]]",,and_ | append | async_session_context | desc | execute | limit | offset | order_by | scalars | select | to_dict | where,__future__ | asyncio | costsim_cb | db_async | sqlalchemy,db_write,yes,52,Query provenance records.
analytics,L6,provenance_driver,write_provenance,"async write_provenance(run_id: Optional[str], tenant_id: Optional[str], variant_slug: str, source: str, model_version: Optional[str], adapter_version: Optional[str], commit_sha: Optional[str], input_hash: Optional[str], output_hash: Optional[str], v1_cost: Optional[float], v2_cost: Optional[float], payload: Optional[Dict[str, Any]], runtime_ms: Optional[int], session: Optional[AsyncSession]) -> int",,AsyncSessionLocal | CostSimProvenanceModel | add | close | debug | error | flush | refresh,__future__ | asyncio | costsim_cb | db_async | sqlalchemy,db_write,yes,82,Write a single provenance record.
analytics,L6,provenance_driver,write_provenance_batch,"async write_provenance_batch(records: List[Dict[str, Any]], session: Optional[AsyncSession]) -> List[int]",,AsyncSessionLocal | CostSimProvenanceModel | add | close | error | get | info | len | list | range,__future__ | asyncio | costsim_cb | db_async | sqlalchemy,db_write,yes,69,Write multiple provenance records in a single transaction.
api_keys,L5,api_keys_facade,APIKeysFacade.__init__,__init__() -> None,?:aos_api_key | L4:api_keys_handler | ?:__init__ | L5:__init__,APIKeysFacadeDriver,__future__ | api_keys_facade_driver | asyncio,pure,no,3,Initialize the facade with its driver.
api_keys,L5,api_keys_facade,APIKeysFacade.get_api_key_detail,"async get_api_key_detail(session: AsyncSession, tenant_id: str, key_id: str) -> Optional[APIKeyDetailResult]",?:aos_api_key | L4:api_keys_handler | ?:__init__ | L5:__init__,APIKeyDetailResult | fetch_api_key_by_id | loads | upper,__future__ | api_keys_facade_driver | asyncio,pure,yes,34,Get API key detail. Tenant isolation enforced.
api_keys,L5,api_keys_facade,APIKeysFacade.list_api_keys,"async list_api_keys(session: AsyncSession, tenant_id: str) -> APIKeysListResult",?:aos_api_key | L4:api_keys_handler | ?:__init__ | L5:__init__,APIKeySummaryResult | APIKeysListResult | count_api_keys | fetch_api_keys | len | upper,__future__ | api_keys_facade_driver | asyncio,pure,yes,42,List API keys for the tenant. Excludes synthetic keys.
api_keys,L5,api_keys_facade,get_api_keys_facade,get_api_keys_facade() -> APIKeysFacade,?:aos_api_key | L4:api_keys_handler | ?:__init__ | L5:__init__,APIKeysFacade,__future__ | api_keys_facade_driver | asyncio,pure,no,6,Get the singleton APIKeysFacade instance.
api_keys,L5,keys_engine,KeysReadEngine.__init__,__init__(session: Session),L4:api_keys_bridge | L3:customer_keys_adapter,get_keys_driver,__future__ | keys_driver | sqlmodel,pure,no,3,Initialize with database session.
api_keys,L5,keys_engine,KeysReadEngine.get_key,"get_key(key_id: str, tenant_id: str) -> Optional[KeySnapshot]",L4:api_keys_bridge | L3:customer_keys_adapter,fetch_key_by_id,__future__ | keys_driver | sqlmodel,pure,no,16,Get a single API key by ID with tenant isolation.
api_keys,L5,keys_engine,KeysReadEngine.get_key_usage_today,"get_key_usage_today(key_id: str, today_start: datetime) -> KeyUsageSnapshot",L4:api_keys_bridge | L3:customer_keys_adapter,fetch_key_usage,__future__ | keys_driver | sqlmodel,pure,no,16,Get today's usage for an API key.
api_keys,L5,keys_engine,KeysReadEngine.list_keys,"list_keys(tenant_id: str, limit: int, offset: int) -> Tuple[List[KeySnapshot], int]",L4:api_keys_bridge | L3:customer_keys_adapter,count_keys | fetch_keys,__future__ | keys_driver | sqlmodel,pure,no,20,List API keys for a tenant.
api_keys,L5,keys_engine,KeysWriteEngine.__init__,__init__(session: Session),L4:api_keys_bridge | L3:customer_keys_adapter,get_keys_driver,__future__ | keys_driver | sqlmodel,pure,no,3,Initialize with database session.
api_keys,L5,keys_engine,KeysWriteEngine.freeze_key,"freeze_key(key_id: str, tenant_id: str) -> Optional[KeySnapshot]",L4:api_keys_bridge | L3:customer_keys_adapter,fetch_key_for_update | now | update_key_frozen,__future__ | keys_driver | sqlmodel,pure,no,30,Freeze an API key.
api_keys,L5,keys_engine,KeysWriteEngine.unfreeze_key,"unfreeze_key(key_id: str, tenant_id: str) -> Optional[KeySnapshot]",L4:api_keys_bridge | L3:customer_keys_adapter,fetch_key_for_update | update_key_unfrozen,__future__ | keys_driver | sqlmodel,pure,no,27,Unfreeze an API key.
api_keys,L5,keys_engine,get_keys_read_engine,get_keys_read_engine(session: Session) -> KeysReadEngine,L4:api_keys_bridge | L3:customer_keys_adapter,KeysReadEngine,__future__ | keys_driver | sqlmodel,pure,no,3,Factory function to get KeysReadEngine instance.
api_keys,L5,keys_engine,get_keys_write_engine,get_keys_write_engine(session: Session) -> KeysWriteEngine,L4:api_keys_bridge | L3:customer_keys_adapter,KeysWriteEngine,__future__ | keys_driver | sqlmodel,pure,no,3,Factory function to get KeysWriteEngine instance.
api_keys,L6,api_keys_facade_driver,APIKeysFacadeDriver.count_api_keys,"async count_api_keys(session: AsyncSession, tenant_id: str) -> int",L5:api_keys_facade,count | execute | scalar | select | where,asyncio | sqlalchemy | tenant,db_write,yes,19,Count API keys for tenant. Excludes synthetic keys.
api_keys,L6,api_keys_facade_driver,APIKeysFacadeDriver.fetch_api_key_by_id,"async fetch_api_key_by_id(session: AsyncSession, tenant_id: str, key_id: str) -> Optional[APIKeyDetailSnapshot]",L5:api_keys_facade,APIKeyDetailSnapshot | execute | scalar_one_or_none | select | where,asyncio | sqlalchemy | tenant,db_write,yes,36,Fetch API key detail by ID. Tenant isolation enforced.
api_keys,L6,api_keys_facade_driver,APIKeysFacadeDriver.fetch_api_keys,"async fetch_api_keys(session: AsyncSession, tenant_id: str) -> List[APIKeySnapshot]",L5:api_keys_facade,APIKeySnapshot | all | desc | execute | limit | offset | order_by | scalars | select | where,asyncio | sqlalchemy | tenant,db_write,yes,38,Fetch API keys for tenant. Excludes synthetic keys.
api_keys,L6,keys_driver,KeysDriver.__init__,__init__(session: Session),L3:customer_keys_adapter | L5:keys_engine,,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,3,Initialize with database session.
api_keys,L6,keys_driver,KeysDriver.count_keys,count_keys(tenant_id: str) -> int,L3:customer_keys_adapter | L5:keys_engine,count | exec | first | select | where,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,5,Count API keys for a tenant.
api_keys,L6,keys_driver,KeysDriver.fetch_key_by_id,"fetch_key_by_id(key_id: str, tenant_id: str) -> Optional[KeySnapshot]",L3:customer_keys_adapter | L5:keys_engine,KeySnapshot | and_ | exec | first | hasattr | select | where,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,32,Fetch a single API key by ID with tenant isolation.
api_keys,L6,keys_driver,KeysDriver.fetch_key_for_update,"fetch_key_for_update(key_id: str, tenant_id: str) -> Optional[APIKey]",L3:customer_keys_adapter | L5:keys_engine,and_ | exec | first | hasattr | select | where,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,14,Fetch raw APIKey model for update operations.
api_keys,L6,keys_driver,KeysDriver.fetch_key_usage,"fetch_key_usage(key_id: str, since: datetime) -> KeyUsageSnapshot",L3:customer_keys_adapter | L5:keys_engine,KeyUsageSnapshot | and_ | coalesce | count | exec | first | select | sum | where,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,23,Fetch usage statistics for an API key since a given time.
api_keys,L6,keys_driver,KeysDriver.fetch_keys,"fetch_keys(tenant_id: str, limit: int, offset: int) -> List[KeySnapshot]",L3:customer_keys_adapter | L5:keys_engine,KeySnapshot | all | desc | exec | hasattr | limit | offset | order_by | select | where,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,33,Fetch API keys for a tenant.
api_keys,L6,keys_driver,KeysDriver.update_key_frozen,"update_key_frozen(key: APIKey, frozen_at: datetime) -> KeySnapshot",L3:customer_keys_adapter | L5:keys_engine,KeySnapshot | add | flush | refresh,killswitch | sqlalchemy | sqlmodel | tenant,db_write,no,25,Update key to frozen state.
api_keys,L6,keys_driver,KeysDriver.update_key_unfrozen,update_key_unfrozen(key: APIKey) -> KeySnapshot,L3:customer_keys_adapter | L5:keys_engine,KeySnapshot | add | flush | refresh,killswitch | sqlalchemy | sqlmodel | tenant,db_write,no,24,Update key to unfrozen state.
api_keys,L6,keys_driver,get_keys_driver,get_keys_driver(session: Session) -> KeysDriver,L3:customer_keys_adapter | L5:keys_engine,KeysDriver,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,3,Factory function to get KeysDriver instance.
apis,L6,keys_driver,KeysDriver.__init__,__init__(session: Session),L3:customer_keys_adapter | L5:keys_engine,,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,3,Initialize with database session.
apis,L6,keys_driver,KeysDriver.fetch_key_by_id,"fetch_key_by_id(key_id: str, tenant_id: str) -> Optional[APIKey]",L3:customer_keys_adapter | L5:keys_engine,and_ | exec | first | select | where,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,23,Fetch a single API key by ID with tenant isolation.
apis,L6,keys_driver,KeysDriver.fetch_key_usage_today,"fetch_key_usage_today(key_id: str, today_start: datetime) -> Tuple[int, int]",L3:customer_keys_adapter | L5:keys_engine,and_ | coalesce | count | exec | first | select | sum | where,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,28,Fetch today's usage for an API key.
apis,L6,keys_driver,KeysDriver.fetch_keys_paginated,"fetch_keys_paginated(tenant_id: str, limit: int, offset: int) -> Tuple[List[APIKey], int]",L3:customer_keys_adapter | L5:keys_engine,all | count | desc | exec | first | hasattr | limit | offset | order_by | select | where,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,33,Fetch API keys for a tenant with pagination.
apis,L6,keys_driver,KeysDriver.update_key_frozen,"update_key_frozen(key: APIKey, is_frozen: bool) -> APIKey",L3:customer_keys_adapter | L5:keys_engine,add | flush | now | refresh,killswitch | sqlalchemy | sqlmodel | tenant,db_write,no,21,Update the frozen status of an API key.
apis,L6,keys_driver,get_keys_driver,get_keys_driver(session: Session) -> KeysDriver,L3:customer_keys_adapter | L5:keys_engine,KeysDriver,killswitch | sqlalchemy | sqlmodel | tenant,pure,no,3,Factory function for KeysDriver.
controls,L5,alert_fatigue_engine,AlertCheckResult.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,,pure,no,9,Serialize for logging.
controls,L5,alert_fatigue_engine,AlertFatigueController.__init__,__init__(redis_client),,Lock | info,,pure,no,22,Initialize fatigue controller.
controls,L5,alert_fatigue_engine,AlertFatigueController._check_deduplication,"_check_deduplication(alert_key: str, tenant_id: str, domain: str, settings: TenantFatigueSettings) -> AlertCheckResult",,AlertCheckResult | encode | get | hexdigest | now | sha256 | timedelta,,pure,no,31,Check for duplicate alerts within dedup window.
controls,L5,alert_fatigue_engine,AlertFatigueController._check_domain_cooldown,"_check_domain_cooldown(tenant_id: str, domain: str, settings: TenantFatigueSettings) -> AlertCheckResult",,AlertCheckResult | get | get_domain_cooldown | now | timedelta,,pure,no,33,Check domain-specific cooldown.
controls,L5,alert_fatigue_engine,AlertFatigueController._check_tenant_rate_limit,"_check_tenant_rate_limit(tenant_id: str, settings: TenantFatigueSettings) -> AlertCheckResult",,AlertCheckResult | get | now | sorted | sum | timedelta,,pure,no,39,Check tenant rate limit (sliding window).
controls,L5,alert_fatigue_engine,AlertFatigueController._cleanup_old_records,_cleanup_old_records(tenant_id: str) -> None,,now | timedelta,,pure,no,11,Remove old records outside the tracking window.
controls,L5,alert_fatigue_engine,AlertFatigueController._get_tenant_settings,_get_tenant_settings(tenant_id: str) -> TenantFatigueSettings,,TenantFatigueSettings,,pure,no,5,Get settings for a tenant (with defaults).
controls,L5,alert_fatigue_engine,AlertFatigueController.check_alert,"check_alert(alert_key: str, tenant_id: str, domain: str) -> AlertCheckResult",,AlertCheckResult | _check_deduplication | _check_domain_cooldown | _check_tenant_rate_limit | _get_tenant_settings,,pure,no,59,Check if an alert should be sent.
controls,L5,alert_fatigue_engine,AlertFatigueController.get_tenant_stats,"get_tenant_stats(tenant_id: str) -> Dict[str, Any]",,_get_tenant_settings | get | len | now | timedelta,,pure,no,30,Get fatigue statistics for a tenant.
controls,L5,alert_fatigue_engine,AlertFatigueController.record_alert_sent,"record_alert_sent(alert_key: str, tenant_id: str, domain: str) -> None",,AlertRecord | _cleanup_old_records | append | debug,,pure,no,45,Record that an alert was sent.
controls,L5,alert_fatigue_engine,AlertFatigueController.set_tenant_settings,"set_tenant_settings(tenant_id: str, settings: TenantFatigueSettings) -> None",,info,,pure,no,23,Set custom fatigue settings for a tenant.
controls,L5,alert_fatigue_engine,AlertFatigueController.should_send_alert,"should_send_alert(alert_key: str, tenant_id: str, domain: str) -> bool",,check_alert,,pure,no,18,Simple check if alert should be sent (convenience method).
controls,L5,alert_fatigue_engine,AlertRecord.__post_init__,__post_init__(),,encode | hexdigest | sha256,,pure,no,5,Compute alert hash for deduplication.
controls,L5,alert_fatigue_engine,AlertRecord.age,age() -> timedelta,,now,,pure,no,3,Time since alert was sent.
controls,L5,alert_fatigue_engine,TenantFatigueSettings.get_domain_cooldown,get_domain_cooldown(domain: str) -> int,,get,,pure,no,5,Get cooldown for a domain.
controls,L5,alert_fatigue_engine,get_alert_fatigue_controller,get_alert_fatigue_controller(redis_client) -> AlertFatigueController,,AlertFatigueController,,pure,no,14,Get or create AlertFatigueController singleton.
controls,L5,alert_fatigue_engine,reset_alert_fatigue_controller,reset_alert_fatigue_controller() -> None,,,,pure,no,4,Reset the singleton (for testing).
controls,L5,budget_enforcement_engine,BudgetEnforcementEngine.__init__,__init__(),?:main,get,budget_enforcement_driver | decisions,pure,no,3,Initialize the budget enforcement engine.
controls,L5,budget_enforcement_engine,BudgetEnforcementEngine._parse_budget_from_error,_parse_budget_from_error(error_message: str) -> Optional[dict],?:main,group | int | search,budget_enforcement_driver | decisions,pure,no,16,Parse budget information from error message.
controls,L5,budget_enforcement_engine,BudgetEnforcementEngine.emit_decision_for_halt,"emit_decision_for_halt(run_id: str, budget_limit_cents: int, budget_consumed_cents: int, step_cost_cents: int, completed_steps: int, total_steps: int, tenant_id: str) -> bool",?:main,debug | emit_budget_enforcement_decision | error | info | str,budget_enforcement_driver | decisions,pure,no,72,Emit budget enforcement decision for a halted run.
controls,L5,budget_enforcement_engine,BudgetEnforcementEngine.process_pending_halts,process_pending_halts() -> int,?:main,_parse_budget_from_error | dispose | emit_decision_for_halt | error | fetch_pending_budget_halts | get | get_budget_enforcement_driver | info | len | loads | str | warning,budget_enforcement_driver | decisions,pure,no,96,Process runs halted for budget that don't have decision records.
controls,L5,budget_enforcement_engine,emit_budget_halt_decision,"emit_budget_halt_decision(run_id: str, budget_limit_cents: int, budget_consumed_cents: int, step_cost_cents: int, completed_steps: int, total_steps: int, tenant_id: str) -> bool",?:main,BudgetEnforcementEngine | emit_decision_for_halt,budget_enforcement_driver | decisions,pure,no,40,Convenience function to emit a budget enforcement decision.
controls,L5,budget_enforcement_engine,process_pending_budget_decisions,async process_pending_budget_decisions() -> int,?:main,BudgetEnforcementEngine | process_pending_halts,budget_enforcement_driver | decisions,pure,yes,15,Process all pending budget halt decisions.
controls,L5,cb_sync_wrapper_engine,_get_executor,_get_executor() -> concurrent.futures.ThreadPoolExecutor,,ThreadPoolExecutor,__future__ | circuit_breaker_async | futures,pure,no,6,Get or create the shared thread pool executor.
controls,L5,cb_sync_wrapper_engine,_run_async_in_thread,"_run_async_in_thread(coro, timeout: float)",,_get_executor | close | new_event_loop | result | run_until_complete | set_event_loop | submit,__future__ | circuit_breaker_async | futures,pure,no,32,Run an async coroutine in a separate thread with its own event loop.
controls,L5,cb_sync_wrapper_engine,get_state_sync,get_state_sync(timeout: float),,_run_async_in_thread | error | get_running_loop | get_state | run,__future__ | circuit_breaker_async | futures,pure,no,27,Sync wrapper for get_state().
controls,L5,cb_sync_wrapper_engine,is_v2_disabled_sync,is_v2_disabled_sync(timeout: float) -> bool,,_run_async_in_thread | bool | error | get_running_loop | is_v2_disabled | run,__future__ | circuit_breaker_async | futures,pure,no,37,Sync wrapper for is_v2_disabled().
controls,L5,cb_sync_wrapper_engine,shutdown_executor,shutdown_executor(),,shutdown,__future__ | circuit_breaker_async | futures,pure,no,6,Shutdown the thread pool executor gracefully.
controls,L5,controls_facade,ControlConfig.to_dict,"to_dict() -> Dict[str, Any]",L4:controls_handler,,,pure,no,18,Convert to dictionary.
controls,L5,controls_facade,ControlStatusSummary.to_dict,"to_dict() -> Dict[str, Any]",L4:controls_handler,,,pure,no,12,Convert to dictionary.
controls,L5,controls_facade,ControlsFacade.__init__,__init__(),L4:controls_handler,,,pure,no,3,Initialize facade with default controls.
controls,L5,controls_facade,ControlsFacade._ensure_default_controls,_ensure_default_controls(tenant_id: str) -> None,L4:controls_handler,ControlConfig | isoformat | now | str | uuid4,,pure,no,28,Ensure default controls exist for tenant.
controls,L5,controls_facade,ControlsFacade.disable_control,"async disable_control(control_id: str, tenant_id: str, actor: str) -> Optional[ControlConfig]",L4:controls_handler,_ensure_default_controls | info | isoformat | now | values,,pure,yes,40,Disable a control.
controls,L5,controls_facade,ControlsFacade.enable_control,"async enable_control(control_id: str, tenant_id: str, actor: str) -> Optional[ControlConfig]",L4:controls_handler,_ensure_default_controls | info | isoformat | now | values,,pure,yes,40,Enable a control.
controls,L5,controls_facade,ControlsFacade.get_control,"async get_control(control_id: str, tenant_id: str) -> Optional[ControlConfig]",L4:controls_handler,_ensure_default_controls | values,,pure,yes,21,Get a specific control.
controls,L5,controls_facade,ControlsFacade.get_status,async get_status(tenant_id: str) -> ControlStatusSummary,L4:controls_handler,ControlStatusSummary | _ensure_default_controls | isoformat | now | values,,pure,yes,48,Get overall control status.
controls,L5,controls_facade,ControlsFacade.list_controls,"async list_controls(tenant_id: str, control_type: Optional[str], state: Optional[str], limit: int, offset: int) -> List[ControlConfig]",L4:controls_handler,_ensure_default_controls | append | sort | values,,pure,yes,35,List controls for a tenant.
controls,L5,controls_facade,ControlsFacade.update_control,"async update_control(control_id: str, tenant_id: str, conditions: Optional[Dict[str, Any]], metadata: Optional[Dict[str, Any]]) -> Optional[ControlConfig]",L4:controls_handler,_ensure_default_controls | isoformat | now | update | values,,pure,yes,39,Update a control.
controls,L5,controls_facade,get_controls_facade,get_controls_facade() -> ControlsFacade,L4:controls_handler,ControlsFacade,,pure,no,14,Get the controls facade instance.
controls,L5,cost_safety_rails_engine,CostSafetyRails.__init__,"__init__(config: SafetyConfig | None, redis_client, db_session)",,SafetyConfig,__future__ | cost_bridges,pure,no,13,
controls,L5,cost_safety_rails_engine,CostSafetyRails._get_action_count,"async _get_action_count(tenant_id: str, action_type: str) -> int",,get | int | warning,__future__ | cost_bridges,external_api,yes,14,Get current action count for tenant.
controls,L5,cost_safety_rails_engine,CostSafetyRails.can_auto_apply_policy,"async can_auto_apply_policy(tenant_id: str, policy_action: str, severity: str) -> tuple[bool, str]",,_get_action_count | get | now | total_seconds | upper,__future__ | cost_bridges,pure,yes,32,Check if a policy can be auto-applied.
controls,L5,cost_safety_rails_engine,CostSafetyRails.can_auto_apply_recovery,"async can_auto_apply_recovery(tenant_id: str, recovery_action: str, affected_count: int) -> tuple[bool, str]",,_get_action_count,__future__ | cost_bridges,pure,yes,24,Check if a recovery action can be auto-applied.
controls,L5,cost_safety_rails_engine,CostSafetyRails.can_auto_apply_routing,"async can_auto_apply_routing(tenant_id: str, adjustment_type: str, magnitude: float) -> tuple[bool, str]",,_get_action_count,__future__ | cost_bridges,pure,yes,24,Check if a routing adjustment can be auto-applied.
controls,L5,cost_safety_rails_engine,CostSafetyRails.get_status,"get_status(tenant_id: str) -> dict[str, Any]",,get | max,__future__ | cost_bridges,pure,no,29,Get current safety rail status for tenant.
controls,L5,cost_safety_rails_engine,CostSafetyRails.record_action,"async record_action(tenant_id: str, action_type: str, action_name: str) -> None",,expire | incr | info | now | warning,__future__ | cost_bridges,pure,yes,32,Record an auto-applied action.
controls,L5,cost_safety_rails_engine,SafeCostLoopOrchestrator.__init__,"__init__(db_session, safety_config: SafetyConfig | None, redis_client)",,CostLoopOrchestrator | CostSafetyRails | ValueError,__future__ | cost_bridges,pure,no,21,Initialize safe orchestrator with database session.
controls,L5,cost_safety_rails_engine,SafeCostLoopOrchestrator.process_anomaly_safe,"async process_anomaly_safe(anomaly) -> dict[str, Any]",,append | can_auto_apply_policy | can_auto_apply_routing | get | get_status | len | process_anomaly,__future__ | cost_bridges,pure,yes,50,Process anomaly with safety rails enforced.
controls,L5,cost_safety_rails_engine,SafetyConfig.production,production() -> 'SafetyConfig',,cls,__future__ | cost_bridges,pure,no,13,Conservative production defaults.
controls,L5,cost_safety_rails_engine,SafetyConfig.testing,testing() -> 'SafetyConfig',,cls,__future__ | cost_bridges,pure,no,13,Relaxed testing defaults.
controls,L5,cost_safety_rails_engine,get_safety_rails,get_safety_rails(config: SafetyConfig | None) -> CostSafetyRails,,CostSafetyRails,__future__ | cost_bridges,pure,no,6,Get or create default safety rails instance.
controls,L5,customer_killswitch_read_engine,CustomerKillswitchReadService.__init__,__init__(session: Optional['Session']),L3:customer_killswitch_adapter,get_killswitch_read_driver,killswitch_read_driver | pydantic | sqlmodel,pure,no,3,Initialize service with optional session (passed to driver).
controls,L5,customer_killswitch_read_engine,CustomerKillswitchReadService.get_killswitch_status,get_killswitch_status(tenant_id: str) -> KillswitchStatusInfo,L3:customer_killswitch_adapter,IncidentStats | KillswitchState | KillswitchStatusInfo | get_killswitch_status,killswitch_read_driver | pydantic | sqlmodel,pure,no,34,Get complete killswitch status for a tenant.
controls,L5,customer_killswitch_read_engine,get_customer_killswitch_read_service,get_customer_killswitch_read_service() -> CustomerKillswitchReadService,L3:customer_killswitch_adapter,CustomerKillswitchReadService,killswitch_read_driver | pydantic | sqlmodel,pure,no,14,Get the singleton CustomerKillswitchReadService instance.
controls,L5,decisions_engine,AnomalySignal.to_signal_response,to_signal_response() -> dict,,,,pure,no,12,Convert to signal format.
controls,L5,decisions_engine,Decision.blocks_request,blocks_request() -> bool,,,,pure,no,3,Check if this decision blocks the request.
controls,L5,decisions_engine,Decision.is_warning_only,is_warning_only() -> bool,,,,pure,no,3,Check if this is a non-blocking warning.
controls,L5,decisions_engine,ProtectionResult.to_error_response,to_error_response() -> dict,,,,pure,no,28,Convert to error response format.
controls,L5,decisions_engine,allow,allow() -> ProtectionResult,,ProtectionResult,,pure,no,3,Create an ALLOW result.
controls,L5,decisions_engine,reject_cost_limit,"reject_cost_limit(current_value: float, allowed_value: float, message: Optional[str]) -> ProtectionResult",,ProtectionResult,,pure,no,11,Create a REJECT result for cost limit.
controls,L5,decisions_engine,reject_rate_limit,"reject_rate_limit(dimension: str, retry_after_ms: int, message: Optional[str]) -> ProtectionResult",,ProtectionResult,,pure,no,10,Create a REJECT result for rate limiting.
controls,L5,decisions_engine,throttle,"throttle(dimension: str, retry_after_ms: int, message: Optional[str]) -> ProtectionResult",,ProtectionResult,,pure,no,10,Create a THROTTLE result.
controls,L5,decisions_engine,warn,"warn(dimension: str, message: Optional[str]) -> ProtectionResult",,ProtectionResult,,pure,no,7,Create a WARN result (non-blocking).
controls,L5,killswitch_engine,KillSwitch.__init__,__init__(),,Lock,,pure,no,5,
controls,L5,killswitch_engine,KillSwitch.activate,"activate(reason: str, triggered_by: KillSwitchTrigger, active_envelopes_count: int) -> KillSwitchEvent",,KillSwitchEvent | append | callback | error | list | str | warning,,pure,no,64,Activate kill-switch. Immediately disables all optimization.
controls,L5,killswitch_engine,KillSwitch.get_events,get_events() -> List[KillSwitchEvent],,list,,pure,no,4,Get all kill-switch events for audit.
controls,L5,killswitch_engine,KillSwitch.get_last_event,get_last_event() -> Optional[KillSwitchEvent],,,,pure,no,4,Get most recent kill-switch event.
controls,L5,killswitch_engine,KillSwitch.is_disabled,is_disabled() -> bool,,,,pure,no,3,True if optimization is blocked.
controls,L5,killswitch_engine,KillSwitch.is_enabled,is_enabled() -> bool,,,,pure,no,3,True if optimization is allowed.
controls,L5,killswitch_engine,KillSwitch.mark_rollback_complete,"mark_rollback_complete(event_id: str, status: RollbackStatus) -> None",,info | isoformat | now,,pure,no,27,Mark rollback as complete for a kill-switch event.
controls,L5,killswitch_engine,KillSwitch.on_activate,"on_activate(callback: Callable[[KillSwitchEvent], None]) -> None",,append,,pure,no,4,Register callback to be called when kill-switch activates.
controls,L5,killswitch_engine,KillSwitch.rearm,rearm(reason: str) -> None,,warning,,pure,no,16,Re-enable optimization. Requires EXPLICIT human action.
controls,L5,killswitch_engine,KillSwitch.state,state() -> KillSwitchState,,,,pure,no,4,Current kill-switch state.
controls,L5,killswitch_engine,get_killswitch,get_killswitch() -> KillSwitch,,KillSwitch,,pure,no,7,Get the global kill-switch instance.
controls,L5,killswitch_engine,reset_killswitch_for_testing,reset_killswitch_for_testing() -> None,,KillSwitch,,pure,no,5,Reset kill-switch state. FOR TESTING ONLY.
controls,L5,overrides,LimitOverrideRequest.validate_override_value,validate_override_value(v: Decimal) -> Decimal,?:override | ?:__init__ | ?:override_service | L4:controls_handler | L6:override_driver | L2:override,ValueError | field_validator,pydantic,pure,no,5,Override value must be positive.
controls,L5,overrides,OverrideApprovalRequest.validate_rejection_reason,"validate_rejection_reason(v: Optional[str], info) -> Optional[str]",?:override | ?:__init__ | ?:override_service | L4:controls_handler | L6:override_driver | L2:override,field_validator,pydantic,pure,no,4,Rejection reason required when rejecting.
controls,L5,policy_limits,CreatePolicyLimitRequest.validate_reset_period,"validate_reset_period(v: Optional[ResetPeriodEnum], info) -> Optional[ResetPeriodEnum]",?:policy_limits_crud | ?:__init__ | ?:policy_limits_service | L5:policy_limits_engine | L2:policy_limits_crud | ?:test_limit_enhancements,field_validator,pydantic,pure,no,5,Reset period required for BUDGET limits.
controls,L5,policy_limits,CreatePolicyLimitRequest.validate_window_seconds,"validate_window_seconds(v: Optional[int], info) -> Optional[int]",?:policy_limits_crud | ?:__init__ | ?:policy_limits_service | L5:policy_limits_engine | L2:policy_limits_crud | ?:test_limit_enhancements,field_validator,pydantic,pure,no,3,Window seconds required for RATE limits.
controls,L5,s2_cost_smoothing_engine,calculate_s2_bounded_value,"calculate_s2_bounded_value(baseline: float, max_decrease_pct: float, prediction_confidence: float) -> float",,max,envelope,pure,no,30,Calculate the bounded value for S2 (decrease only).
controls,L5,s2_cost_smoothing_engine,create_s2_envelope,"create_s2_envelope(baseline_value: float, reference_id: str) -> Envelope",,Envelope | EnvelopeBaseline | EnvelopeBounds | EnvelopeScope | EnvelopeTimebox | EnvelopeTrigger,envelope,pure,no,50,Create a fresh S2 envelope instance with specified baseline.
controls,L5,s2_cost_smoothing_engine,validate_s2_envelope,validate_s2_envelope(envelope: Envelope) -> None,,EnvelopeValidationError,envelope,pure,no,33,Validate S2-specific rules (additive to V1-V5).
controls,L5,threshold_engine,LLMRunEvaluator.__init__,__init__(resolver: LLMRunThresholdResolver),?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,,pydantic | threshold_driver | threshold_signals,pure,no,2,
controls,L5,threshold_engine,LLMRunEvaluator.evaluate_completed_run,"async evaluate_completed_run(run_id: str, tenant_id: str, status: str, execution_time_ms: int, tokens_used: int, cost_usd: float, agent_id: Optional[str], project_id: Optional[str]) -> ThresholdEvaluationResult",?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,ThresholdEvaluationResult | append | info | model_dump | now | resolve,pydantic | threshold_driver | threshold_signals,pure,yes,77,Evaluate a completed LLM run.
controls,L5,threshold_engine,LLMRunEvaluator.evaluate_live_run,"async evaluate_live_run(run_id: str, tenant_id: str, started_at_ms: int, tokens_used: int, agent_id: Optional[str], project_id: Optional[str]) -> ThresholdEvaluationResult",?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,ThresholdEvaluationResult | append | info | int | model_dump | now | resolve | timestamp,pydantic | threshold_driver | threshold_signals,pure,yes,59,Evaluate a live (running) LLM run.
controls,L5,threshold_engine,LLMRunEvaluatorSync.__init__,__init__(resolver: LLMRunThresholdResolverSync),?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,,pydantic | threshold_driver | threshold_signals,pure,no,2,
controls,L5,threshold_engine,LLMRunEvaluatorSync.evaluate_completed_run,"evaluate_completed_run(run_id: str, tenant_id: str, status: str, execution_time_ms: int, tokens_used: int, cost_usd: float, agent_id: Optional[str], project_id: Optional[str]) -> ThresholdEvaluationResult",?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,ThresholdEvaluationResult | append | info | model_dump | now | resolve,pydantic | threshold_driver | threshold_signals,pure,no,60,Evaluate a completed LLM run (sync version).
controls,L5,threshold_engine,LLMRunThresholdResolver.__init__,__init__(driver: 'ThresholdDriver'),?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,,pydantic | threshold_driver | threshold_signals,pure,no,8,Initialize resolver with a driver.
controls,L5,threshold_engine,LLMRunThresholdResolver.resolve,"async resolve(tenant_id: str, agent_id: Optional[str], project_id: Optional[str]) -> ThresholdParams",?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,ThresholdParams | copy | debug | get_active_threshold_limits | items | str | warning,pydantic | threshold_driver | threshold_signals,pure,yes,64,Resolve effective threshold params for a run.
controls,L5,threshold_engine,LLMRunThresholdResolverSync.__init__,__init__(driver: 'ThresholdDriverSync'),?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,,pydantic | threshold_driver | threshold_signals,pure,no,8,Initialize resolver with a sync driver.
controls,L5,threshold_engine,LLMRunThresholdResolverSync.resolve,"resolve(tenant_id: str, agent_id: Optional[str], project_id: Optional[str]) -> ThresholdParams",?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,ThresholdParams | copy | debug | get_active_threshold_limits | items | str | warning,pydantic | threshold_driver | threshold_signals,pure,no,55,Resolve effective threshold params for a run (sync version).
controls,L5,threshold_engine,ThresholdDriverProtocol.get_active_threshold_limits,async get_active_threshold_limits(tenant_id: str) -> list['LimitSnapshot'],?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,,pydantic | threshold_driver | threshold_signals,pure,yes,5,Query active threshold limits for a tenant.
controls,L5,threshold_engine,ThresholdDriverSyncProtocol.get_active_threshold_limits,get_active_threshold_limits(tenant_id: str) -> list['LimitSnapshot'],?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,,pydantic | threshold_driver | threshold_signals,pure,no,5,Query active threshold limits for a tenant (sync).
controls,L5,threshold_engine,ThresholdParams.coerce_decimal_to_float,coerce_decimal_to_float(v),?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,field_validator | float | isinstance,pydantic | threshold_driver | threshold_signals,pure,no,5,Handle Decimal input from database.
controls,L5,threshold_engine,collect_signals_from_evaluation,"collect_signals_from_evaluation(evaluation: ThresholdEvaluationResult, tenant_id: str, state: str) -> list[ThresholdSignalRecord]",?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,append | create_threshold_signal_record,pydantic | threshold_driver | threshold_signals,pure,no,28,Collect all signals from an evaluation result into records.
controls,L5,threshold_engine,create_threshold_signal_record,"create_threshold_signal_record(tenant_id: str, run_id: str, state: str, signal: ThresholdSignal, params_used: dict) -> ThresholdSignalRecord",?:policy_limits_crud | ?:runner | L4:controls_handler | L5s:threshold_signals,ThresholdSignalRecord | info | now,pydantic | threshold_driver | threshold_signals,pure,no,41,Create a threshold signal record for activity domain.
controls,L6,budget_enforcement_driver,BudgetEnforcementDriver.__init__,__init__(db_url: Optional[str]),L5:budget_enforcement_engine,get,sqlalchemy,pure,no,4,Initialize driver with database URL.
controls,L6,budget_enforcement_driver,BudgetEnforcementDriver._get_engine,_get_engine(),L5:budget_enforcement_engine,RuntimeError | create_engine,sqlalchemy,pure,no,7,Lazy engine creation.
controls,L6,budget_enforcement_driver,BudgetEnforcementDriver.dispose,dispose() -> None,L5:budget_enforcement_engine,dispose,sqlalchemy,pure,no,5,Dispose of the engine connection pool.
controls,L6,budget_enforcement_driver,BudgetEnforcementDriver.fetch_pending_budget_halts,"fetch_pending_budget_halts(limit: int) -> list[dict[str, Any]]",L5:budget_enforcement_engine,_get_engine | append | connect | error | execute | text,sqlalchemy,pure,no,48,Fetch runs halted for budget that don't have decision records.
controls,L6,budget_enforcement_driver,get_budget_enforcement_driver,get_budget_enforcement_driver(db_url: Optional[str]) -> BudgetEnforcementDriver,L5:budget_enforcement_engine,BudgetEnforcementDriver,sqlalchemy,pure,no,3,Get a BudgetEnforcementDriver instance.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.__init__,"__init__(failure_threshold: Optional[int], drift_threshold: Optional[float], name: str)",,get_config,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,no,11,
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.disable_v2,"async disable_v2(reason: str, disabled_by: str, disabled_until: Optional[datetime]) -> Tuple[bool, Optional[Incident]]",,disable_v2,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,8,Disable V2.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.enable_v2,"async enable_v2(enabled_by: str, reason: Optional[str]) -> bool",,enable_v2,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,7,Enable V2.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.get_incidents,"get_incidents(include_resolved: bool, limit: int) -> List[Incident]",,get_event_loop | get_incidents | is_running | run | run_until_complete | warning,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,no,16,Get incidents (runs async in sync context).
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.get_state,async get_state() -> CircuitBreakerState,,get_state,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,3,Get current state.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.is_closed,is_closed() -> bool,,is_open,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,file_io,no,3,Check if circuit breaker is closed.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.is_disabled,async is_disabled() -> bool,,is_v2_disabled,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,3,Check if V2 is disabled.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.is_open,is_open() -> bool,,is_v2_disabled_sync,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,file_io,no,10,Sync check if circuit breaker is open.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.report_drift,"async report_drift(drift_score: float, sample_count: int, details: Optional[Dict[str, Any]]) -> Optional[Incident]",,report_drift,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,8,Report drift observation.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.report_schema_error,"async report_schema_error(error_count: int, details: Optional[Dict[str, Any]]) -> Optional[Incident]",,report_schema_error,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,7,Report schema errors.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.reset,"async reset(reason: Optional[str], reset_by: Optional[str]) -> bool",,enable_v2,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,7,Reset circuit breaker.
controls,L6,circuit_breaker_async_driver,AsyncCircuitBreaker.reset_v2,"async reset_v2(reason: Optional[str], reset_by: Optional[str]) -> bool",,reset,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,7,Reset circuit breaker (alias for reset).
controls,L6,circuit_breaker_async_driver,CircuitBreakerState.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,no,12,Convert to dictionary.
controls,L6,circuit_breaker_async_driver,Incident.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,no,17,Convert to dictionary.
controls,L6,circuit_breaker_async_driver,_auto_recover,"async _auto_recover(session: AsyncSession, state: CostSimCBStateModel) -> None",,_build_enable_alert_payload | _enqueue_alert | _resolve_incident | info | now,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,45,Legacy auto-recover function (deprecated).
controls,L6,circuit_breaker_async_driver,_build_disable_alert_payload,"_build_disable_alert_payload(incident: Incident, disabled_until: Optional[datetime]) -> List[Dict[str, Any]]",,get_config | isoformat | lower | now,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,no,30,Build Alertmanager payload for disable alert.
controls,L6,circuit_breaker_async_driver,_build_enable_alert_payload,"_build_enable_alert_payload(enabled_by: str, reason: Optional[str]) -> List[Dict[str, Any]]",,get_config | isoformat | now,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,no,25,Build Alertmanager payload for enable/resolved alert.
controls,L6,circuit_breaker_async_driver,_enqueue_alert,"async _enqueue_alert(session: AsyncSession, alert_type: str, payload: List[Dict[str, Any]], incident_id: Optional[str]) -> None",,CostSimAlertQueueModel | add | debug | flush,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,db_write,yes,26,Enqueue alert for reliable delivery.
controls,L6,circuit_breaker_async_driver,_get_or_create_state,"async _get_or_create_state(session: AsyncSession, lock: bool) -> CostSimCBStateModel",,CostSimCBStateModel | add | execute | first | flush | now | scalars | select | where | with_for_update,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,db_write,yes,36,Get or create circuit breaker state row.
controls,L6,circuit_breaker_async_driver,_resolve_incident,"async _resolve_incident(session: AsyncSession, incident_id: str, resolved_by: str, resolution_notes: str) -> None",,execute | first | now | scalars | select | where,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,db_write,yes,15,Resolve an incident.
controls,L6,circuit_breaker_async_driver,_trip,"async _trip(session: AsyncSession, state: CostSimCBStateModel, reason: str, drift_score: float, sample_count: int, details: Optional[Dict[str, Any]], severity: str, disabled_by: str, disabled_until: Optional[datetime]) -> Incident",,CostSimCBIncidentModel | Incident | _build_disable_alert_payload | _enqueue_alert | add | dumps | error | flush | get_config | get_metrics | now | record_cb_disabled | record_cb_incident | set_circuit_breaker_state | str,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,db_write,yes,91,Trip the circuit breaker.
controls,L6,circuit_breaker_async_driver,_try_auto_recover,async _try_auto_recover(state_id: int) -> bool,,AsyncSessionLocal | _build_enable_alert_payload | _enqueue_alert | _resolve_incident | async_session_context | begin | error | execute | first | flush | get_metrics | info | now | record_auto_recovery | record_cb_enabled,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,db_write,yes,90,Attempt auto-recovery with proper locking to avoid TOCTOU race.
controls,L6,circuit_breaker_async_driver,disable_v2,"async disable_v2(reason: str, disabled_by: str, disabled_until: Optional[datetime]) -> Tuple[bool, Optional[Incident]]",,AsyncSessionLocal | _get_or_create_state | _trip | begin,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,39,Manually disable CostSim V2.
controls,L6,circuit_breaker_async_driver,enable_v2,"async enable_v2(enabled_by: str, reason: Optional[str]) -> bool",,AsyncSessionLocal | _build_enable_alert_payload | _enqueue_alert | _get_or_create_state | _resolve_incident | begin | get_metrics | info | now | record_cb_enabled | set_circuit_breaker_state | set_consecutive_failures,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,64,Manually enable CostSim V2.
controls,L6,circuit_breaker_async_driver,get_async_circuit_breaker,get_async_circuit_breaker() -> AsyncCircuitBreaker,,AsyncCircuitBreaker,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,no,6,Get the global async circuit breaker instance.
controls,L6,circuit_breaker_async_driver,get_incidents,"async get_incidents(include_resolved: bool, limit: int) -> List[Incident]",,Incident | append | async_session_context | desc | execute | get_details | limit | order_by | scalars | select | where,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,db_write,yes,45,Get recent incidents.
controls,L6,circuit_breaker_async_driver,get_state,async get_state() -> CircuitBreakerState,,CircuitBreakerState | _get_or_create_state | async_session_context,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,15,Get current circuit breaker state.
controls,L6,circuit_breaker_async_driver,is_v2_disabled,async is_v2_disabled(session: Optional[AsyncSession]) -> bool,,AsyncSessionLocal | _try_auto_recover | close | execute | first | get_config | limit | now | replace | scalars | select | where,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,db_write,yes,59,Check if V2 is disabled.
controls,L6,circuit_breaker_async_driver,report_drift,"async report_drift(drift_score: float, sample_count: int, details: Optional[Dict[str, Any]]) -> Optional[Incident]",,AsyncSessionLocal | _get_or_create_state | _trip | begin | get_config | get_metrics | info | now | set_consecutive_failures | warning,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,60,Report drift observation.
controls,L6,circuit_breaker_async_driver,report_schema_error,"async report_schema_error(error_count: int, details: Optional[Dict[str, Any]]) -> Optional[Incident]",,AsyncSessionLocal | _get_or_create_state | _trip | begin | get_config,__future__ | asyncio | cb_sync_wrapper | config | costsim_cb | db_async | infra | metrics | sqlalchemy,pure,yes,33,Report schema validation errors.
controls,L6,circuit_breaker_driver,CircuitBreaker.__init__,"__init__(session: Session, failure_threshold: Optional[int], drift_threshold: Optional[float], name: str)",,Path | get | getLogger | get_config | mkdir | warning,__future__ | config | db | httpx | infra | sqlmodel,pure,no,45,Initialize circuit breaker.
controls,L6,circuit_breaker_driver,CircuitBreaker._auto_recover,async _auto_recover(session: Session) -> None,,_get_or_create_state | _resolve_incident_db | _send_alert_enable | info | log_status_change | now,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,47,Auto-recover circuit breaker after TTL expires.
controls,L6,circuit_breaker_driver,CircuitBreaker._get_or_create_state,_get_or_create_state(session: Session) -> CostSimCBState,,CostSimCBState | add | exec | first | flush | hasattr | refresh | select | where | with_for_update,__future__ | config | db | httpx | infra | sqlmodel,db_write,no,31,Get or create circuit breaker state row.
controls,L6,circuit_breaker_driver,CircuitBreaker._post_alertmanager,"async _post_alertmanager(payload: List[Dict[str, Any]]) -> bool",,AsyncClient | error | get | info | post | raise_for_status | range | sleep | warning,__future__ | config | db | httpx | infra | sqlmodel,external_api,yes,44,Post alert to Alertmanager with retry logic.
controls,L6,circuit_breaker_driver,CircuitBreaker._resolve_incident_db,"_resolve_incident_db(session: Session, incident_id: str, resolved_by: str, resolution_notes: str) -> None",,exec | first | hasattr | now | select | where,__future__ | config | db | httpx | infra | sqlmodel,pure,no,23,Resolve an incident in the database.
controls,L6,circuit_breaker_driver,CircuitBreaker._save_incident_file,_save_incident_file(incident: Incident) -> None,,dump | error | open | to_dict,__future__ | config | db | httpx | infra | sqlmodel,file_io,no,8,Save incident to file (legacy backup).
controls,L6,circuit_breaker_driver,CircuitBreaker._send_alert_disable,"async _send_alert_disable(incident: Incident, disabled_until: Optional[datetime]) -> bool",,_post_alertmanager | isoformat | lower | now | warning,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,37,Send P1/P2/P3 alert when circuit breaker trips.
controls,L6,circuit_breaker_driver,CircuitBreaker._send_alert_enable,"async _send_alert_enable(enabled_by: str, reason: Optional[str]) -> bool",,_post_alertmanager | isoformat | now | warning,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,32,Send resolved alert when circuit breaker is re-enabled.
controls,L6,circuit_breaker_driver,CircuitBreaker._trip,"async _trip(session: Session, reason: str, drift_score: float, sample_count: int, details: Optional[Dict[str, Any]], severity: str, disabled_by: str, disabled_until: Optional[datetime]) -> Incident",,CostSimCBIncident | Incident | _get_or_create_state | _save_incident_file | _send_alert_disable | add | dumps | error | isoformat | log_status_change | now | str | timedelta | uuid4,__future__ | config | db | httpx | infra | sqlmodel,db_write,yes,107,Trip the circuit breaker.
controls,L6,circuit_breaker_driver,CircuitBreaker.disable_v2,"async disable_v2(reason: str, disabled_by: str, disabled_until: Optional[datetime]) -> Tuple[bool, Optional[Incident]]",,_get_or_create_state | _trip,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,42,Manually disable CostSim V2.
controls,L6,circuit_breaker_driver,CircuitBreaker.enable_v2,"async enable_v2(enabled_by: str, reason: Optional[str]) -> bool",,reset,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,18,Manually enable CostSim V2.
controls,L6,circuit_breaker_driver,CircuitBreaker.get_incidents,"get_incidents(include_resolved: bool, limit: int) -> List[Incident]",,Incident | append | cast | desc | exec | get_details | limit | order_by | select | where,__future__ | config | db | httpx | infra | sqlmodel,pure,no,46,Get recent incidents from database.
controls,L6,circuit_breaker_driver,CircuitBreaker.get_state,get_state() -> CircuitBreakerState,,CircuitBreakerState | _get_or_create_state,__future__ | config | db | httpx | infra | sqlmodel,pure,no,14,Get current circuit breaker state.
controls,L6,circuit_breaker_driver,CircuitBreaker.is_closed,is_closed() -> bool,,is_open,__future__ | config | db | httpx | infra | sqlmodel,file_io,no,3,Check if circuit breaker is closed (V2 enabled).
controls,L6,circuit_breaker_driver,CircuitBreaker.is_disabled,async is_disabled() -> bool,,_auto_recover | _get_or_create_state | now | replace,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,33,Check if V2 is disabled.
controls,L6,circuit_breaker_driver,CircuitBreaker.is_open,is_open() -> bool,,_get_or_create_state,__future__ | config | db | httpx | infra | sqlmodel,file_io,no,9,Synchronous check if circuit breaker is open (V2 disabled).
controls,L6,circuit_breaker_driver,CircuitBreaker.report_drift,"async report_drift(drift_score: float, sample_count: int, details: Optional[Dict[str, Any]]) -> Optional[Incident]",,_get_or_create_state | _trip | info | now | warning,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,54,Report drift observation.
controls,L6,circuit_breaker_driver,CircuitBreaker.report_schema_error,"async report_schema_error(error_count: int, details: Optional[Dict[str, Any]]) -> Optional[Incident]",,_trip,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,28,Report schema validation errors.
controls,L6,circuit_breaker_driver,CircuitBreaker.reset,"async reset(reason: Optional[str], reset_by: Optional[str]) -> bool",,_get_or_create_state | _resolve_incident_db | _send_alert_enable | info | log_status_change | now,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,72,Reset the circuit breaker.
controls,L6,circuit_breaker_driver,CircuitBreakerState.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,__future__ | config | db | httpx | infra | sqlmodel,pure,no,12,Convert to dictionary.
controls,L6,circuit_breaker_driver,Incident.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,__future__ | config | db | httpx | infra | sqlmodel,pure,no,17,Convert to dictionary.
controls,L6,circuit_breaker_driver,create_circuit_breaker,"create_circuit_breaker(session: Session, failure_threshold: Optional[int], drift_threshold: Optional[float], name: str) -> CircuitBreaker",,CircuitBreaker,__future__ | config | db | httpx | infra | sqlmodel,pure,no,27,Create CircuitBreaker with required session.
controls,L6,circuit_breaker_driver,disable_v2,"async disable_v2(session: Session, reason: str, disabled_by: str, disabled_until: Optional[datetime]) -> Tuple[bool, Optional[Incident]]",,create_circuit_breaker | disable_v2,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,13,Disable CostSim V2.
controls,L6,circuit_breaker_driver,enable_v2,"async enable_v2(session: Session, enabled_by: str, reason: Optional[str]) -> bool",,create_circuit_breaker | enable_v2,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,11,Enable CostSim V2.
controls,L6,circuit_breaker_driver,is_v2_disabled,async is_v2_disabled(session: Session) -> bool,,create_circuit_breaker | is_disabled,__future__ | config | db | httpx | infra | sqlmodel,pure,yes,4,Check if CostSim V2 is disabled.
controls,L6,killswitch_read_driver,KillswitchReadDriver.__init__,__init__(session: Optional[Session]),L4:controls_bridge | L5:__init__ | L5:customer_killswitch_read_engine,,db | killswitch | pydantic | sqlalchemy | sqlmodel,pure,no,3,Initialize driver with optional session (lazy loaded).
controls,L6,killswitch_read_driver,KillswitchReadDriver._get_active_guardrails,_get_active_guardrails(session: Session) -> List[str],L4:controls_bridge | L5:__init__ | L5:customer_killswitch_read_engine,all | exec | hasattr | select | where,db | killswitch | pydantic | sqlalchemy | sqlmodel,pure,no,8,Get list of active guardrail names.
controls,L6,killswitch_read_driver,KillswitchReadDriver._get_incident_stats,"_get_incident_stats(session: Session, tenant_id: str) -> IncidentStatsDTO",L4:controls_bridge | L5:__init__ | L5:customer_killswitch_read_engine,IncidentStatsDTO | and_ | count | desc | exec | first | limit | now | order_by | select | timedelta | where,db | killswitch | pydantic | sqlalchemy | sqlmodel,pure,no,27,Get incident statistics for a tenant.
controls,L6,killswitch_read_driver,KillswitchReadDriver._get_killswitch_state,"_get_killswitch_state(session: Session, tenant_id: str) -> KillswitchStateDTO",L4:controls_bridge | L5:__init__ | L5:customer_killswitch_read_engine,KillswitchStateDTO | and_ | exec | first | select | where,db | killswitch | pydantic | sqlalchemy | sqlmodel,pure,no,20,Get killswitch state for a tenant.
controls,L6,killswitch_read_driver,KillswitchReadDriver._get_session,_get_session() -> Session,L4:controls_bridge | L5:__init__ | L5:customer_killswitch_read_engine,get_session | next,db | killswitch | pydantic | sqlalchemy | sqlmodel,pure,no,5,Get the database session (lazy loaded).
controls,L6,killswitch_read_driver,KillswitchReadDriver.get_killswitch_status,get_killswitch_status(tenant_id: str) -> KillswitchStatusDTO,L4:controls_bridge | L5:__init__ | L5:customer_killswitch_read_engine,KillswitchStatusDTO | ValueError | _get_active_guardrails | _get_incident_stats | _get_killswitch_state | _get_session,db | killswitch | pydantic | sqlalchemy | sqlmodel,pure,no,35,Get complete killswitch status for a tenant.
controls,L6,killswitch_read_driver,get_killswitch_read_driver,get_killswitch_read_driver(session: Optional[Session]) -> KillswitchReadDriver,L4:controls_bridge | L5:__init__ | L5:customer_killswitch_read_engine,KillswitchReadDriver,db | killswitch | pydantic | sqlalchemy | sqlmodel,pure,no,11,Get KillswitchReadDriver instance.
controls,L6,limits_read_driver,LimitsReadDriver.__init__,__init__(session: AsyncSession),L4:controls_bridge | L5:policies_limits_query_engine,,asyncio | policy_control_plane | sqlalchemy | time,pure,no,2,
controls,L6,limits_read_driver,LimitsReadDriver.fetch_budget_limits,"async fetch_budget_limits(tenant_id: str, scope: Optional[str], status: str, limit: int, offset: int) -> list[dict]",L4:controls_bridge | L5:policies_limits_query_engine,all | and_ | desc | execute | limit | offset | order_by | scalars | select | str | where,asyncio | policy_control_plane | sqlalchemy | time,db_write,yes,40,Fetch budget definitions for tenant.
controls,L6,limits_read_driver,LimitsReadDriver.fetch_limit_by_id,"async fetch_limit_by_id(tenant_id: str, limit_id: str) -> Optional[dict]",L4:controls_bridge | L5:policies_limits_query_engine,coalesce | count | execute | first | getattr | group_by | join | label | max | outerjoin | select | subquery | timedelta | utc_now | where,asyncio | policy_control_plane | sqlalchemy | time,db_write,yes,68,Fetch limit detail. Returns None if not found.
controls,L6,limits_read_driver,LimitsReadDriver.fetch_limits,"async fetch_limits(tenant_id: str) -> tuple[list[dict], int]",L4:controls_bridge | L5:policies_limits_query_engine,all | and_ | coalesce | count | desc | dict | endswith | execute | group_by | join | label | limit | max | nullslast | offset,asyncio | policy_control_plane | sqlalchemy | time,db_write,yes,121,Fetch limits with filters and pagination.
controls,L6,limits_read_driver,get_limits_read_driver,get_limits_read_driver(session: AsyncSession) -> LimitsReadDriver,L4:controls_bridge | L5:policies_limits_query_engine,LimitsReadDriver,asyncio | policy_control_plane | sqlalchemy | time,pure,no,3,Factory function for LimitsReadDriver.
controls,L6,override_driver,LimitOverrideService.__init__,__init__(session: AsyncSession),L4:controls_handler | L5s:override_types,,asyncio | cross_domain | override_types | overrides | policy_control_plane | sqlalchemy | time,pure,no,2,
controls,L6,override_driver,LimitOverrideService._get_limit,"async _get_limit(tenant_id: str, limit_id: str) -> Limit",L4:controls_handler | L5s:override_types,LimitNotFoundError | and_ | execute | scalar_one_or_none | select | where,asyncio | cross_domain | override_types | overrides | policy_control_plane | sqlalchemy | time,db_write,yes,15,Get limit by ID with tenant check.
controls,L6,override_driver,LimitOverrideService._to_response,_to_response(data: dict) -> LimitOverrideResponse,L4:controls_handler | L5s:override_types,Decimal | LimitOverrideResponse | OverrideStatus | fromisoformat | str,asyncio | cross_domain | override_types | overrides | policy_control_plane | sqlalchemy | time,pure,no,20,Convert stored data to response.
controls,L6,override_driver,LimitOverrideService.cancel_override,"async cancel_override(tenant_id: str, override_id: str, cancelled_by: str) -> LimitOverrideResponse",L4:controls_handler | L5s:override_types,OverrideNotFoundError | OverrideValidationError | _to_response | get,asyncio | cross_domain | override_types | overrides | policy_control_plane | sqlalchemy | time,pure,yes,16,Cancel a pending or active override.
controls,L6,override_driver,LimitOverrideService.get_override,"async get_override(tenant_id: str, override_id: str) -> LimitOverrideResponse",L4:controls_handler | L5s:override_types,OverrideNotFoundError | _to_response | get,asyncio | cross_domain | override_types | overrides | policy_control_plane | sqlalchemy | time,pure,yes,10,Get an override by ID.
controls,L6,override_driver,LimitOverrideService.list_overrides,"async list_overrides(tenant_id: str, status: Optional[str], limit: int, offset: int) -> tuple[list[LimitOverrideResponse], int]",L4:controls_handler | L5s:override_types,_to_response | len | values,asyncio | cross_domain | override_types | overrides | policy_control_plane | sqlalchemy | time,pure,yes,16,List overrides for a tenant.
controls,L6,override_driver,LimitOverrideService.request_override,"async request_override(tenant_id: str, request: LimitOverrideRequest, requested_by: str) -> LimitOverrideResponse",L4:controls_handler | L5s:override_types,OverrideValidationError | StackingAbuseError | _get_limit | _to_response | float | generate_uuid | isoformat | next | sum | timedelta | utc_now | values,asyncio | cross_domain | override_types | overrides | policy_control_plane | sqlalchemy | time,pure,yes,82,Request a temporary limit override.
controls,L6,policy_limits_driver,PolicyLimitsDriver.__init__,__init__(session: AsyncSession),L4:controls_bridge | L5:policy_limits_engine,,asyncio | policy_control_plane | sqlalchemy,pure,no,2,
controls,L6,policy_limits_driver,PolicyLimitsDriver.add_integrity,add_integrity(integrity: 'LimitIntegrity') -> None,L4:controls_bridge | L5:policy_limits_engine,add,asyncio | policy_control_plane | sqlalchemy,db_write,no,8,Add an integrity record to the session.
controls,L6,policy_limits_driver,PolicyLimitsDriver.add_limit,add_limit(limit: 'Limit') -> None,L4:controls_bridge | L5:policy_limits_engine,add,asyncio | policy_control_plane | sqlalchemy,db_write,no,8,Add a limit to the session.
controls,L6,policy_limits_driver,PolicyLimitsDriver.fetch_limit_by_id,"async fetch_limit_by_id(tenant_id: str, limit_id: str) -> Optional['Limit']",L4:controls_bridge | L5:policy_limits_engine,execute | scalar_one_or_none | select | where,asyncio | policy_control_plane | sqlalchemy,db_write,yes,23,Fetch a limit by ID with tenant scope.
controls,L6,policy_limits_driver,PolicyLimitsDriver.flush,async flush() -> None,L4:controls_bridge | L5:policy_limits_engine,flush,asyncio | policy_control_plane | sqlalchemy,db_write,yes,3,Flush pending changes without committing.
controls,L6,policy_limits_driver,get_policy_limits_driver,get_policy_limits_driver(session: AsyncSession) -> PolicyLimitsDriver,L4:controls_bridge | L5:policy_limits_engine,PolicyLimitsDriver,asyncio | policy_control_plane | sqlalchemy,pure,no,3,Factory function for PolicyLimitsDriver.
controls,L6,scoped_execution_driver,BoundExecutionScope.can_execute,"can_execute(action: str, incident_id: str) -> tuple[bool, str]",,is_valid,__future__,pure,no,16,Check if action can be executed within this scope.
controls,L6,scoped_execution_driver,BoundExecutionScope.consume,"consume(action: str, cost_usd: float) -> None",,append | isoformat | now,__future__,pure,no,20,Consume one execution attempt.
controls,L6,scoped_execution_driver,BoundExecutionScope.is_valid,is_valid() -> bool,,now,__future__,pure,no,11,Check if scope is still valid for execution.
controls,L6,scoped_execution_driver,BoundExecutionScope.to_dict,"to_dict() -> Dict[str, Any]",,isoformat | max,__future__,pure,no,18,Serialize scope for API response.
controls,L6,scoped_execution_driver,ScopeStore.__new__,__new__() -> 'ScopeStore',,__new__ | super,__future__,pure,no,8,
controls,L6,scoped_execution_driver,ScopeStore.cleanup_expired,cleanup_expired() -> int,,items | len | now,__future__,pure,no,7,Remove expired scopes from memory.
controls,L6,scoped_execution_driver,ScopeStore.create_scope,"create_scope(incident_id: str, allowed_actions: List[str], max_cost_usd: float, max_attempts: int, ttl_seconds: int, intent: str, target_agents: Optional[List[str]], created_by: str) -> BoundExecutionScope",,BoundExecutionScope | append | info | isoformat | now | timedelta | token_hex,__future__,pure,no,44,Create a new bound execution scope.
controls,L6,scoped_execution_driver,ScopeStore.get_scope,get_scope(scope_id: str) -> Optional[BoundExecutionScope],,get,__future__,pure,no,3,Get scope by ID.
controls,L6,scoped_execution_driver,ScopeStore.get_scopes_for_incident,get_scopes_for_incident(incident_id: str) -> List[BoundExecutionScope],,get,__future__,pure,no,4,Get all scopes for an incident.
controls,L6,scoped_execution_driver,ScopeStore.revoke_scope,revoke_scope(scope_id: str) -> bool,,get | info,__future__,pure,no,8,Revoke a scope (admin action).
controls,L6,scoped_execution_driver,ScopedExecutionContext.__init__,"__init__(action: RecoveryAction, scope: ExecutionScope, scope_fraction: float, timeout_ms: int)",,max | min,__future__,pure,no,13,
controls,L6,scoped_execution_driver,ScopedExecutionContext._compute_hash,"_compute_hash(data: Dict[str, Any]) -> str",,dumps | encode | hexdigest | sha256,__future__,pure,no,6,Compute deterministic hash of execution.
controls,L6,scoped_execution_driver,ScopedExecutionContext._dry_run_validate,async _dry_run_validate() -> ScopedExecutionResult,,ScopedExecutionResult | _compute_hash | _elapsed_ms | _estimate_cost | append | len,__future__,pure,yes,36,Validate action without actual execution.
controls,L6,scoped_execution_driver,ScopedExecutionContext._elapsed_ms,_elapsed_ms() -> int,,int | now | total_seconds,__future__,pure,no,7,Get elapsed time in milliseconds.
controls,L6,scoped_execution_driver,ScopedExecutionContext._estimate_cost,_estimate_cost() -> int,,get,__future__,pure,no,10,Estimate cost in cents for the action.
controls,L6,scoped_execution_driver,ScopedExecutionContext._execute_scoped,async _execute_scoped() -> ScopedExecutionResult,,ScopedExecutionResult | _compute_hash | _elapsed_ms | int,__future__,pure,yes,31,Execute action on scoped subset.
controls,L6,scoped_execution_driver,ScopedExecutionContext.execute,async execute() -> ScopedExecutionResult,,ScopedExecutionResult | _compute_hash | _dry_run_validate | _elapsed_ms | _execute_scoped | error | now | str,__future__,pure,yes,32,Execute action in scoped context.
controls,L6,scoped_execution_driver,create_recovery_scope,"async create_recovery_scope(incident_id: str, action: str, intent: str, max_cost_usd: float, max_attempts: int, ttl_seconds: int, target_agents: Optional[List[str]], created_by: str) -> Dict[str, Any]",,create_scope | get_scope_store | to_dict,__future__,pure,yes,33,Create a bound execution scope for recovery action.
controls,L6,scoped_execution_driver,execute_with_scope,"async execute_with_scope(scope_id: str, action: str, incident_id: str, parameters: Optional[Dict[str, Any]]) -> Dict[str, Any]",,ScopeActionMismatch | ScopeExhausted | ScopeExpired | ScopeIncidentMismatch | ScopeNotFound | ScopedExecutionRequired | can_execute | consume | get_scope | get_scope_store | info | isoformat | lower | max | now,__future__,pure,yes,55,Execute a recovery action within a valid scope.
controls,L6,scoped_execution_driver,get_scope_store,get_scope_store() -> ScopeStore,,,__future__,pure,no,3,Get the global scope store.
controls,L6,scoped_execution_driver,requires_scoped_execution,requires_scoped_execution(risk_threshold: RiskClass),,ScopedExecutionRequired | func | get | index | isinstance | pop | wraps,__future__,pure,no,37,Decorator to enforce scoped pre-execution for risky recovery actions.
controls,L6,scoped_execution_driver,test_recovery_scope,"async test_recovery_scope(action_id: str, action_name: str, action_type: str, risk_class: str, parameters: Dict[str, Any], scope_type: str, scope_fraction: float) -> Dict[str, Any]",,ExecutionScope | RecoveryAction | RiskClass | ScopedExecutionContext | execute,__future__,pure,yes,39,Test a recovery action in scoped execution.
controls,L6,scoped_execution_driver,validate_scope_required,"async validate_scope_required(incident_id: str, action: str) -> None",,ScopedExecutionRequired,__future__,pure,yes,15,Validate that execution without scope should fail.
controls,L6,threshold_driver,ThresholdDriver.__init__,__init__(session: AsyncSession),?:runner | L4:threshold_types | L4:signal_coordinator | L5:threshold_engine,,asyncio | event_emitter | policy_control_plane | sqlalchemy | sqlmodel | threshold_signals | threshold_types,pure,no,2,
controls,L6,threshold_driver,ThresholdDriver.get_active_threshold_limits,async get_active_threshold_limits(tenant_id: str) -> list[LimitSnapshot],?:runner | L4:threshold_types | L4:signal_coordinator | L5:threshold_engine,LimitSnapshot | all | execute | order_by | scalars | select | str | where,asyncio | event_emitter | policy_control_plane | sqlalchemy | sqlmodel | threshold_signals | threshold_types,db_write,yes,36,Query active threshold limits for a tenant.
controls,L6,threshold_driver,ThresholdDriver.get_threshold_limit_by_scope,"async get_threshold_limit_by_scope(tenant_id: str, scope: str, scope_id: Optional[str]) -> Optional[LimitSnapshot]",?:runner | L4:threshold_types | L4:signal_coordinator | L5:threshold_engine,LimitSnapshot | execute | scalar_one_or_none | select | str | where,asyncio | event_emitter | policy_control_plane | sqlalchemy | sqlmodel | threshold_signals | threshold_types,db_write,yes,43,Query a single threshold limit by scope.
controls,L6,threshold_driver,ThresholdDriverSync.__init__,__init__(session: Any),?:runner | L4:threshold_types | L4:signal_coordinator | L5:threshold_engine,,asyncio | event_emitter | policy_control_plane | sqlalchemy | sqlmodel | threshold_signals | threshold_types,pure,no,8,Initialize with a sync SQLAlchemy Session.
controls,L6,threshold_driver,ThresholdDriverSync.get_active_threshold_limits,get_active_threshold_limits(tenant_id: str) -> list[LimitSnapshot],?:runner | L4:threshold_types | L4:signal_coordinator | L5:threshold_engine,LimitSnapshot | execute | fetchall | str | text,asyncio | event_emitter | policy_control_plane | sqlalchemy | sqlmodel | threshold_signals | threshold_types,db_write,no,37,Query active threshold limits for a tenant (sync version).
controls,L6,threshold_driver,emit_threshold_signal_sync,"emit_threshold_signal_sync(session: Any, tenant_id: str, run_id: str, state: str, signal: Any, params_used: dict) -> None",?:runner | L4:threshold_types | L4:signal_coordinator | L5:threshold_engine,EventEmitter | OpsEvent | UUID | emit | get | info | isinstance,asyncio | event_emitter | policy_control_plane | sqlalchemy | sqlmodel | threshold_signals | threshold_types,pure,no,69,Emit a threshold signal to ops_events table (sync).
incidents,L5,anomaly_bridge,AnomalyIncidentBridge.__init__,__init__(driver: IncidentWriteDriver),L4:anomaly_types | L4:anomaly_incident_coordinator | L5:cost_anomaly_detector_engine,,anomaly_types | governance | incident_write_driver | metrics,pure,no,11,Initialize bridge with incident write driver.
incidents,L5,anomaly_bridge,AnomalyIncidentBridge._check_existing_incident,_check_existing_incident(fact: CostAnomalyFact) -> Optional[str],L4:anomaly_types | L4:anomaly_incident_coordinator | L5:cost_anomaly_detector_engine,,anomaly_types | governance | incident_write_driver | metrics,pure,no,10,Check for existing unresolved incident for this anomaly.
incidents,L5,anomaly_bridge,AnomalyIncidentBridge._create_incident,_create_incident(fact: CostAnomalyFact) -> str,L4:anomaly_types | L4:anomaly_incident_coordinator | L5:cost_anomaly_detector_engine,Decimal | GovernanceError | get | inc | info | insert_incident_from_anomaly | labels | now | str | upper | uuid4,anomaly_types | governance | incident_write_driver | metrics,db_write,no,73,Create an incident from the cost anomaly fact.
incidents,L5,anomaly_bridge,AnomalyIncidentBridge._is_suppressed,_is_suppressed(fact: CostAnomalyFact) -> bool,L4:anomaly_types | L4:anomaly_incident_coordinator | L5:cost_anomaly_detector_engine,fetch_suppressing_policy,anomaly_types | governance | incident_write_driver | metrics,pure,no,16,Check if an active policy suppresses this anomaly type.
incidents,L5,anomaly_bridge,AnomalyIncidentBridge._meets_severity_threshold,_meets_severity_threshold(severity: str) -> bool,L4:anomaly_types | L4:anomaly_incident_coordinator | L5:cost_anomaly_detector_engine,upper,anomaly_types | governance | incident_write_driver | metrics,pure,no,4,Check if severity meets threshold for incident creation.
incidents,L5,anomaly_bridge,AnomalyIncidentBridge.ingest,ingest(fact: CostAnomalyFact) -> Optional[str],L4:anomaly_types | L4:anomaly_incident_coordinator | L5:cost_anomaly_detector_engine,_check_existing_incident | _create_incident | _is_suppressed | _meets_severity_threshold | debug | info,anomaly_types | governance | incident_write_driver | metrics,pure,no,43,Process a cost anomaly fact and create an incident if warranted.
incidents,L5,anomaly_bridge,get_anomaly_incident_bridge,get_anomaly_incident_bridge(session) -> AnomalyIncidentBridge,L4:anomaly_types | L4:anomaly_incident_coordinator | L5:cost_anomaly_detector_engine,AnomalyIncidentBridge | get_incident_write_driver,anomaly_types | governance | incident_write_driver | metrics,pure,no,8,Factory function to get AnomalyIncidentBridge instance.
incidents,L5,export_engine,ExportEngine.__init__,__init__(driver),L4:incidents_bridge,,,pure,no,7,Initialize with L6 driver (satisfies ExportBundleProtocol).
incidents,L5,export_engine,ExportEngine.export_evidence,"async export_evidence(incident_id: str, exported_by: str, export_reason: Optional[str], include_raw_steps: bool) -> Dict[str, Any]",L4:incidents_bridge,ValueError | create_evidence_bundle | info,,pure,yes,40,Generate evidence export bundle.
incidents,L5,export_engine,ExportEngine.export_executive_debrief,"async export_executive_debrief(incident_id: str, exported_by: str) -> Dict[str, Any]",L4:incidents_bridge,ValueError | create_executive_debrief | info,,pure,yes,34,Generate executive debrief export bundle.
incidents,L5,export_engine,ExportEngine.export_soc2,"async export_soc2(incident_id: str, exported_by: str, custom_controls: Optional[dict]) -> Dict[str, Any]",L4:incidents_bridge,ValueError | create_soc2_bundle | info,,pure,yes,37,Generate SOC2 compliance export bundle.
incidents,L5,export_schemas,ExportBundleProtocol.create_evidence_bundle,"async create_evidence_bundle(incident_id: str, exported_by: str, export_reason: Optional[str], include_raw_steps: bool) -> Any",,,__future__,pure,yes,9,Create evidence bundle from incident.
incidents,L5,export_schemas,ExportBundleProtocol.create_executive_debrief,"async create_executive_debrief(incident_id: str, exported_by: str) -> Any",,,__future__,pure,yes,7,Create executive debrief bundle from incident.
incidents,L5,export_schemas,ExportBundleProtocol.create_soc2_bundle,"async create_soc2_bundle(incident_id: str, exported_by: str, custom_controls: Optional[dict]) -> Any",,,__future__,pure,yes,8,Create SOC2 compliance bundle from incident.
incidents,L5,hallucination_detector,HallucinationDetector.__init__,__init__(config: Optional[HallucinationConfig]),?:hallucination_hook | ?:__init__,HallucinationConfig | compile,,pure,no,26,Initialize detector with config.
incidents,L5,hallucination_detector,HallucinationDetector._detect_contradictions,_detect_contradictions(content: str) -> list[HallucinationIndicator],?:hallucination_hook | ?:__init__,HallucinationIndicator | append | group | lower | search,,pure,no,25,Detect self-contradictions in content.
incidents,L5,hallucination_detector,HallucinationDetector._detect_suspicious_citations,_detect_suspicious_citations(content: str) -> list[HallucinationIndicator],?:hallucination_hook | ?:__init__,HallucinationIndicator | append | findall | int | list | now,,pure,no,39,Detect potentially fabricated academic citations.
incidents,L5,hallucination_detector,HallucinationDetector._detect_suspicious_urls,_detect_suspicious_urls(content: str) -> list[HallucinationIndicator],?:hallucination_hook | ?:__init__,HallucinationIndicator | append | findall | search,,pure,no,27,Detect potentially fabricated URLs.
incidents,L5,hallucination_detector,HallucinationDetector._detect_temporal_issues,"_detect_temporal_issues(content: str, context: Optional[dict[str, Any]]) -> list[HallucinationIndicator]",?:hallucination_hook | ?:__init__,HallucinationIndicator | append | compile | findall | lower | now,,pure,no,27,Detect temporal impossibilities.
incidents,L5,hallucination_detector,HallucinationDetector._hash_content,_hash_content(content: str) -> str,?:hallucination_hook | ?:__init__,encode | hexdigest | sha256,,pure,no,3,Generate hash of content for evidence tracking.
incidents,L5,hallucination_detector,HallucinationDetector.detect,"detect(content: str, context: Optional[dict[str, Any]], customer_blocking_opted_in: bool) -> HallucinationResult",?:hallucination_hook | ?:__init__,HallucinationResult | _detect_contradictions | _detect_suspicious_citations | _detect_suspicious_urls | _detect_temporal_issues | _hash_content | extend | get | len | min | sum,,pure,no,81,Detect potential hallucinations in content.
incidents,L5,hallucination_detector,HallucinationIndicator.to_dict,"to_dict() -> dict[str, Any]",?:hallucination_hook | ?:__init__,,,pure,no,10,Convert to dictionary for serialization.
incidents,L5,hallucination_detector,HallucinationResult._derive_severity,_derive_severity() -> HallucinationSeverity,?:hallucination_hook | ?:__init__,any,,pure,no,17,Derive overall severity from indicators.
incidents,L5,hallucination_detector,HallucinationResult.to_incident_data,"to_incident_data() -> dict[str, Any]",?:hallucination_hook | ?:__init__,_derive_severity | isoformat | to_dict,,pure,no,15,Convert to incident creation data.
incidents,L5,hallucination_detector,create_detector_for_tenant,"create_detector_for_tenant(tenant_config: Optional[dict[str, Any]]) -> HallucinationDetector",?:hallucination_hook | ?:__init__,HallucinationConfig | HallucinationDetector | get,,pure,no,23,Create a detector configured for a specific tenant.
incidents,L5,incident_decision_port,IncidentDecisionPort.check_and_create_incident,"check_and_create_incident(run_id: str, status: str, error_message: Optional[str], tenant_id: Optional[str], agent_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",L6:incident_driver,,__future__,pure,no,16,Check if a run warrants an incident and create one if so.
incidents,L5,incident_decision_port,IncidentDecisionPort.create_incident_for_run,"create_incident_for_run(run_id: str, tenant_id: str, run_status: str, error_code: Optional[str], error_message: Optional[str], agent_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",L6:incident_driver,,__future__,pure,no,17,Create an incident record for any run (success or failure).
incidents,L5,incident_decision_port,IncidentDecisionPort.get_incidents_for_run,"get_incidents_for_run(run_id: str) -> List[Dict[str, Any]]",L6:incident_driver,,__future__,pure,no,7,Get all incidents associated with a run.
incidents,L5,incident_engine,IncidentEngine.__init__,"__init__(db_url: Optional[str], driver: Optional[IncidentWriteDriver], evidence_recorder: Any)",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,get,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,19,Initialize the incident engine.
incidents,L5,incident_engine,IncidentEngine._check_policy_suppression,"_check_policy_suppression(tenant_id: str, error_code: Optional[str], category: str) -> Optional[Dict[str, Any]]",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,_get_driver | fetch_suppressing_policy | warning,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,35,Check if an active policy_rule suppresses this incident pattern.
incidents,L5,incident_engine,IncidentEngine._extract_error_code,_extract_error_code(error_message: Optional[str]) -> str,?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,keys | upper,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,11,Extract error code from error message.
incidents,L5,incident_engine,IncidentEngine._generate_title,"_generate_title(error_code: Optional[str], run_id: str) -> str",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,16,Generate human-readable incident title.
incidents,L5,incident_engine,IncidentEngine._get_driver,_get_driver() -> IncidentWriteDriver,?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,RuntimeError | SessionLocal | create_engine | get_incident_write_driver | sessionmaker,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,24,Get or create the write driver.
incidents,L5,incident_engine,IncidentEngine._maybe_create_policy_proposal,"_maybe_create_policy_proposal(incident_id: str, tenant_id: str, severity: str, category: str, error_code: Optional[str], run_id: str, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,_get_driver | get | info | insert_policy_proposal | replace | str | title | utc_now | uuid4 | warning,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,93,Create a policy proposal for high-severity incidents.
incidents,L5,incident_engine,IncidentEngine._write_prevention_record,"_write_prevention_record(policy_id: str, run_id: str, tenant_id: str, error_code: Optional[str], source_incident_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> str",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,_get_driver | info | insert_prevention_record | utc_now | uuid4,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,51,Write a prevention_record when a run is suppressed by an active policy.
incidents,L5,incident_engine,IncidentEngine.check_and_create_incident,"check_and_create_incident(run_id: str, status: str, error_message: Optional[str], tenant_id: Optional[str], agent_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,_extract_error_code | create_incident_for_failed_run | warning,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,51,Check if a run status warrants an incident and create one if so.
incidents,L5,incident_engine,IncidentEngine.create_incident_for_all_runs,"create_incident_for_all_runs(run_id: str, status: str, error_message: Optional[str], tenant_id: Optional[str], agent_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,_extract_error_code | create_incident_for_run | warning,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,46,Create an incident for ANY run (PIN-407: Success as First-Class Data).
incidents,L5,incident_engine,IncidentEngine.create_incident_for_failed_run,"create_incident_for_failed_run(run_id: str, tenant_id: str, error_code: Optional[str], error_message: Optional[str], agent_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,PyUUID | _check_policy_suppression | _generate_title | _get_driver | _maybe_create_policy_proposal | _write_prevention_record | debug | error | get | info | insert_incident | isinstance | record_evidence | update_run_incident_count | update_trace_incident_id,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,158,Create an incident for a failed run.
incidents,L5,incident_engine,IncidentEngine.create_incident_for_run,"create_incident_for_run(run_id: str, tenant_id: str, run_status: str, error_code: Optional[str], error_message: Optional[str], agent_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,PyUUID | _check_policy_suppression | _generate_title | _get_driver | _maybe_create_policy_proposal | _write_prevention_record | error | get | info | insert_incident | isinstance | lower | record_evidence | update_run_incident_count | update_trace_incident_id,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,175,Create an incident for ANY run (PIN-407: Success as First-Class Data).
incidents,L5,incident_engine,IncidentEngine.get_incidents_for_run,"get_incidents_for_run(run_id: str) -> List[Dict[str, Any]]",?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,_get_driver | error | fetch_incidents_by_run_id,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,21,Get all incidents linked to a run.
incidents,L5,incident_engine,get_incident_engine,get_incident_engine() -> IncidentEngine,?:hallucination_detector | ?:incident_driver | L4:incidents_bridge | L4:lessons_coordinator | L5:hallucination_detector | ?:inject_synthetic,IncidentEngine | get_lessons_coordinator,incident_write_driver | lessons_coordinator | orm | sqlalchemy | sqlmodel | time,pure,no,10,Get or create singleton incident engine instance.
incidents,L5,incident_pattern_engine,IncidentPatternService.__init__,"__init__(session: 'AsyncSession', driver: Optional[IncidentPatternDriver])",L5:incidents_facade,get_incident_pattern_driver,asyncio | incident_pattern_driver | time,pure,no,14,Initialize with session and optional driver.
incidents,L5,incident_pattern_engine,IncidentPatternService._detect_cascade_failures,"async _detect_cascade_failures(tenant_id: str, window_start: datetime, limit: int) -> list[PatternMatch]",L5:incidents_facade,PatternMatch | append | fetch_cascade_failures | min | round,asyncio | incident_pattern_driver | time,pure,yes,31,Detect multiple incidents from same source run.
incidents,L5,incident_pattern_engine,IncidentPatternService._detect_category_clusters,"async _detect_category_clusters(tenant_id: str, window_start: datetime, limit: int) -> list[PatternMatch]",L5:incidents_facade,PatternMatch | append | fetch_category_clusters | min | round,asyncio | incident_pattern_driver | time,pure,yes,31,Detect categories with multiple incidents.
incidents,L5,incident_pattern_engine,IncidentPatternService._detect_severity_spikes,"async _detect_severity_spikes(tenant_id: str, limit: int) -> list[PatternMatch]",L5:incidents_facade,PatternMatch | append | fetch_severity_spikes | min | round,asyncio | incident_pattern_driver | time,pure,yes,33,Detect multiple high/critical incidents in short window.
incidents,L5,incident_pattern_engine,IncidentPatternService.detect_patterns,"async detect_patterns(tenant_id: str, window_hours: int, limit: int) -> PatternResult",L5:incidents_facade,PatternResult | _detect_cascade_failures | _detect_category_clusters | _detect_severity_spikes | extend | fetch_incidents_count | min | timedelta | utc_now,asyncio | incident_pattern_driver | time,pure,yes,53,Detect all patterns within the time window.
incidents,L5,incident_read_engine,IncidentReadService.__init__,__init__(session: 'Session'),L4:incidents_bridge | L3:customer_incidents_adapter,get_incident_read_driver,incident_read_driver | killswitch | sqlmodel,pure,no,3,Initialize with database session (passed to driver).
incidents,L5,incident_read_engine,IncidentReadService.count_incidents_since,"count_incidents_since(tenant_id: str, since: datetime) -> int",L4:incidents_bridge | L3:customer_incidents_adapter,count_incidents_since,incident_read_driver | killswitch | sqlmodel,pure,no,11,Count incidents since a given time.
incidents,L5,incident_read_engine,IncidentReadService.get_incident,"get_incident(incident_id: str, tenant_id: str) -> Optional['Incident']",L4:incidents_bridge | L3:customer_incidents_adapter,get_incident,incident_read_driver | killswitch | sqlmodel,pure,no,11,Get a single incident by ID with tenant isolation.
incidents,L5,incident_read_engine,IncidentReadService.get_incident_events,get_incident_events(incident_id: str) -> List['IncidentEvent'],L4:incidents_bridge | L3:customer_incidents_adapter,get_incident_events,incident_read_driver | killswitch | sqlmodel,pure,no,10,Get timeline events for an incident.
incidents,L5,incident_read_engine,IncidentReadService.get_last_incident,get_last_incident(tenant_id: str) -> Optional['Incident'],L4:incidents_bridge | L3:customer_incidents_adapter,get_last_incident,incident_read_driver | killswitch | sqlmodel,pure,no,10,Get the most recent incident for a tenant.
incidents,L5,incident_read_engine,IncidentReadService.list_incidents,"list_incidents(tenant_id: str, status: Optional[str], severity: Optional[str], from_date: Optional[datetime], to_date: Optional[datetime], limit: int, offset: int) -> Tuple[List['Incident'], int]",L4:incidents_bridge | L3:customer_incidents_adapter,list_incidents,incident_read_driver | killswitch | sqlmodel,pure,no,24,List incidents for a tenant with optional filters.
incidents,L5,incident_read_engine,get_incident_read_service,get_incident_read_service(session: 'Session') -> IncidentReadService,L4:incidents_bridge | L3:customer_incidents_adapter,IncidentReadService,incident_read_driver | killswitch | sqlmodel,pure,no,3,Factory function to get IncidentReadService instance.
incidents,L5,incident_write_engine,IncidentWriteService.__init__,"__init__(session: 'Session', audit: Any)",?:incident_write_service | L4:incidents_handler | L4:incidents_bridge | L3:customer_incidents_adapter,get_incident_write_driver,audit_ledger | incident_write_driver | killswitch | sqlmodel,pure,no,13,Initialize with database session and optional audit service.
incidents,L5,incident_write_engine,IncidentWriteService.acknowledge_incident,"acknowledge_incident(incident: 'Incident', acknowledged_by: str, actor_type: ActorType, reason: Optional[str]) -> 'Incident'",?:incident_write_service | L4:incidents_handler | L4:incidents_bridge | L3:customer_incidents_adapter,begin | create_incident_event | incident_acknowledged | now | refresh_incident | str | update_incident_acknowledged,audit_ledger | incident_write_driver | killswitch | sqlmodel,pure,no,58,Acknowledge an incident and create a timeline event.
incidents,L5,incident_write_engine,IncidentWriteService.manual_close_incident,"manual_close_incident(incident: 'Incident', closed_by: str, reason: Optional[str], actor_type: ActorType) -> 'Incident'",?:incident_write_service | L4:incidents_handler | L4:incidents_bridge | L3:customer_incidents_adapter,begin | create_incident_event | hasattr | incident_manually_closed | isoformat | now | refresh_incident | str | update_incident_resolved,audit_ledger | incident_write_driver | killswitch | sqlmodel,pure,no,84,Manually close an incident without resolution workflow.
incidents,L5,incident_write_engine,IncidentWriteService.resolve_incident,"resolve_incident(incident: 'Incident', resolved_by: str, resolution_notes: Optional[str], resolution_method: Optional[str], actor_type: ActorType, reason: Optional[str]) -> 'Incident'",?:incident_write_service | L4:incidents_handler | L4:incidents_bridge | L3:customer_incidents_adapter,begin | create_incident_event | incident_resolved | now | refresh_incident | str | update_incident_resolved,audit_ledger | incident_write_driver | killswitch | sqlmodel,pure,no,71,Resolve an incident and create a timeline event.
incidents,L5,incident_write_engine,get_incident_write_service,"get_incident_write_service(session: 'Session', audit: Any) -> IncidentWriteService",?:incident_write_service | L4:incidents_handler | L4:incidents_bridge | L3:customer_incidents_adapter,IncidentWriteService,audit_ledger | incident_write_driver | killswitch | sqlmodel,pure,no,9,Factory function to get IncidentWriteService instance.
incidents,L5,incidents_facade,IncidentsFacade._snapshot_to_summary,_snapshot_to_summary(snapshot: IncidentSnapshot) -> IncidentSummaryResult,?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,IncidentSummaryResult,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,no,18,Convert snapshot to summary result. Applies business defaults.
incidents,L5,incidents_facade,IncidentsFacade.analyze_cost_impact,"async analyze_cost_impact(session: 'AsyncSession', tenant_id: str) -> CostImpactResult",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,CostImpactResult | CostImpactSummaryResult | IncidentsFacadeDriver | append | fetch_cost_impact_data | min | now,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,37,Analyze cost impact across incidents.
incidents,L5,incidents_facade,IncidentsFacade.analyze_recurrence,"async analyze_recurrence(session: 'AsyncSession', tenant_id: str) -> RecurrenceAnalysisResult",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,RecurrenceAnalysisResult | RecurrenceAnalysisService | RecurrenceGroupResult | analyze_recurrence,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,44,Analyze recurring incident types.
incidents,L5,incidents_facade,IncidentsFacade.detect_patterns,"async detect_patterns(session: 'AsyncSession', tenant_id: str) -> PatternDetectionResult",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,IncidentPatternService | PatternDetectionResult | PatternMatchResult | detect_patterns,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,43,Detect incident patterns.
incidents,L5,incidents_facade,IncidentsFacade.get_incident_detail,"async get_incident_detail(session: 'AsyncSession', tenant_id: str, incident_id: str) -> Optional[IncidentDetailResult]",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,IncidentDetailResult | IncidentsFacadeDriver | fetch_incident_by_id,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,37,Get incident detail. Tenant isolation enforced.
incidents,L5,incidents_facade,IncidentsFacade.get_incident_learnings,"async get_incident_learnings(session: 'AsyncSession', tenant_id: str, incident_id: str) -> Optional[LearningsResult]",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,LearningInsightResult | LearningsResult | PostMortemService | ResolutionSummaryResult | get_incident_learnings,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,60,Extract post-mortem learnings from an incident.
incidents,L5,incidents_facade,IncidentsFacade.get_incidents_for_run,"async get_incidents_for_run(session: 'AsyncSession', tenant_id: str, run_id: str) -> IncidentsByRunResult",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,IncidentsByRunResult | IncidentsFacadeDriver | _snapshot_to_summary | fetch_incidents_by_run | len,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,21,Get all incidents linked to a specific run.
incidents,L5,incidents_facade,IncidentsFacade.get_metrics,"async get_metrics(session: 'AsyncSession', tenant_id: str) -> IncidentMetricsResult",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,IncidentMetricsResult | IncidentsFacadeDriver | fetch_metrics_aggregates | now | round,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,60,"Get incident metrics. Backend-computed, deterministic."
incidents,L5,incidents_facade,IncidentsFacade.list_active_incidents,"async list_active_incidents(session: 'AsyncSession', tenant_id: str) -> IncidentListResult",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,IncidentListResult | IncidentsFacadeDriver | PaginationResult | _snapshot_to_summary | fetch_active_incidents | isoformat | len,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,61,List active incidents (ACTIVE + ACKED states).
incidents,L5,incidents_facade,IncidentsFacade.list_historical_incidents,"async list_historical_incidents(session: 'AsyncSession', tenant_id: str) -> IncidentListResult",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,IncidentListResult | IncidentsFacadeDriver | PaginationResult | _snapshot_to_summary | fetch_historical_incidents | isoformat | len | now | timedelta,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,54,List historical incidents (resolved beyond retention window).
incidents,L5,incidents_facade,IncidentsFacade.list_resolved_incidents,"async list_resolved_incidents(session: 'AsyncSession', tenant_id: str) -> IncidentListResult",?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,IncidentListResult | IncidentsFacadeDriver | PaginationResult | _snapshot_to_summary | fetch_resolved_incidents | isoformat | len,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,yes,61,List resolved incidents.
incidents,L5,incidents_facade,get_incidents_facade,get_incidents_facade() -> IncidentsFacade,?:incidents | L4:incidents_handler | ?:learning_insight_result | ?:recurrence_group_result | ?:recurrence_analysis_result | ?:resolution_summary_result | ?:pattern_match_result | ?:learnings_result | ?:pattern_detection_result,IncidentsFacade,asyncio | incident_pattern_engine | incidents_facade_driver | postmortem_engine | recurrence_analysis_engine,pure,no,6,Get the singleton IncidentsFacade instance.
incidents,L5,llm_failure_engine,LLMFailureFact.__post_init__,__post_init__(),?:llm_failure_service,ValueError | generate_uuid | utc_now,asyncio | llm_failure_driver | runtime,pure,no,10,
incidents,L5,llm_failure_engine,LLMFailureService.__init__,"__init__(session: 'AsyncSession', uuid_fn: UuidFn, clock_fn: ClockFn, driver: Optional[LLMFailureDriver])",?:llm_failure_service,get_llm_failure_driver,asyncio | llm_failure_driver | runtime,pure,no,20,Constructor requires explicit DI - no get_llm_failure_service() factory.
incidents,L5,llm_failure_engine,LLMFailureService._capture_evidence,"async _capture_evidence(failure: LLMFailureFact, timestamp: datetime) -> str",?:llm_failure_service,_uuid_fn | insert_evidence | isoformat,asyncio | llm_failure_driver | runtime,pure,yes,31,Capture evidence for the failure (mandatory per Invariant 4).
incidents,L5,llm_failure_engine,LLMFailureService._mark_run_failed,"async _mark_run_failed(failure: LLMFailureFact, timestamp: datetime) -> bool",?:llm_failure_service,update_run_failed,asyncio | llm_failure_driver | runtime,pure,yes,17,Mark the run as FAILED.
incidents,L5,llm_failure_engine,LLMFailureService._persist_failure,"async _persist_failure(failure: LLMFailureFact, timestamp: datetime) -> None",?:llm_failure_service,ValueError | insert_failure,asyncio | llm_failure_driver | runtime,pure,yes,22,Persist failure fact to run_failures table.
incidents,L5,llm_failure_engine,LLMFailureService._verify_no_contamination,async _verify_no_contamination(failure: LLMFailureFact) -> None,?:llm_failure_service,RuntimeError | fetch_contamination_check,asyncio | llm_failure_driver | runtime,pure,yes,26,Verify no downstream artifacts were created (AC-4).
incidents,L5,llm_failure_engine,LLMFailureService.get_failure_by_run_id,"async get_failure_by_run_id(run_id: str, tenant_id: str) -> Optional[LLMFailureFact]",?:llm_failure_service,LLMFailureFact | fetch_failure_by_run_id | loads,asyncio | llm_failure_driver | runtime,pure,yes,29,Retrieve failure fact by run ID (with tenant isolation).
incidents,L5,llm_failure_engine,LLMFailureService.persist_failure_and_mark_run,"async persist_failure_and_mark_run(failure: LLMFailureFact, auto_action: str) -> LLMFailureResult",?:llm_failure_service,LLMFailureResult | _capture_evidence | _clock_fn | _mark_run_failed | _persist_failure | _verify_no_contamination,asyncio | llm_failure_driver | runtime,pure,yes,48,"Persist failure fact to DB, then mark run as FAILED."
incidents,L5,policy_violation_engine,PolicyViolationService.__init__,"__init__(session: 'AsyncSession', driver: Optional[PolicyViolationDriver])",,get_policy_violation_driver,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,no,14,Initialize with async session and optional driver.
incidents,L5,policy_violation_engine,PolicyViolationService.check_incident_exists,"async check_incident_exists(run_id: str, policy_id: str, tenant_id: str) -> Optional[str]",,fetch_incident_by_violation,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,7,"Idempotency check: Only one incident per (run_id, policy_id)."
incidents,L5,policy_violation_engine,PolicyViolationService.check_policy_enabled,"async check_policy_enabled(tenant_id: str, policy_id: str) -> bool",,fetch_policy_enabled,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,7,AC-2 prerequisite: Policy must be explicitly enabled for tenant.
incidents,L5,policy_violation_engine,PolicyViolationService.check_violation_persisted,async check_violation_persisted(violation_id: str) -> bool,,fetch_violation_exists,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,3,Check if a violation fact has been persisted.
incidents,L5,policy_violation_engine,PolicyViolationService.create_incident_from_violation,"async create_incident_from_violation(violation: ViolationFact, auto_action: Optional[str]) -> str",,Decimal | RuntimeError | Session | check_violation_persisted | create_incident_aggregator | get_or_create_incident | info | items | list | str,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,68,Create an incident from a persisted violation.
incidents,L5,policy_violation_engine,PolicyViolationService.persist_evidence,"async persist_evidence(violation_id: str, incident_id: str, evidence: Dict[str, Any]) -> str",,generate_uuid | info | insert_evidence_event,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,27,AC-3: Persist evidence linked to violation.
incidents,L5,policy_violation_engine,PolicyViolationService.persist_violation_and_create_incident,"async persist_violation_and_create_incident(violation: ViolationFact, auto_action: Optional[str]) -> ViolationIncident",,RuntimeError | ViolationIncident | check_incident_exists | create_incident_from_violation | info | persist_evidence | persist_violation_fact,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,56,Full S3 flow: Violation  Persistence  Incident  Evidence.
incidents,L5,policy_violation_engine,PolicyViolationService.persist_violation_fact,async persist_violation_fact(violation: ViolationFact) -> str,,ValueError | info | insert_violation_record,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,37,Persist a violation fact to the database.
incidents,L5,policy_violation_engine,PolicyViolationService.verify_violation_truth,"async verify_violation_truth(run_id: str, tenant_id: str, policy_id: str) -> Dict[str, Any]",,all | fetch_violation_truth_check | values,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,60,Verify that a violation satisfies S3 truth requirements.
incidents,L5,policy_violation_engine,create_policy_evaluation_record,"async create_policy_evaluation_record(session: 'AsyncSession', run_id: str, tenant_id: str, outcome: str, policies_checked: int, reason: str, draft_candidate: bool, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> str",,generate_uuid | get_policy_violation_driver | info | insert_policy_evaluation | now,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,65,Create a policy evaluation record for ANY run (PIN-407).
incidents,L5,policy_violation_engine,create_policy_evaluation_sync,"create_policy_evaluation_sync(run_id: str, tenant_id: str, run_status: str, policies_checked: int, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",,debug | error | generate_uuid | getenv | info | insert_policy_evaluation_sync | lower | now,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,no,80,Create a policy evaluation record for ANY run (PIN-407) - SYNC VERSION.
incidents,L5,policy_violation_engine,handle_policy_evaluation_for_run,"async handle_policy_evaluation_for_run(session: 'AsyncSession', run_id: str, tenant_id: str, run_status: str, policies_checked: int, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> str",,create_policy_evaluation_record | lower,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,54,Create a policy evaluation record for ANY run (PIN-407).
incidents,L5,policy_violation_engine,handle_policy_violation,"async handle_policy_violation(session: 'AsyncSession', run_id: str, tenant_id: str, policy_type: str, policy_id: str, violated_rule: str, reason: str, severity: str, evidence: Optional[Dict[str, Any]]) -> Optional[ViolationIncident]",,PolicyViolationService | ViolationFact | persist_violation_and_create_incident,asyncio | db | incident_aggregator | policy_violation_driver | runtime | sqlmodel,pure,yes,35,Handle a policy violation with S3 truth guarantees.
incidents,L5,postmortem_engine,PostMortemService.__init__,"__init__(session: 'AsyncSession', driver: Optional[PostMortemDriver])",L5:incidents_facade,get_postmortem_driver,asyncio | postmortem_driver,pure,no,14,Initialize with session and optional driver.
incidents,L5,postmortem_engine,PostMortemService._extract_insights,"async _extract_insights(tenant_id: str, resolution: ResolutionSummary, similar: list[ResolutionSummary]) -> list[LearningInsight]",L5:incidents_facade,LearningInsight | append | int | len | min | round | sum,asyncio | postmortem_driver,pure,yes,72,Extract insights from incident and similar incidents.
incidents,L5,postmortem_engine,PostMortemService._find_similar_incidents,"async _find_similar_incidents(tenant_id: str, exclude_incident_id: str, category: Optional[str], limit: int) -> list[ResolutionSummary]",L5:incidents_facade,ResolutionSummary | fetch_similar_incidents | int,asyncio | postmortem_driver,pure,yes,32,Find similar resolved incidents.
incidents,L5,postmortem_engine,PostMortemService._generate_category_insights,"_generate_category_insights(category: str, total_incidents: int, resolved_count: int, avg_resolution_time_ms: Optional[float], common_methods: list[tuple[str, int]], recurrence_rate: float) -> list[LearningInsight]",L5:incidents_facade,LearningInsight | append | min | round,asyncio | postmortem_driver,pure,no,55,Generate insights based on category statistics.
incidents,L5,postmortem_engine,PostMortemService._get_resolution_summary,"async _get_resolution_summary(tenant_id: str, incident_id: str) -> Optional[ResolutionSummary]",L5:incidents_facade,ResolutionSummary | fetch_resolution_summary | int,asyncio | postmortem_driver,pure,yes,24,Get resolution summary for an incident.
incidents,L5,postmortem_engine,PostMortemService.get_category_learnings,"async get_category_learnings(tenant_id: str, category: str, baseline_days: int) -> Optional[CategoryLearnings]",L5:incidents_facade,CategoryLearnings | _generate_category_insights | fetch_category_stats | fetch_recurrence_data | fetch_resolution_methods | float | max | min | round,asyncio | postmortem_driver,pure,yes,63,Get aggregated learnings for a category.
incidents,L5,postmortem_engine,PostMortemService.get_incident_learnings,"async get_incident_learnings(tenant_id: str, incident_id: str) -> Optional[PostMortemResult]",L5:incidents_facade,PostMortemResult | _extract_insights | _find_similar_incidents | _get_resolution_summary | now,asyncio | postmortem_driver,pure,yes,42,Get post-mortem learnings for a specific incident.
incidents,L5,prevention_engine,BaseValidator.validate,validate(ctx: PreventionContext) -> List[PolicyViolation],?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,,db_async | policy_violation_service | prometheus_client,pure,no,3,Validate and return list of violations. Empty list = pass.
incidents,L5,prevention_engine,BudgetValidator.__init__,"__init__(max_tokens: int, max_cost_usd: float)",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,,db_async | policy_violation_service | prometheus_client,pure,no,3,
incidents,L5,prevention_engine,BudgetValidator.validate,validate(ctx: PreventionContext) -> List[PolicyViolation],?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PolicyViolation | append,db_async | policy_violation_service | prometheus_client,pure,no,41,
incidents,L5,prevention_engine,ContentAccuracyValidatorV2.__init__,__init__(strict_mode: bool),?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,compile,db_async | policy_violation_service | prometheus_client,pure,no,4,
incidents,L5,prevention_engine,ContentAccuracyValidatorV2._extract_claim,"_extract_claim(text: str, terms: List[str]) -> str",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,any | escape | search | split | strip,db_async | policy_violation_service | prometheus_client,pure,no,6,
incidents,L5,prevention_engine,ContentAccuracyValidatorV2._get_value,"_get_value(data: Dict[str, Any], key: str) -> Any",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,get | isinstance | split,db_async | policy_violation_service | prometheus_client,pure,no,10,
incidents,L5,prevention_engine,ContentAccuracyValidatorV2.validate,validate(ctx: PreventionContext) -> List[PolicyViolation],?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PolicyViolation | _extract_claim | _get_value | any | append | escape | items | lower | search,db_async | policy_violation_service | prometheus_client,pure,no,46,
incidents,L5,prevention_engine,HallucinationValidator.__init__,__init__(context_required_fields: Optional[List[str]]),?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,compile,db_async | policy_violation_service | prometheus_client,pure,no,3,
incidents,L5,prevention_engine,HallucinationValidator._claim_in_context,"_claim_in_context(claim: str, context: Dict[str, Any]) -> bool",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,findall | len | lower | str | sum,db_async | policy_violation_service | prometheus_client,pure,no,8,Check if claim can be found anywhere in context.
incidents,L5,prevention_engine,HallucinationValidator.validate,validate(ctx: PreventionContext) -> List[PolicyViolation],?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PolicyViolation | _claim_in_context | append | group | search,db_async | policy_violation_service | prometheus_client,pure,no,27,
incidents,L5,prevention_engine,PIIValidator.__init__,__init__(allowed_pii: Optional[Set[str]]),?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,compile | items | set,db_async | policy_violation_service | prometheus_client,pure,no,5,
incidents,L5,prevention_engine,PIIValidator._redact,_redact(value: str) -> str,?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,len,db_async | policy_violation_service | prometheus_client,pure,no,4,
incidents,L5,prevention_engine,PIIValidator.validate,validate(ctx: PreventionContext) -> List[PolicyViolation],?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PolicyViolation | _redact | append | findall | items | len | upper,db_async | policy_violation_service | prometheus_client,pure,no,26,
incidents,L5,prevention_engine,PolicyViolation.to_dict,"to_dict() -> Dict[str, Any]",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,,db_async | policy_violation_service | prometheus_client,pure,no,12,
incidents,L5,prevention_engine,PreventionContext.hash_output,hash_output() -> str,?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,encode | hexdigest | sha256,db_async | policy_violation_service | prometheus_client,pure,no,3,Generate deterministic hash of output for replay verification.
incidents,L5,prevention_engine,PreventionEngine.__init__,"__init__(validators: Optional[List[BaseValidator]], strict_mode: bool, block_on_critical: bool, modify_on_high: bool, emit_metrics: bool)",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,BudgetValidator | ContentAccuracyValidatorV2 | HallucinationValidator | PIIValidator | SafetyValidator,db_async | policy_violation_service | prometheus_client,pure,no,21,
incidents,L5,prevention_engine,PreventionEngine._emit_metrics,"_emit_metrics(ctx: PreventionContext, result: PreventionResult) -> None",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,Counter | Histogram | inc | labels | observe,db_async | policy_violation_service | prometheus_client,pure,no,41,Emit Prometheus metrics for prevention results.
incidents,L5,prevention_engine,PreventionEngine._generate_safe_response,"_generate_safe_response(ctx: PreventionContext, violations: List[PolicyViolation]) -> str",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,,db_async | policy_violation_service | prometheus_client,pure,no,40,Generate a safe response based on the violation type.
incidents,L5,prevention_engine,PreventionEngine.evaluate,evaluate(ctx: PreventionContext) -> PreventionResult,?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PolicyViolation | PreventionResult | _emit_metrics | _generate_safe_response | append | extend | get | int | len | sort | str | time | validate,db_async | policy_violation_service | prometheus_client,pure,no,69,Evaluate all policies and return result.
incidents,L5,prevention_engine,PreventionResult.highest_severity,highest_severity() -> Optional[Severity],?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,any,db_async | policy_violation_service | prometheus_client,pure,no,8,
incidents,L5,prevention_engine,PreventionResult.primary_violation,primary_violation() -> Optional[PolicyViolation],?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,,db_async | policy_violation_service | prometheus_client,pure,no,4,
incidents,L5,prevention_engine,PreventionResult.to_dict,"to_dict() -> Dict[str, Any]",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,to_dict,db_async | policy_violation_service | prometheus_client,pure,no,11,
incidents,L5,prevention_engine,SafetyValidator.__init__,__init__(),?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,compile | items,db_async | policy_violation_service | prometheus_client,pure,no,4,
incidents,L5,prevention_engine,SafetyValidator.validate,validate(ctx: PreventionContext) -> List[PolicyViolation],?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PolicyViolation | append | group | items | search | upper,db_async | policy_violation_service | prometheus_client,pure,no,25,
incidents,L5,prevention_engine,_create_incident_with_service,"async _create_incident_with_service(session: Any, ctx: PreventionContext, primary: PolicyViolation, evidence: dict) -> Optional[str]",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PolicyViolationService | ViolationFact | persist_violation_and_create_incident,db_async | policy_violation_service | prometheus_client,pure,yes,29,Helper to create incident using PolicyViolationService.
incidents,L5,prevention_engine,create_incident_from_violation,"async create_incident_from_violation(ctx: PreventionContext, result: PreventionResult, session: Optional[Any]) -> Optional[str]",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,AsyncSessionLocal | _create_incident_with_service | getLogger | isoformat | items | list | str | warning,db_async | policy_violation_service | prometheus_client,pure,yes,59,Create an incident from prevention violation.
incidents,L5,prevention_engine,evaluate_prevention,"evaluate_prevention(tenant_id: str, call_id: str, user_query: str, llm_output: str, context_data: Dict[str, Any], model: str, user_id: Optional[str]) -> PreventionResult",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PreventionContext | evaluate | get_prevention_engine,db_async | policy_violation_service | prometheus_client,pure,no,36,Convenience function to evaluate prevention.
incidents,L5,prevention_engine,get_prevention_engine,get_prevention_engine() -> PreventionEngine,?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PreventionEngine,db_async | policy_violation_service | prometheus_client,pure,no,6,Get global prevention engine instance.
incidents,L5,recovery_rule_engine,CompositeRule.__init__,"__init__(rule_id: str, name: str, rules: List[Rule], logic: str, **kwargs)",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,__init__ | lower | super,,pure,no,11,
incidents,L5,recovery_rule_engine,CompositeRule.evaluate,evaluate(context: RuleContext) -> RuleResult,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,RuleResult | all | any | evaluate | join | max | min | sorted | to_dict,,pure,no,28,
incidents,L5,recovery_rule_engine,ErrorCodeRule.__init__,"__init__(rule_id: str, name: str, error_patterns: List[str], action_code: str, score: float, **kwargs)",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,__init__ | super | upper,,pure,no,7,
incidents,L5,recovery_rule_engine,ErrorCodeRule.evaluate,evaluate(context: RuleContext) -> RuleResult,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,RuleResult | startswith | upper,,pure,no,22,
incidents,L5,recovery_rule_engine,EvaluationResult.to_dict,"to_dict() -> Dict[str, Any]",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,to_dict,,pure,no,9,
incidents,L5,recovery_rule_engine,HistoricalPatternRule.__init__,"__init__(rule_id: str, name: str, min_occurrences: int, min_success_rate: float, **kwargs)",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,__init__ | super,,pure,no,4,
incidents,L5,recovery_rule_engine,HistoricalPatternRule.evaluate,evaluate(context: RuleContext) -> RuleResult,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,Counter | RuleResult | get | len | most_common | sum,,pure,no,64,
incidents,L5,recovery_rule_engine,OccurrenceThresholdRule.__init__,"__init__(rule_id: str, name: str, threshold: int, action_code: str, score: float, **kwargs)",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,__init__ | super,,pure,no,5,
incidents,L5,recovery_rule_engine,OccurrenceThresholdRule.evaluate,evaluate(context: RuleContext) -> RuleResult,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,RuleResult,,pure,no,23,
incidents,L5,recovery_rule_engine,RecoveryRuleEngine.__init__,__init__(rules: Optional[List[Rule]]),?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,copy | sort,,pure,no,10,Initialize rule engine.
incidents,L5,recovery_rule_engine,RecoveryRuleEngine.add_rule,add_rule(rule: Rule) -> None,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,append | sort,,pure,no,4,Add a rule to the engine.
incidents,L5,recovery_rule_engine,RecoveryRuleEngine.evaluate,evaluate(context: RuleContext) -> EvaluationResult,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,EvaluationResult | RuleResult | append | debug | evaluate | int | join | len | max | min | perf_counter | round | sort | str | sum,,pure,no,79,Evaluate all rules against the context.
incidents,L5,recovery_rule_engine,RecoveryRuleEngine.remove_rule,remove_rule(rule_id: str) -> bool,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,len,,pure,no,5,Remove a rule by ID.
incidents,L5,recovery_rule_engine,Rule.__init__,"__init__(rule_id: str, name: str, priority: int, weight: float)",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,,,pure,no,11,
incidents,L5,recovery_rule_engine,Rule.__repr__,__repr__() -> str,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,,,pure,no,2,
incidents,L5,recovery_rule_engine,Rule.evaluate,evaluate(context: RuleContext) -> RuleResult,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,,,pure,no,3,Evaluate rule against context. Override in subclasses.
incidents,L5,recovery_rule_engine,RuleContext.to_dict,"to_dict() -> Dict[str, Any]",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,isoformat | len,,pure,no,11,
incidents,L5,recovery_rule_engine,RuleResult.to_dict,"to_dict() -> Dict[str, Any]",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,,,pure,no,11,
incidents,L5,recovery_rule_engine,SkillSpecificRule.__init__,"__init__(rule_id: str, name: str, skill_ids: List[str], action_code: str, score: float, **kwargs)",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,__init__ | super,,pure,no,5,
incidents,L5,recovery_rule_engine,SkillSpecificRule.evaluate,evaluate(context: RuleContext) -> RuleResult,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,RuleResult,,pure,no,19,
incidents,L5,recovery_rule_engine,classify_error_category,classify_error_category(error_codes: List[str]) -> str,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,any | items | join | lower,,pure,no,19,Classify error codes into a category.
incidents,L5,recovery_rule_engine,combine_confidences,"combine_confidences(rule_confidence: float, match_confidence: float) -> float",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,,,pure,no,16,Combine rule and matcher confidence scores.
incidents,L5,recovery_rule_engine,evaluate_rules,"evaluate_rules(error_code: str, error_message: str, skill_id: Optional[str], tenant_id: Optional[str], occurrence_count: int, historical_matches: Optional[List[Dict[str, Any]]], custom_rules: Optional[List[Rule]]) -> EvaluationResult",?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,RecoveryRuleEngine | RuleContext | add_rule | evaluate,,pure,no,40,Convenience function to evaluate rules against a failure.
incidents,L5,recovery_rule_engine,should_auto_execute,should_auto_execute(confidence: float) -> bool,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,,,pure,no,13,Determine if a recovery action should be auto-executed based on confidence.
incidents,L5,recovery_rule_engine,should_select_action,should_select_action(combined_confidence: float) -> bool,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,,,pure,no,15,Determine if an action should be selected based on combined confidence.
incidents,L5,recovery_rule_engine,suggest_recovery_mode,suggest_recovery_mode(error_codes: List[str]) -> str,?:recovery | ?:failure_intelligence | ?:failure_classification_engine | ?:recovery_evaluation_engine | L4:recovery_decisions | L5:recovery_evaluation_engine | L2:recovery | ?:test_m10_recovery_enhanced,any | items | join | lower,,pure,no,19,Suggest a recovery mode based on error codes.
incidents,L5,recurrence_analysis_engine,RecurrenceAnalysisService.__init__,__init__(session: 'AsyncSession'),L5:incidents_facade,RecurrenceAnalysisDriver,asyncio | recurrence_analysis_driver | time,pure,no,2,
incidents,L5,recurrence_analysis_engine,RecurrenceAnalysisService._snapshot_to_group,_snapshot_to_group(snapshot: RecurrenceGroupSnapshot) -> RecurrenceGroup,L5:incidents_facade,RecurrenceGroup,asyncio | recurrence_analysis_driver | time,pure,no,12,Convert driver snapshot to domain type. No business logic.
incidents,L5,recurrence_analysis_engine,RecurrenceAnalysisService.analyze_recurrence,"async analyze_recurrence(tenant_id: str, baseline_days: int, recurrence_threshold: int, limit: int) -> RecurrenceResult",L5:incidents_facade,RecurrenceResult | _snapshot_to_group | append | fetch_recurrence_groups | max | min | utc_now,asyncio | recurrence_analysis_driver | time,pure,yes,47,Analyze incident recurrence patterns.
incidents,L5,recurrence_analysis_engine,RecurrenceAnalysisService.get_recurrence_for_category,"async get_recurrence_for_category(tenant_id: str, category: str, baseline_days: int) -> RecurrenceGroup | None",L5:incidents_facade,_snapshot_to_group | fetch_recurrence_for_category,asyncio | recurrence_analysis_driver | time,pure,yes,28,Get recurrence details for a specific category.
incidents,L5,semantic_failures,format_violation_message,"format_violation_message(code: FailureCode, context_msg: str) -> str",?:semantic_validator | ?:__init__,get_failure_info,semantic_types,pure,no,4,Format a violation message with context.
incidents,L5,semantic_failures,get_failure_info,"get_failure_info(code: FailureCode) -> Dict[str, Any]",?:semantic_validator | ?:__init__,get | hasattr | str,semantic_types,pure,no,11,Get failure taxonomy info for a code (INT-* or SEM-*).
incidents,L5,semantic_failures,get_fix_action,get_fix_action(code: FailureCode) -> str,?:semantic_validator | ?:__init__,get | get_failure_info,semantic_types,pure,no,3,Get the fix action for a failure code.
incidents,L5,semantic_failures,get_fix_owner,get_fix_owner(code: FailureCode) -> str,?:semantic_validator | ?:__init__,get | get_failure_info,semantic_types,pure,no,3,Get the fix owner for a failure code.
incidents,L5,semantic_failures,get_violation_class,get_violation_class(code: FailureCode) -> ViolationClass,?:semantic_validator | ?:__init__,get | get_failure_info,semantic_types,pure,no,3,Get the violation class for a failure code.
incidents,L5,severity_policy,IncidentSeverityEngine.__init__,__init__(config: SeverityConfig | None),L6:incident_aggregator,default,killswitch,pure,no,2,
incidents,L5,severity_policy,IncidentSeverityEngine.calculate_severity_for_calls,calculate_severity_for_calls(calls_affected: int) -> str,L6:incident_aggregator,,killswitch,pure,no,22,Calculate severity based on number of affected calls.
incidents,L5,severity_policy,IncidentSeverityEngine.get_initial_severity,get_initial_severity(trigger_type: str) -> str,L6:incident_aggregator,get,killswitch,pure,no,13,Get initial severity based on trigger type.
incidents,L5,severity_policy,IncidentSeverityEngine.should_escalate,"should_escalate(current_severity: str, calls_affected: int) -> Tuple[bool, str]",L6:incident_aggregator,calculate_severity_for_calls | index,killswitch,pure,no,35,Determine if an incident should be escalated.
incidents,L5,severity_policy,SeverityConfig.default,default() -> 'SeverityConfig',L6:incident_aggregator,cls,killswitch,pure,no,10,Default severity thresholds.
incidents,L5,severity_policy,generate_incident_title,"generate_incident_title(trigger_type: str, trigger_value: str) -> str",L6:incident_aggregator,get,killswitch,pure,no,22,Generate human-readable incident title.
incidents,L6,export_bundle_driver,ExportBundleDriver.__init__,__init__(trace_store: Optional[TraceStore]),L4:incidents_handler | L4:incidents_bridge,,db | export_bundles | killswitch | sqlmodel | store,pure,no,8,Initialize export bundle service.
incidents,L6,export_bundle_driver,ExportBundleDriver._assess_business_impact,"_assess_business_impact(incident: Incident, run: Optional[Run]) -> str",L4:incidents_handler | L4:incidents_bridge,,db | export_bundles | killswitch | sqlmodel | store,pure,no,10,Assess business impact for executive summary.
incidents,L6,export_bundle_driver,ExportBundleDriver._assess_risk_level,_assess_risk_level(incident: Incident) -> str,L4:incidents_handler | L4:incidents_bridge,getattr,db | export_bundles | killswitch | sqlmodel | store,pure,no,8,Assess risk level for executive summary.
incidents,L6,export_bundle_driver,ExportBundleDriver._compute_bundle_hash,_compute_bundle_hash(bundle: EvidenceBundle) -> str,L4:incidents_handler | L4:incidents_bridge,dumps | encode | hexdigest | isoformat | model_dump | sha256,db | export_bundles | killswitch | sqlmodel | store,pure,no,14,Compute SHA256 hash of bundle for integrity verification.
incidents,L6,export_bundle_driver,ExportBundleDriver._generate_attestation,_generate_attestation(bundle: EvidenceBundle) -> str,L4:incidents_handler | L4:incidents_bridge,,db | export_bundles | killswitch | sqlmodel | store,pure,no,10,Generate SOC2 attestation statement.
incidents,L6,export_bundle_driver,ExportBundleDriver._generate_incident_summary,"_generate_incident_summary(incident: Incident, run: Optional[Run]) -> str",L4:incidents_handler | L4:incidents_bridge,getattr,db | export_bundles | killswitch | sqlmodel | store,pure,no,12,Generate non-technical incident summary.
incidents,L6,export_bundle_driver,ExportBundleDriver._generate_recommendations,"_generate_recommendations(incident: Incident, run: Optional[Run]) -> list[str]",L4:incidents_handler | L4:incidents_bridge,,db | export_bundles | killswitch | sqlmodel | store,pure,no,10,Generate recommended actions.
incidents,L6,export_bundle_driver,ExportBundleDriver.create_evidence_bundle,"async create_evidence_bundle(incident_id: str, exported_by: str, export_reason: Optional[str], include_raw_steps: bool) -> EvidenceBundle",L4:incidents_handler | L4:incidents_bridge,EvidenceBundle | PolicyContext | Session | TraceStepEvidence | ValueError | _compute_bundle_hash | append | enumerate | exec | first | get | get_trace_steps | get_trace_summary | getattr | info,db | export_bundles | killswitch | sqlmodel | store,pure,yes,115,Create evidence bundle from incident.
incidents,L6,export_bundle_driver,ExportBundleDriver.create_executive_debrief,"async create_executive_debrief(incident_id: str, prepared_for: Optional[str], prepared_by: str) -> ExecutiveDebriefBundle",L4:incidents_handler | L4:incidents_bridge,ExecutiveDebriefBundle | Session | ValueError | _assess_business_impact | _assess_risk_level | _generate_incident_summary | _generate_recommendations | exec | first | get | getattr | hasattr | info | int | select,db | export_bundles | killswitch | sqlmodel | store,pure,yes,75,Create executive summary (non-technical).
incidents,L6,export_bundle_driver,ExportBundleDriver.create_soc2_bundle,"async create_soc2_bundle(incident_id: str, exported_by: str, compliance_period_start: Optional[datetime], compliance_period_end: Optional[datetime], auditor_notes: Optional[str]) -> SOC2Bundle",L4:incidents_handler | L4:incidents_bridge,SOC2Bundle | _generate_attestation | create_evidence_bundle | info | len | list | now | replace,db | export_bundles | killswitch | sqlmodel | store,pure,yes,70,Create SOC2-compliant bundle.
incidents,L6,export_bundle_driver,ExportBundleDriver.trace_store,trace_store() -> TraceStore,L4:incidents_handler | L4:incidents_bridge,TraceStore,db | export_bundles | killswitch | sqlmodel | store,pure,no,5,Get or create TraceStore instance.
incidents,L6,export_bundle_driver,get_export_bundle_driver,get_export_bundle_driver() -> ExportBundleDriver,L4:incidents_handler | L4:incidents_bridge,ExportBundleDriver,db | export_bundles | killswitch | sqlmodel | store,pure,no,6,Get or create ExportBundleDriver singleton.
incidents,L6,incident_aggregator,IncidentAggregator.__init__,"__init__(clock: ClockFn, uuid_fn: UuidFn, config: Optional[IncidentAggregatorConfig], severity_engine: Optional[IncidentSeverityEngine])",L5:policy_violation_engine,IncidentAggregatorConfig | IncidentSeverityEngine,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,pure,no,27,Construct an IncidentAggregator with explicit dependencies.
incidents,L6,incident_aggregator,IncidentAggregator._add_call_to_incident,"_add_call_to_incident(session: Session, incident: Incident, call_id: Optional[str], cost_delta_cents: Decimal, metadata: Optional[Dict[str, Any]]) -> None",L5:policy_violation_engine,_add_incident_event | add | add_related_call | clock | get_related_call_ids | len | should_escalate | warning,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,db_write,no,47,Add a call to an existing incident and potentially escalate.
incidents,L6,incident_aggregator,IncidentAggregator._add_incident_event,"_add_incident_event(session: Session, incident: Incident, event_type: str, description: str, data: Optional[Dict[str, Any]]) -> IncidentEvent",L5:policy_violation_engine,IncidentEvent | add | set_data | uuid_fn,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,db_write,no,22,Add an event to an incident's timeline.
incidents,L6,incident_aggregator,IncidentAggregator._can_create_incident,"_can_create_incident(session: Session, tenant_id: str, now: datetime) -> bool",L5:policy_violation_engine,and_ | count | exec | isinstance | one | select | timedelta | where,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,pure,no,15,Check if we can create a new incident (rate limiting).
incidents,L6,incident_aggregator,IncidentAggregator._create_incident,"_create_incident(session: Session, key: IncidentKey, trigger_value: str, call_id: Optional[str], cost_delta_cents: Decimal, auto_action: Optional[str], metadata: Optional[Dict[str, Any]]) -> Incident",L5:policy_violation_engine,Incident | _add_incident_event | add | add_related_call | clock | flush | generate_incident_title | get_initial_severity | info | refresh | uuid_fn,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,db_write,no,54,Create a new incident.
incidents,L6,incident_aggregator,IncidentAggregator._find_open_incident,"_find_open_incident(session: Session, key: IncidentKey, now: datetime) -> Optional[Incident]",L5:policy_violation_engine,and_ | cast | desc | exec | first | limit | order_by | select | timedelta | where,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,pure,no,21,Find an open incident matching the key within the current window.
incidents,L6,incident_aggregator,IncidentAggregator._get_rate_limit_incident,"_get_rate_limit_incident(session: Session, tenant_id: str, now: datetime) -> Incident",L5:policy_violation_engine,Incident | add | and_ | cast | desc | exec | first | flush | limit | order_by | refresh | select | timedelta | uuid_fn | where,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,db_write,no,47,Get or create a rate-limit overflow incident.
incidents,L6,incident_aggregator,IncidentAggregator.get_incident_stats,"get_incident_stats(session: Session, tenant_id: str, since: Optional[datetime]) -> Dict[str, Any]",L5:policy_violation_engine,and_ | clock | count | exec | isoformat | one | select | sum | timedelta | where,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,pure,no,60,Get incident statistics for a tenant.
incidents,L6,incident_aggregator,IncidentAggregator.get_or_create_incident,"get_or_create_incident(session: Session, tenant_id: str, trigger_type: str, trigger_value: str, call_id: Optional[str], cost_delta_cents: Decimal, auto_action: Optional[str], metadata: Optional[Dict[str, Any]]) -> Tuple[Incident, bool]",L5:policy_violation_engine,Decimal | _add_call_to_incident | _can_create_incident | _create_incident | _find_open_incident | _get_rate_limit_incident | clock | from_event | warning,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,pure,no,59,Get existing incident or create new one if needed.
incidents,L6,incident_aggregator,IncidentAggregator.resolve_stale_incidents,"resolve_stale_incidents(session: Session, tenant_id: Optional[str]) -> int",L5:policy_violation_engine,_add_incident_event | add | all | and_ | append | clock | exec | info | resolve | select | timedelta | where,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,db_write,no,42,Auto-resolve incidents that have been open without activity.
incidents,L6,incident_aggregator,IncidentKey.__eq__,__eq__(other),L5:policy_violation_engine,isinstance,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,pure,no,8,
incidents,L6,incident_aggregator,IncidentKey.__hash__,__hash__(),L5:policy_violation_engine,hash | isoformat,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,pure,no,2,
incidents,L6,incident_aggregator,IncidentKey.from_event,"from_event(tenant_id: str, trigger_type: str, event_time: datetime, window_seconds: int) -> 'IncidentKey'",L5:policy_violation_engine,cls | fromtimestamp | int | timestamp,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,pure,no,10,"Create an incident key from an event, bucketed to window."
incidents,L6,incident_aggregator,create_incident_aggregator,create_incident_aggregator(config: Optional[IncidentAggregatorConfig]) -> IncidentAggregator,L5:policy_violation_engine,IncidentAggregator,killswitch | runtime | severity_policy | sqlalchemy | sqlmodel,pure,no,19,Create an IncidentAggregator with canonical dependencies.
incidents,L6,incident_driver,IncidentDriver.__init__,__init__(decision_port: IncidentDecisionPort),?:incident_driver | ?:__init__ | L4:incidents_bridge | L5:incident_driver,,audit_store | incident_decision_port | rac_models,pure,no,9,Initialize driver with a decision port.
incidents,L6,incident_driver,IncidentDriver._emit_ack,"_emit_ack(run_id: str, result_id: Optional[str], error: Optional[str]) -> None",?:incident_driver | ?:__init__ | L4:incidents_bridge | L5:incident_driver,DomainAck | UUID | add_ack | debug | get_audit_store | warning,audit_store | incident_decision_port | rac_models,pure,no,39,Emit RAC acknowledgment for incident creation.
incidents,L6,incident_driver,IncidentDriver.check_and_create_incident,"check_and_create_incident(run_id: str, status: str, error_message: Optional[str], tenant_id: Optional[str], agent_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",?:incident_driver | ?:__init__ | L4:incidents_bridge | L5:incident_driver,check_and_create_incident | debug,audit_store | incident_decision_port | rac_models,pure,no,41,Check if an incident should be created for a run and create it.
incidents,L6,incident_driver,IncidentDriver.create_incident_for_run,"create_incident_for_run(run_id: str, tenant_id: str, run_status: str, error_code: Optional[str], error_message: Optional[str], agent_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",?:incident_driver | ?:__init__ | L4:incidents_bridge | L5:incident_driver,_emit_ack | create_incident_for_run | debug | error | str,audit_store | incident_decision_port | rac_models,pure,no,63,Create an incident record for any run (success or failure).
incidents,L6,incident_driver,IncidentDriver.get_incidents_for_run,"get_incidents_for_run(run_id: str) -> List[Dict[str, Any]]",?:incident_driver | ?:__init__ | L4:incidents_bridge | L5:incident_driver,get_incidents_for_run,audit_store | incident_decision_port | rac_models,pure,no,11,Get all incidents associated with a run.
incidents,L6,incident_pattern_driver,IncidentPatternDriver.__init__,__init__(session: AsyncSession),L5:incident_pattern_engine,,asyncio | sqlalchemy,pure,no,3,Initialize with async database session.
incidents,L6,incident_pattern_driver,IncidentPatternDriver.fetch_cascade_failures,"async fetch_cascade_failures(tenant_id: str, window_start: datetime, threshold: int, limit: int) -> List[Dict[str, Any]]",L5:incident_pattern_engine,dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,42,Fetch incidents grouped by source run for cascade detection.
incidents,L6,incident_pattern_driver,IncidentPatternDriver.fetch_category_clusters,"async fetch_category_clusters(tenant_id: str, window_start: datetime, threshold: int, limit: int) -> List[Dict[str, Any]]",L5:incident_pattern_engine,dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,42,Fetch incidents grouped by category for cluster detection.
incidents,L6,incident_pattern_driver,IncidentPatternDriver.fetch_incidents_count,"async fetch_incidents_count(tenant_id: str, window_start: datetime) -> int",L5:incident_pattern_engine,execute | scalar | text,asyncio | sqlalchemy,db_write,yes,28,Count incidents within time window.
incidents,L6,incident_pattern_driver,IncidentPatternDriver.fetch_severity_spikes,"async fetch_severity_spikes(tenant_id: str, threshold: int, limit: int) -> List[Dict[str, Any]]",L5:incident_pattern_engine,dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,45,Fetch high/critical incidents in the last hour.
incidents,L6,incident_pattern_driver,get_incident_pattern_driver,get_incident_pattern_driver(session: AsyncSession) -> IncidentPatternDriver,L5:incident_pattern_engine,IncidentPatternDriver,asyncio | sqlalchemy,pure,no,3,Factory function to get IncidentPatternDriver instance.
incidents,L6,incident_read_driver,IncidentReadDriver.__init__,__init__(session: Session),L6:__init__ | L5:incident_read_engine,,killswitch | sqlalchemy | sqlmodel,pure,no,3,Initialize with database session.
incidents,L6,incident_read_driver,IncidentReadDriver.count_incidents_since,"count_incidents_since(tenant_id: str, since: datetime) -> int",L6:__init__ | L5:incident_read_engine,and_ | count | exec | first | select | where,killswitch | sqlalchemy | sqlmodel,pure,no,23,Count incidents since a given time.
incidents,L6,incident_read_driver,IncidentReadDriver.get_incident,"get_incident(incident_id: str, tenant_id: str) -> Optional[Incident]",L6:__init__ | L5:incident_read_engine,and_ | exec | first | select | where,killswitch | sqlalchemy | sqlmodel,pure,no,23,Get a single incident by ID with tenant isolation.
incidents,L6,incident_read_driver,IncidentReadDriver.get_incident_events,get_incident_events(incident_id: str) -> List[IncidentEvent],L6:__init__ | L5:incident_read_engine,all | exec | hasattr | order_by | select | where,killswitch | sqlalchemy | sqlmodel,pure,no,16,Get timeline events for an incident.
incidents,L6,incident_read_driver,IncidentReadDriver.get_last_incident,get_last_incident(tenant_id: str) -> Optional[Incident],L6:__init__ | L5:incident_read_engine,desc | exec | first | limit | order_by | select | where,killswitch | sqlalchemy | sqlmodel,pure,no,16,Get the most recent incident for a tenant.
incidents,L6,incident_read_driver,IncidentReadDriver.list_incidents,"list_incidents(tenant_id: str, status: Optional[str], severity: Optional[str], from_date: Optional[datetime], to_date: Optional[datetime], limit: int, offset: int) -> Tuple[List[Incident], int]",L6:__init__ | L5:incident_read_engine,all | and_ | append | count | desc | exec | first | hasattr | limit | min | offset | order_by | select | where,killswitch | sqlalchemy | sqlmodel,pure,no,51,List incidents for a tenant with optional filters.
incidents,L6,incident_read_driver,get_incident_read_driver,get_incident_read_driver(session: Session) -> IncidentReadDriver,L6:__init__ | L5:incident_read_engine,IncidentReadDriver,killswitch | sqlalchemy | sqlmodel,pure,no,3,Factory function to get IncidentReadDriver instance.
incidents,L6,incident_write_driver,IncidentWriteDriver.__init__,__init__(session: Session),?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,,killswitch | sqlalchemy | sqlmodel,pure,no,3,Initialize with database session.
incidents,L6,incident_write_driver,IncidentWriteDriver.create_incident_event,"create_incident_event(incident_id: str, event_type: str, description: str) -> IncidentEvent",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,IncidentEvent | add,killswitch | sqlalchemy | sqlmodel,db_write,no,24,Create a new incident timeline event.
incidents,L6,incident_write_driver,IncidentWriteDriver.fetch_incidents_by_run_id,"fetch_incidents_by_run_id(run_id: str) -> List[Dict[str, Any]]",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,append | execute | isoformat | text,killswitch | sqlalchemy | sqlmodel,db_write,no,31,Get all incidents linked to a run.
incidents,L6,incident_write_driver,IncidentWriteDriver.fetch_suppressing_policy,"fetch_suppressing_policy(tenant_id: str, error_code: str, category: str) -> Optional[Dict[str, Any]]",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,execute | fetchone | text,killswitch | sqlalchemy | sqlmodel,db_write,no,47,Check if an active policy_rule suppresses this incident pattern.
incidents,L6,incident_write_driver,IncidentWriteDriver.insert_incident,"insert_incident(incident_id: str, tenant_id: str, title: str, severity: str, status: str, trigger_type: str, category: str, description: str, source_run_id: str, source_type: str, now: datetime, error_code: Optional[str], error_message: Optional[str], agent_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> bool",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,execute | fetchone | text,killswitch | sqlalchemy | sqlmodel,db_write,no,88,Insert a new incident record.
incidents,L6,incident_write_driver,IncidentWriteDriver.insert_incident_from_anomaly,"insert_incident_from_anomaly(incident_id: str, tenant_id: str, title: str, severity: str, trigger_type: str, category: str, description: str, impact_scope: str, affected_agent_id: Optional[str], cost_delta_cents: Any, now: datetime) -> bool",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,execute | fetchone | str | text,killswitch | sqlalchemy | sqlmodel,db_write,no,63,Insert incident from cost anomaly (no source_run_id).
incidents,L6,incident_write_driver,IncidentWriteDriver.insert_policy_proposal,"insert_policy_proposal(proposal_id: str, tenant_id: str, proposal_name: str, proposal_type: str, rationale: str, proposed_rule: Dict[str, Any], triggering_feedback_ids: List[str], now: datetime, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> bool",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,dumps | execute | text,killswitch | sqlalchemy | sqlmodel,db_write,no,58,Insert a policy proposal for high-severity incidents.
incidents,L6,incident_write_driver,IncidentWriteDriver.insert_prevention_record,"insert_prevention_record(prevention_id: str, policy_id: str, pattern_id: str, original_incident_id: str, blocked_incident_id: str, tenant_id: str, now: datetime, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> bool",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,execute | text,killswitch | sqlalchemy | sqlmodel,db_write,no,58,Insert a prevention record when policy suppresses an incident.
incidents,L6,incident_write_driver,IncidentWriteDriver.refresh_incident,refresh_incident(incident: Incident) -> Incident,?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,refresh,killswitch | sqlalchemy | sqlmodel,pure,no,12,Refresh incident from database after commit.
incidents,L6,incident_write_driver,IncidentWriteDriver.update_incident_acknowledged,"update_incident_acknowledged(incident: Incident, acknowledged_at: datetime, acknowledged_by: str) -> None",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,add,killswitch | sqlalchemy | sqlmodel,db_write,no,18,Update incident to acknowledged status.
incidents,L6,incident_write_driver,IncidentWriteDriver.update_incident_resolved,"update_incident_resolved(incident: Incident, resolved_at: datetime, resolved_by: str, resolution_method: str | None) -> None",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,add,killswitch | sqlalchemy | sqlmodel,db_write,no,22,Update incident to resolved status.
incidents,L6,incident_write_driver,IncidentWriteDriver.update_run_incident_count,update_run_incident_count(run_id: str) -> bool,?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,execute | fetchone | text,killswitch | sqlalchemy | sqlmodel,db_write,no,21,Increment incident_count on the runs table.
incidents,L6,incident_write_driver,IncidentWriteDriver.update_trace_incident_id,"update_trace_incident_id(run_id: str, incident_id: str) -> int",?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,execute | text,killswitch | sqlalchemy | sqlmodel,db_write,no,20,Propagate incident_id to aos_traces for cross-domain correlation.
incidents,L6,incident_write_driver,get_incident_write_driver,get_incident_write_driver(session: Session) -> IncidentWriteDriver,?:incident_write_engine | L6:__init__ | L5:incident_write_engine | L5:incident_engine | L5:anomaly_bridge,IncidentWriteDriver,killswitch | sqlalchemy | sqlmodel,pure,no,3,Factory function to get IncidentWriteDriver instance.
incidents,L6,incidents_facade_driver,IncidentsFacadeDriver.__init__,__init__(session: AsyncSession),L5:incidents_facade,,asyncio | killswitch | sqlalchemy,pure,no,2,
incidents,L6,incidents_facade_driver,IncidentsFacadeDriver._to_snapshot,_to_snapshot(inc: Incident) -> IncidentSnapshot,L5:incidents_facade,IncidentSnapshot,asyncio | killswitch | sqlalchemy,pure,no,22,Convert ORM model to snapshot. No business logic.
incidents,L6,incidents_facade_driver,IncidentsFacadeDriver.fetch_active_incidents,async fetch_active_incidents(tenant_id: str) -> IncidentListSnapshot,L5:incidents_facade,IncidentListSnapshot | _to_snapshot | all | asc | count | desc | execute | getattr | in_ | limit | offset | order_by | scalar | scalars | select,asyncio | killswitch | sqlalchemy,db_write,yes,73,Fetch active incidents (ACTIVE + ACKED states).
incidents,L6,incidents_facade_driver,IncidentsFacadeDriver.fetch_cost_impact_data,"async fetch_cost_impact_data(tenant_id: str, baseline_days: int, limit: int) -> list[CostImpactRowSnapshot]",L5:incidents_facade,CostImpactRowSnapshot | append | execute | float | mappings | text,asyncio | killswitch | sqlalchemy,db_write,yes,40,Fetch cost impact aggregates using raw SQL.
incidents,L6,incidents_facade_driver,IncidentsFacadeDriver.fetch_historical_incidents,"async fetch_historical_incidents(tenant_id: str, cutoff_date: datetime) -> IncidentListSnapshot",L5:incidents_facade,IncidentListSnapshot | _to_snapshot | all | asc | count | desc | execute | getattr | limit | offset | order_by | scalar | scalars | select | where,asyncio | killswitch | sqlalchemy,db_write,yes,58,Fetch historical incidents (resolved before cutoff date).
incidents,L6,incidents_facade_driver,IncidentsFacadeDriver.fetch_incident_by_id,"async fetch_incident_by_id(tenant_id: str, incident_id: str) -> Optional[IncidentSnapshot]",L5:incidents_facade,_to_snapshot | execute | scalar_one_or_none | select | where,asyncio | killswitch | sqlalchemy,db_write,yes,19,Fetch single incident by ID with tenant isolation.
incidents,L6,incidents_facade_driver,IncidentsFacadeDriver.fetch_incidents_by_run,"async fetch_incidents_by_run(tenant_id: str, run_id: str) -> list[IncidentSnapshot]",L5:incidents_facade,_to_snapshot | all | desc | execute | order_by | scalars | select | where,asyncio | killswitch | sqlalchemy,db_write,yes,17,Fetch all incidents linked to a specific run.
incidents,L6,incidents_facade_driver,IncidentsFacadeDriver.fetch_metrics_aggregates,"async fetch_metrics_aggregates(tenant_id: str, window_days: int) -> Optional[MetricsSnapshot]",L5:incidents_facade,MetricsSnapshot | execute | first | mappings | text,asyncio | killswitch | sqlalchemy,db_write,yes,65,Fetch aggregated metrics using raw SQL.
incidents,L6,incidents_facade_driver,IncidentsFacadeDriver.fetch_resolved_incidents,async fetch_resolved_incidents(tenant_id: str) -> IncidentListSnapshot,L5:incidents_facade,IncidentListSnapshot | _to_snapshot | all | asc | count | desc | execute | getattr | limit | offset | order_by | scalar | scalars | select | where,asyncio | killswitch | sqlalchemy,db_write,yes,71,Fetch resolved incidents.
incidents,L6,lessons_driver,LessonsDriver.__init__,__init__(session: Session),L4:incidents_bridge | L5:lessons_engine,,sqlalchemy | sqlmodel,pure,no,3,Initialize with database session.
incidents,L6,lessons_driver,LessonsDriver.fetch_debounce_count,"fetch_debounce_count(tenant_id: str, lesson_type: str, metric_type: str, hours: int, threshold_band: Optional[str]) -> int",L4:incidents_bridge | L5:lessons_engine,execute | fetchone | text,sqlalchemy | sqlmodel,db_write,no,45,Count recent lessons for debounce check.
incidents,L6,lessons_driver,LessonsDriver.fetch_expired_deferred,"fetch_expired_deferred(deferred_status: str, limit: int) -> List[tuple]",L4:incidents_bridge | L5:lessons_engine,execute | fetchall | list | text,sqlalchemy | sqlmodel,db_write,no,28,Get deferred lessons whose deferred_until has passed.
incidents,L6,lessons_driver,LessonsDriver.fetch_lesson_by_id,"fetch_lesson_by_id(lesson_id: str, tenant_id: str) -> Optional[Dict[str, Any]]",L4:incidents_bridge | L5:lessons_engine,execute | fetchone | isoformat | str | text,sqlalchemy | sqlmodel,db_write,no,55,Get a specific lesson by ID.
incidents,L6,lessons_driver,LessonsDriver.fetch_lesson_stats,fetch_lesson_stats(tenant_id: str) -> List[tuple],L4:incidents_bridge | L5:lessons_engine,execute | fetchall | list | text,sqlalchemy | sqlmodel,db_write,no,20,Get lesson counts grouped by type and status.
incidents,L6,lessons_driver,LessonsDriver.fetch_lessons_list,"fetch_lessons_list(tenant_id: str, lesson_type: Optional[str], status: Optional[str], severity: Optional[str], include_synthetic: bool, limit: int, offset: int) -> List[Dict[str, Any]]",L4:incidents_bridge | L5:lessons_engine,append | execute | fetchall | isoformat | join | str | text,sqlalchemy | sqlmodel,db_write,no,74,List lessons with optional filters.
incidents,L6,lessons_driver,LessonsDriver.insert_lesson,"insert_lesson(lesson_id: str, tenant_id: str, lesson_type: str, severity: Optional[str], source_event_id: str, source_event_type: str, source_run_id: Optional[str], title: str, description: str, proposed_action: Optional[str], detected_pattern: Optional[Dict[str, Any]], now: datetime, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> bool",L4:incidents_bridge | L5:lessons_engine,dumps | execute | fetchone | text,sqlalchemy | sqlmodel,db_write,no,76,Insert a new lesson record.
incidents,L6,lessons_driver,LessonsDriver.insert_policy_proposal_from_lesson,"insert_policy_proposal_from_lesson(proposal_id: str, tenant_id: str, title: str, description: str, proposed_action: Optional[str], source_lesson_id: str, created_at: datetime, created_by: str) -> bool",L4:incidents_bridge | L5:lessons_engine,execute | text,sqlalchemy | sqlmodel,db_write,no,51,Insert a draft policy proposal from a lesson.
incidents,L6,lessons_driver,LessonsDriver.update_lesson_converted,"update_lesson_converted(lesson_id: str, tenant_id: str, converted_status: str, proposal_id: str, converted_at: datetime) -> bool",L4:incidents_bridge | L5:lessons_engine,execute | fetchone | text,sqlalchemy | sqlmodel,db_write,no,42,Update lesson to converted_to_draft status.
incidents,L6,lessons_driver,LessonsDriver.update_lesson_deferred,"update_lesson_deferred(lesson_id: str, tenant_id: str, deferred_status: str, defer_until: datetime) -> bool",L4:incidents_bridge | L5:lessons_engine,execute | fetchone | text,sqlalchemy | sqlmodel,db_write,no,37,Update lesson to deferred status.
incidents,L6,lessons_driver,LessonsDriver.update_lesson_dismissed,"update_lesson_dismissed(lesson_id: str, tenant_id: str, dismissed_status: str, dismissed_at: datetime, dismissed_by: str, reason: str) -> bool",L4:incidents_bridge | L5:lessons_engine,execute | fetchone | text,sqlalchemy | sqlmodel,db_write,no,46,Update lesson to dismissed status.
incidents,L6,lessons_driver,LessonsDriver.update_lesson_reactivated,"update_lesson_reactivated(lesson_id: str, tenant_id: str, pending_status: str, from_status: str) -> bool",L4:incidents_bridge | L5:lessons_engine,execute | fetchone | text,sqlalchemy | sqlmodel,db_write,no,38,Reactivate a deferred lesson to pending.
incidents,L6,lessons_driver,get_lessons_driver,get_lessons_driver(session: Session) -> LessonsDriver,L4:incidents_bridge | L5:lessons_engine,LessonsDriver,sqlalchemy | sqlmodel,pure,no,3,Factory function to get LessonsDriver instance.
incidents,L6,llm_failure_driver,LLMFailureDriver.__init__,__init__(session: AsyncSession),?:llm_failure_engine | L5:llm_failure_engine,,asyncio | sqlalchemy,pure,no,3,Initialize with async database session.
incidents,L6,llm_failure_driver,LLMFailureDriver.fetch_contamination_check,"async fetch_contamination_check(run_id: str) -> Dict[str, int]",?:llm_failure_engine | L5:llm_failure_engine,execute | scalar | text,asyncio | sqlalchemy,db_write,yes,55,Check for downstream contamination (verification mode).
incidents,L6,llm_failure_driver,LLMFailureDriver.fetch_failure_by_run_id,"async fetch_failure_by_run_id(run_id: str, tenant_id: str) -> Optional[Tuple]",?:llm_failure_engine | L5:llm_failure_engine,execute | fetchone | text,asyncio | sqlalchemy,db_write,yes,30,Fetch failure record by run ID.
incidents,L6,llm_failure_driver,LLMFailureDriver.insert_evidence,"async insert_evidence(evidence_id: str, failure_id: str, evidence_type: str, evidence_data: Dict[str, Any], is_immutable: bool, created_at: datetime) -> None",?:llm_failure_engine | L5:llm_failure_engine,dumps | execute | text,asyncio | sqlalchemy,db_write,yes,42,Insert evidence record into failure_evidence table.
incidents,L6,llm_failure_driver,LLMFailureDriver.insert_failure,"async insert_failure(failure_id: str, run_id: str, tenant_id: str, failure_type: str, error_code: str, error_message: str, model: str, request_id: Optional[str], duration_ms: Optional[int], metadata: Dict[str, Any], created_at: datetime) -> None",?:llm_failure_engine | L5:llm_failure_engine,dumps | execute | text,asyncio | sqlalchemy,db_write,yes,59,Insert failure fact into run_failures table.
incidents,L6,llm_failure_driver,LLMFailureDriver.update_run_failed,"async update_run_failed(run_id: str, tenant_id: str, error: str, completed_at: datetime) -> bool",?:llm_failure_engine | L5:llm_failure_engine,execute | fetchone | text,asyncio | sqlalchemy,db_write,yes,41,Mark run as failed in worker_runs table.
incidents,L6,llm_failure_driver,get_llm_failure_driver,get_llm_failure_driver(session: AsyncSession) -> LLMFailureDriver,?:llm_failure_engine | L5:llm_failure_engine,LLMFailureDriver,asyncio | sqlalchemy,pure,no,3,Factory function to get LLMFailureDriver instance.
incidents,L6,policy_violation_driver,PolicyViolationDriver.__init__,__init__(session: AsyncSession),L5:policy_violation_engine,,asyncio | psycopg2 | sqlalchemy,pure,no,3,Initialize with async database session.
incidents,L6,policy_violation_driver,PolicyViolationDriver.fetch_incident_by_violation,"async fetch_incident_by_violation(run_id: str, policy_id: str, tenant_id: str) -> Optional[str]",L5:policy_violation_engine,execute | scalar_one_or_none | text,asyncio | psycopg2 | sqlalchemy,db_write,yes,33,Check for existing incident by violation pattern.
incidents,L6,policy_violation_driver,PolicyViolationDriver.fetch_policy_enabled,"async fetch_policy_enabled(tenant_id: str, policy_id: str) -> bool",L5:policy_violation_engine,execute | scalar_one_or_none | text,asyncio | psycopg2 | sqlalchemy,db_write,yes,25,Check if policy is active for tenant.
incidents,L6,policy_violation_driver,PolicyViolationDriver.fetch_violation_exists,async fetch_violation_exists(violation_id: str) -> bool,L5:policy_violation_engine,execute | scalar_one_or_none | text,asyncio | psycopg2 | sqlalchemy,db_write,yes,15,Check if a violation fact has been persisted.
incidents,L6,policy_violation_driver,PolicyViolationDriver.fetch_violation_truth_check,"async fetch_violation_truth_check(run_id: str, tenant_id: str, policy_id: str) -> Dict[str, Any]",L5:policy_violation_engine,execute | fetchone | scalar | text,asyncio | psycopg2 | sqlalchemy,db_write,yes,90,Fetch all data needed for violation truth verification.
incidents,L6,policy_violation_driver,PolicyViolationDriver.insert_evidence_event,"async insert_evidence_event(evidence_id: str, incident_id: str, violation_id: str, evidence: Dict[str, Any]) -> None",L5:policy_violation_engine,dumps | execute | text,asyncio | psycopg2 | sqlalchemy,db_write,yes,39,Insert evidence capture event.
incidents,L6,policy_violation_driver,PolicyViolationDriver.insert_policy_evaluation,"async insert_policy_evaluation(evaluation_id: str, run_id: str, tenant_id: str, outcome: str, policies_checked: int, confidence: float, created_at: datetime, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> None",L5:policy_violation_engine,execute | replace | text,asyncio | psycopg2 | sqlalchemy,db_write,yes,57,Insert policy evaluation record.
incidents,L6,policy_violation_driver,PolicyViolationDriver.insert_violation_record,"async insert_violation_record(violation_id: str, policy_id: str, rule_id: str, run_id: str, tenant_id: str, created_at: datetime) -> None",L5:policy_violation_engine,execute | replace | text,asyncio | psycopg2 | sqlalchemy,db_write,yes,45,Insert a violation fact record into prevention_records.
incidents,L6,policy_violation_driver,get_policy_violation_driver,get_policy_violation_driver(session: AsyncSession) -> PolicyViolationDriver,L5:policy_violation_engine,PolicyViolationDriver,asyncio | psycopg2 | sqlalchemy,pure,no,3,Factory function to get PolicyViolationDriver instance.
incidents,L6,policy_violation_driver,insert_policy_evaluation_sync,"insert_policy_evaluation_sync(database_url: str, evaluation_id: str, run_id: str, tenant_id: str, outcome: str, policies_checked: int, confidence: float, created_at: datetime, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[str]",L5:policy_violation_engine,close | commit | connect | cursor | error | execute | fetchone,asyncio | psycopg2 | sqlalchemy,db_write,no,71,Insert policy evaluation record using sync psycopg2 connection.
incidents,L6,postmortem_driver,PostMortemDriver.__init__,__init__(session: AsyncSession),L5:postmortem_engine,,asyncio | sqlalchemy,pure,no,3,Initialize with async database session.
incidents,L6,postmortem_driver,PostMortemDriver.fetch_category_stats,"async fetch_category_stats(tenant_id: str, category: str, baseline_days: int) -> Optional[Dict[str, Any]]",L5:postmortem_engine,dict | execute | first | mappings | text,asyncio | sqlalchemy,db_write,yes,43,Fetch category statistics.
incidents,L6,postmortem_driver,PostMortemDriver.fetch_recurrence_data,"async fetch_recurrence_data(tenant_id: str, category: str, baseline_days: int) -> Dict[str, int]",L5:postmortem_engine,execute | first | mappings | text,asyncio | sqlalchemy,db_write,yes,38,Fetch recurrence rate data.
incidents,L6,postmortem_driver,PostMortemDriver.fetch_resolution_methods,"async fetch_resolution_methods(tenant_id: str, category: str, baseline_days: int, limit: int) -> List[Dict[str, Any]]",L5:postmortem_engine,dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,41,Fetch common resolution methods for a category.
incidents,L6,postmortem_driver,PostMortemDriver.fetch_resolution_summary,"async fetch_resolution_summary(tenant_id: str, incident_id: str) -> Optional[Dict[str, Any]]",L5:postmortem_engine,dict | execute | first | mappings | text,asyncio | sqlalchemy,db_write,yes,44,Fetch resolution summary for an incident.
incidents,L6,postmortem_driver,PostMortemDriver.fetch_similar_incidents,"async fetch_similar_incidents(tenant_id: str, exclude_incident_id: str, category: str, limit: int) -> List[Dict[str, Any]]",L5:postmortem_engine,dict | execute | mappings | text,asyncio | sqlalchemy,db_write,yes,51,Fetch similar resolved incidents.
incidents,L6,postmortem_driver,get_postmortem_driver,get_postmortem_driver(session: AsyncSession) -> PostMortemDriver,L5:postmortem_engine,PostMortemDriver,asyncio | sqlalchemy,pure,no,3,Factory function to get PostMortemDriver instance.
incidents,L6,recurrence_analysis_driver,RecurrenceAnalysisDriver.__init__,__init__(session: AsyncSession),?:incidents_facade | ?:__init__ | L5:recurrence_analysis_engine,,asyncio | sqlalchemy,pure,no,2,
incidents,L6,recurrence_analysis_driver,RecurrenceAnalysisDriver.fetch_recurrence_for_category,"async fetch_recurrence_for_category(tenant_id: str, category: str, baseline_days: int) -> Optional[RecurrenceGroupSnapshot]",?:incidents_facade | ?:__init__ | L5:recurrence_analysis_engine,RecurrenceGroupSnapshot | execute | first | mappings | max | round | text,asyncio | sqlalchemy,db_write,yes,57,Fetch recurrence details for a specific category.
incidents,L6,recurrence_analysis_driver,RecurrenceAnalysisDriver.fetch_recurrence_groups,"async fetch_recurrence_groups(tenant_id: str, baseline_days: int, recurrence_threshold: int, limit: int) -> list[RecurrenceGroupSnapshot]",?:incidents_facade | ?:__init__ | L5:recurrence_analysis_engine,RecurrenceGroupSnapshot | append | execute | float | mappings | text,asyncio | sqlalchemy,db_write,yes,70,Fetch recurrence groups from database.
integrations,L5,audit_schemas,PolicyActivationAudit.to_dict,to_dict() -> dict,L6:bridges_driver | L5s:__init__ | L5:bridges_engine,isoformat,__future__,pure,no,12,
integrations,L5,bridges_engine,BaseBridge.process,async process(event: LoopEvent) -> LoopEvent,,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,3,Process an event and return the result event.
integrations,L5,bridges_engine,BaseBridge.register,register(dispatcher: 'IntegrationDispatcher') -> None,,info | register_handler,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,4,Register this bridge with the dispatcher.
integrations,L5,bridges_engine,BaseBridge.stage,stage() -> LoopStage,,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,3,The stage this bridge handles.
integrations,L5,bridges_engine,IncidentToCatalogBridge.__init__,__init__(db_session_factory),,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,2,
integrations,L5,bridges_engine,IncidentToCatalogBridge._calculate_fuzzy_confidence,"_calculate_fuzzy_confidence(query_sig: dict, stored_sig: dict) -> float",,get | isinstance | items | loads,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,24,Calculate fuzzy match confidence between signatures.
integrations,L5,bridges_engine,IncidentToCatalogBridge._create_pattern,"async _create_pattern(session, signature: dict, signature_hash: str, incident_id: str, tenant_id: str) -> str",,dumps | execute | get | info | text | uuid4,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,40,Create new failure pattern.
integrations,L5,bridges_engine,IncidentToCatalogBridge._extract_signature,_extract_signature(incident: dict) -> dict,,get | keys | sorted,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,18,Extract normalized failure signature from incident.
integrations,L5,bridges_engine,IncidentToCatalogBridge._find_matching_pattern,"async _find_matching_pattern(signature: dict, signature_hash: str, incident_id: str, tenant_id: str) -> PatternMatchResult",,_calculate_fuzzy_confidence | _create_pattern | _increment_pattern_count | db_factory | error | execute | fetchall | fetchone | from_match | get | isinstance | loads | no_match | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,92,Find matching pattern with confidence scoring.
integrations,L5,bridges_engine,IncidentToCatalogBridge._hash_signature,_hash_signature(signature: dict) -> str,,dumps | encode | hexdigest | sha256,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,4,Create deterministic hash of signature for exact matching.
integrations,L5,bridges_engine,IncidentToCatalogBridge._increment_pattern_count,"async _increment_pattern_count(session, pattern_id: str) -> None",,execute | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,15,Increment pattern occurrence count.
integrations,L5,bridges_engine,IncidentToCatalogBridge.process,async process(event: LoopEvent) -> LoopEvent,,_extract_signature | _find_matching_pattern | _hash_signature | exception | get | info | str | to_dict,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,49,Match incident against known patterns.
integrations,L5,bridges_engine,IncidentToCatalogBridge.stage,stage() -> LoopStage,,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,2,
integrations,L5,bridges_engine,LoopStatusBridge.__init__,"__init__(db_session_factory, redis_client)",,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,3,
integrations,L5,bridges_engine,LoopStatusBridge._build_loop_status,async _build_loop_status(event: LoopEvent) -> LoopStatus,,LoopStatus | append | db_factory | execute | fetchall | get | isinstance | loads | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,41,Build complete loop status from event chain.
integrations,L5,bridges_engine,LoopStatusBridge._push_sse_update,"async _push_sse_update(tenant_id: str, incident_id: str, data: dict) -> None",,dumps | publish,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,18,Push SSE update to connected consoles.
integrations,L5,bridges_engine,LoopStatusBridge.process,async process(event: LoopEvent) -> LoopEvent,,_build_loop_status | _push_sse_update | exception | info | to_console_display | to_dict,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,28,Update console with final loop status.
integrations,L5,bridges_engine,LoopStatusBridge.stage,stage() -> LoopStage,,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,2,
integrations,L5,bridges_engine,PatternToRecoveryBridge.__init__,"__init__(db_session_factory, llm_client)",,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,3,
integrations,L5,bridges_engine,PatternToRecoveryBridge._apply_recovery,async _apply_recovery(suggestion: RecoverySuggestion) -> RecoverySuggestion,,_persist_recovery | info,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,6,Apply recovery immediately.
integrations,L5,bridges_engine,PatternToRecoveryBridge._generate_recovery,"async _generate_recovery(pattern: dict, incident_id: str, confidence_band: ConfidenceBand) -> RecoverySuggestion",,calculate_recovery_confidence | create | get | get_confirmation_level | info | isinstance | loads | should_auto_apply,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,58,Generate recovery suggestion (LLM or heuristics).
integrations,L5,bridges_engine,PatternToRecoveryBridge._instantiate_template,"async _instantiate_template(pattern: dict, incident_id: str, confidence_band: ConfidenceBand) -> RecoverySuggestion",,create | get | isinstance | loads,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,20,Instantiate recovery from pattern template.
integrations,L5,bridges_engine,PatternToRecoveryBridge._load_pattern,async _load_pattern(pattern_id: str) -> Optional[dict],,db_factory | dict | execute | fetchone | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,13,Load pattern from database.
integrations,L5,bridges_engine,PatternToRecoveryBridge._persist_recovery,async _persist_recovery(suggestion: RecoverySuggestion) -> None,,db_factory | dumps | execute | str | text | uuid4,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,39,Persist recovery suggestion to database.
integrations,L5,bridges_engine,PatternToRecoveryBridge._queue_for_review,async _queue_for_review(suggestion: RecoverySuggestion) -> RecoverySuggestion,,_persist_recovery | info,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,6,Queue recovery for human review.
integrations,L5,bridges_engine,PatternToRecoveryBridge.process,async process(event: LoopEvent) -> LoopEvent,,_apply_recovery | _generate_recovery | _instantiate_template | _load_pattern | _queue_for_review | exception | from_confidence | get | info | str | to_dict,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,62,Generate recovery suggestion for matched pattern.
integrations,L5,bridges_engine,PatternToRecoveryBridge.stage,stage() -> LoopStage,,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,2,
integrations,L5,bridges_engine,PolicyToRoutingBridge.__init__,"__init__(db_session_factory, config)",,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,4,
integrations,L5,bridges_engine,PolicyToRoutingBridge._create_adjustment,"async _create_adjustment(agent_id: str, policy: PolicyRule, tenant_id: str) -> Optional[RoutingAdjustment]",,_get_active_adjustments | _get_agent_kpi | _persist_adjustment | create | get | max | min | sum | warning,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,46,Create routing adjustment with guardrails.
integrations,L5,bridges_engine,PolicyToRoutingBridge._get_active_adjustments,async _get_active_adjustments(agent_id: str) -> list[RoutingAdjustment],,RoutingAdjustment | db_factory | execute | fetchall | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,32,Get active adjustments for an agent.
integrations,L5,bridges_engine,PolicyToRoutingBridge._get_agent_kpi,async _get_agent_kpi(agent_id: str) -> float,,db_factory | execute | fetchone | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,23,Get current KPI for an agent (success rate).
integrations,L5,bridges_engine,PolicyToRoutingBridge._identify_affected_agents,"async _identify_affected_agents(policy: PolicyRule, tenant_id: str) -> list[str]",,db_factory | execute | fetchall | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,19,Identify agents affected by this policy.
integrations,L5,bridges_engine,PolicyToRoutingBridge._persist_adjustment,async _persist_adjustment(adjustment: RoutingAdjustment) -> None,,db_factory | execute | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,28,Persist routing adjustment to database.
integrations,L5,bridges_engine,PolicyToRoutingBridge.process,async process(event: LoopEvent) -> LoopEvent,,PolicyMode | PolicyRule | _create_adjustment | _identify_affected_agents | append | exception | from_confidence | get | info | isinstance | len | str | to_dict,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,78,Adjust CARE routing based on new policy.
integrations,L5,bridges_engine,PolicyToRoutingBridge.stage,stage() -> LoopStage,,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,2,
integrations,L5,bridges_engine,RecoveryToPolicyBridge.__init__,"__init__(db_session_factory, config)",,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,3,
integrations,L5,bridges_engine,RecoveryToPolicyBridge._generate_policy,"_generate_policy(pattern: dict, recovery: RecoverySuggestion) -> PolicyRule",,create | get | isinstance | loads,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,36,Generate prevention policy from pattern and recovery.
integrations,L5,bridges_engine,RecoveryToPolicyBridge._load_pattern,async _load_pattern(pattern_id: str) -> Optional[dict],,db_factory | dict | execute | fetchone | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,13,Load pattern from database.
integrations,L5,bridges_engine,RecoveryToPolicyBridge._persist_policy,"async _persist_policy(policy: PolicyRule, tenant_id: str) -> None",,db_factory | dumps | execute | text,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,db_write,yes,45,Persist policy to database.
integrations,L5,bridges_engine,RecoveryToPolicyBridge.process,async process(event: LoopEvent) -> LoopEvent,,RecoverySuggestion | _generate_policy | _load_pattern | _persist_policy | exception | from_confidence | get | getattr | info | isinstance | str | to_dict,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,yes,82,Generate prevention policy from applied recovery.
integrations,L5,bridges_engine,RecoveryToPolicyBridge.stage,stage() -> LoopStage,,,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,2,
integrations,L5,bridges_engine,_check_frozen,_check_frozen() -> None,,info,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,6,Log that frozen mechanics are in use.
integrations,L5,bridges_engine,create_bridges,"create_bridges(db_session_factory, redis_client, config) -> list[BaseBridge]",,IncidentToCatalogBridge | LoopStatusBridge | PatternToRecoveryBridge | PolicyToRoutingBridge | RecoveryToPolicyBridge,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,13,Create all bridges with shared configuration.
integrations,L5,bridges_engine,register_all_bridges,"register_all_bridges(dispatcher: 'IntegrationDispatcher', db_session_factory, redis_client, config) -> None",,create_bridges | register,__future__ | audit_schemas | bridges_driver | dispatcher_engine | loop_events | sqlalchemy,pure,no,10,Register all bridges with the dispatcher.
integrations,L5,channel_engine,NotificationSender.send,"async send(config: NotifyChannelConfig, event_type: NotifyEventType, payload: Dict[str, Any]) -> NotifyDeliveryResult",,,,pure,yes,8,Send notification via this channel.
integrations,L5,channel_engine,NotifyChannelConfig.is_configured,is_configured() -> bool,,bool | len,,pure,no,15,Check if channel has required configuration.
integrations,L5,channel_engine,NotifyChannelConfig.is_event_enabled,is_event_enabled(event_type: NotifyEventType) -> bool,,,,pure,no,3,Check if an event type is enabled for this channel.
integrations,L5,channel_engine,NotifyChannelConfig.record_failure,record_failure() -> None,,now,,pure,no,5,Record a failed delivery.
integrations,L5,channel_engine,NotifyChannelConfig.record_success,record_success() -> None,,now,,pure,no,4,Record a successful delivery.
integrations,L5,channel_engine,NotifyChannelConfig.to_dict,"to_dict() -> Dict[str, Any]",,is_configured | isoformat,,pure,no,20,Convert to dictionary for API responses.
integrations,L5,channel_engine,NotifyChannelConfigResponse.to_dict,"to_dict() -> Dict[str, Any]",,,,pure,no,9,Convert to dictionary for API responses.
integrations,L5,channel_engine,NotifyChannelError.__init__,"__init__(message: str, channel: NotifyChannel, event_type: Optional[NotifyEventType], details: Optional[Dict[str, Any]])",,__init__ | super,,pure,no,11,
integrations,L5,channel_engine,NotifyChannelError.to_dict,"to_dict() -> Dict[str, Any]",,str,,pure,no,9,Convert to dictionary for logging/API responses.
integrations,L5,channel_engine,NotifyChannelService.__init__,__init__(default_channels: Optional[Set[NotifyChannel]]),,,,pure,no,13,Initialize the notification channel service.
integrations,L5,channel_engine,NotifyChannelService._send_email_notification,"async _send_email_notification(config: NotifyChannelConfig, event_type: NotifyEventType, payload: Dict[str, Any], start_time: datetime) -> NotifyDeliveryResult",,NotifyDeliveryResult | encode | hexdigest | info | int | isoformat | join | now | sha256 | total_seconds,,pure,yes,44,Send email notification.
integrations,L5,channel_engine,NotifyChannelService._send_pagerduty_notification,"async _send_pagerduty_notification(config: NotifyChannelConfig, event_type: NotifyEventType, payload: Dict[str, Any], start_time: datetime) -> NotifyDeliveryResult",,NotifyDeliveryResult | encode | hexdigest | info | int | isoformat | now | sha256 | total_seconds,,pure,yes,42,Send PagerDuty notification.
integrations,L5,channel_engine,NotifyChannelService._send_slack_notification,"async _send_slack_notification(config: NotifyChannelConfig, event_type: NotifyEventType, payload: Dict[str, Any], start_time: datetime) -> NotifyDeliveryResult",,NotifyDeliveryResult | encode | hexdigest | info | int | isoformat | now | sha256 | total_seconds,,pure,yes,43,Send Slack notification.
integrations,L5,channel_engine,NotifyChannelService._send_teams_notification,"async _send_teams_notification(config: NotifyChannelConfig, event_type: NotifyEventType, payload: Dict[str, Any], start_time: datetime) -> NotifyDeliveryResult",,NotifyDeliveryResult | encode | hexdigest | info | int | isoformat | now | sha256 | total_seconds,,pure,yes,42,Send Microsoft Teams notification.
integrations,L5,channel_engine,NotifyChannelService._send_ui_notification,"async _send_ui_notification(tenant_id: str, event_type: NotifyEventType, payload: Dict[str, Any], start_time: datetime) -> NotifyDeliveryResult",,NotifyDeliveryResult | encode | hexdigest | info | int | isoformat | now | sha256 | total_seconds,,pure,yes,34,Send UI notification.
integrations,L5,channel_engine,NotifyChannelService._send_via_channel,"async _send_via_channel(config: NotifyChannelConfig, event_type: NotifyEventType, payload: Dict[str, Any], start_time: datetime) -> NotifyDeliveryResult",,NotifyDeliveryResult | _send_email_notification | _send_pagerduty_notification | _send_slack_notification | _send_teams_notification | _send_ui_notification | _send_webhook_notification | now,,pure,yes,42,Send notification via a specific channel.
integrations,L5,channel_engine,NotifyChannelService._send_webhook_notification,"async _send_webhook_notification(config: NotifyChannelConfig, event_type: NotifyEventType, payload: Dict[str, Any], start_time: datetime) -> NotifyDeliveryResult",,NotifyDeliveryResult | encode | hexdigest | info | int | isoformat | now | sha256 | total_seconds,,pure,yes,45,Send webhook notification.
integrations,L5,channel_engine,NotifyChannelService.check_health,"async check_health(tenant_id: str) -> Dict[NotifyChannel, Dict[str, Any]]",,get_channel_config | is_configured | isoformat,,pure,yes,41,Check health of all configured channels for a tenant.
integrations,L5,channel_engine,NotifyChannelService.configure_channel,"configure_channel(tenant_id: str, channel: NotifyChannel, status: NotifyChannelStatus, webhook_url: Optional[str], webhook_secret: Optional[str], email_recipients: Optional[List[str]], slack_webhook_url: Optional[str], slack_channel: Optional[str], pagerduty_routing_key: Optional[str], teams_webhook_url: Optional[str], enabled_events: Optional[Set[NotifyEventType]], retry_count: int, timeout_seconds: int) -> NotifyChannelConfig",,NotifyChannelConfig | info | is_configured | set,,pure,no,73,Configure a notification channel for a tenant.
integrations,L5,channel_engine,NotifyChannelService.disable_channel,"disable_channel(tenant_id: str, channel: NotifyChannel) -> NotifyChannelConfigResponse",,NotifyChannelConfigResponse | get_channel_config | info | is_configured | list | now,,pure,no,41,Disable a notification channel.
integrations,L5,channel_engine,NotifyChannelService.enable_channel,"enable_channel(tenant_id: str, channel: NotifyChannel) -> NotifyChannelConfigResponse",,NotifyChannelConfigResponse | get_channel_config | info | is_configured | list | now,,pure,no,50,Enable a notification channel.
integrations,L5,channel_engine,NotifyChannelService.get_all_configs,"get_all_configs(tenant_id: str) -> Dict[NotifyChannel, NotifyChannelConfig]",,get,,pure,no,14,Get all channel configurations for a tenant.
integrations,L5,channel_engine,NotifyChannelService.get_channel_config,"get_channel_config(tenant_id: str, channel: NotifyChannel) -> Optional[NotifyChannelConfig]",,get,,pure,no,18,Get configuration for a specific channel.
integrations,L5,channel_engine,NotifyChannelService.get_delivery_history,"get_delivery_history(tenant_id: str, limit: int) -> List[NotifyDeliveryResult]",,,,pure,no,18,Get recent delivery history for a tenant.
integrations,L5,channel_engine,NotifyChannelService.get_enabled_channels,"get_enabled_channels(tenant_id: str, event_type: Optional[NotifyEventType]) -> List[NotifyChannel]",,append | is_configured | is_event_enabled | items | list,,pure,no,29,Get list of enabled channels for a tenant.
integrations,L5,channel_engine,NotifyChannelService.send,"async send(tenant_id: str, event_type: NotifyEventType, payload: Dict[str, Any], channels: Optional[List[NotifyChannel]]) -> List[NotifyDeliveryResult]",,NotifyDeliveryResult | _send_ui_notification | _send_via_channel | append | extend | get_channel_config | get_enabled_channels | is_event_enabled | len | now | record_failure | record_success | str,,pure,yes,80,Send notification via all enabled channels.
integrations,L5,channel_engine,NotifyChannelService.set_event_filter,"set_event_filter(tenant_id: str, channel: NotifyChannel, enabled_events: Set[NotifyEventType]) -> NotifyChannelConfigResponse",,NotifyChannelConfigResponse | get_channel_config | info | is_configured | list | now,,pure,no,47,Set which events trigger notifications for a channel.
integrations,L5,channel_engine,NotifyDeliveryResult.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,,pure,no,13,Convert to dictionary for API responses.
integrations,L5,channel_engine,_reset_notify_service,_reset_notify_service() -> None,,,,pure,no,4,Reset the notification service (for testing).
integrations,L5,channel_engine,check_channel_health,"async check_channel_health(tenant_id: str) -> Dict[NotifyChannel, Dict[str, Any]]",,check_health | get_notify_service,,pure,yes,14,Quick helper to check channel health.
integrations,L5,channel_engine,get_channel_config,"get_channel_config(tenant_id: str, channel: NotifyChannel) -> Optional[NotifyChannelConfig]",,get_channel_config | get_notify_service,,pure,no,16,Quick helper to get channel configuration.
integrations,L5,channel_engine,get_notify_service,get_notify_service() -> NotifyChannelService,,NotifyChannelService,,pure,no,6,Get or create the notification service singleton.
integrations,L5,channel_engine,send_notification,"async send_notification(tenant_id: str, event_type: NotifyEventType, payload: Dict[str, Any], channels: Optional[List[NotifyChannel]]) -> List[NotifyDeliveryResult]",,get_notify_service | send,,pure,yes,20,Quick helper to send notification.
integrations,L5,connectors_facade,ConnectorInfo.to_dict,"to_dict() -> Dict[str, Any]",L4:integrations_handler,isoformat,connector_registry_driver,pure,no,16,Convert to dictionary.
integrations,L5,connectors_facade,ConnectorsFacade.__init__,__init__(),L4:integrations_handler,,connector_registry_driver,pure,no,9,Initialize facade with lazy-loaded services.
integrations,L5,connectors_facade,ConnectorsFacade._get_capabilities_for_type,_get_capabilities_for_type(connector_type: str) -> List[str],L4:integrations_handler,get,connector_registry_driver,pure,no,12,Get default capabilities for connector type.
integrations,L5,connectors_facade,ConnectorsFacade.delete_connector,"async delete_connector(connector_id: str, tenant_id: str) -> bool",L4:integrations_handler,get | info,connector_registry_driver,pure,yes,25,Delete a connector.
integrations,L5,connectors_facade,ConnectorsFacade.get_connector,"async get_connector(connector_id: str, tenant_id: str) -> Optional[ConnectorInfo]",L4:integrations_handler,get,connector_registry_driver,pure,yes,19,Get a specific connector.
integrations,L5,connectors_facade,ConnectorsFacade.list_connectors,"async list_connectors(tenant_id: str, connector_type: Optional[str], status: Optional[str], limit: int, offset: int) -> List[ConnectorInfo]",L4:integrations_handler,append | debug | values,connector_registry_driver,pure,yes,39,List connectors for a tenant.
integrations,L5,connectors_facade,ConnectorsFacade.register_connector,"async register_connector(tenant_id: str, name: str, connector_type: str, endpoint: Optional[str], config: Optional[Dict[str, Any]], metadata: Optional[Dict[str, Any]]) -> ConnectorInfo",L4:integrations_handler,ConnectorInfo | _get_capabilities_for_type | info | now | str | uuid4,connector_registry_driver,pure,yes,53,Register a new connector.
integrations,L5,connectors_facade,ConnectorsFacade.registry,registry(),L4:integrations_handler,ConnectorRegistry | warning,connector_registry_driver,pure,no,9,Lazy-load ConnectorRegistry.
integrations,L5,connectors_facade,ConnectorsFacade.test_connector,"async test_connector(connector_id: str, tenant_id: str) -> TestResult",L4:integrations_handler,TestResult | get | int | now | str | time,connector_registry_driver,pure,yes,63,Test a connector connection.
integrations,L5,connectors_facade,ConnectorsFacade.update_connector,"async update_connector(connector_id: str, tenant_id: str, name: Optional[str], endpoint: Optional[str], config: Optional[Dict[str, Any]], metadata: Optional[Dict[str, Any]]) -> Optional[ConnectorInfo]",L4:integrations_handler,get | now,connector_registry_driver,pure,yes,38,Update a connector.
integrations,L5,connectors_facade,TestResult.to_dict,"to_dict() -> Dict[str, Any]",L4:integrations_handler,,connector_registry_driver,pure,no,9,Convert to dictionary.
integrations,L5,connectors_facade,get_connectors_facade,get_connectors_facade() -> ConnectorsFacade,L4:integrations_handler,ConnectorsFacade,connector_registry_driver,pure,no,14,Get the connectors facade instance.
integrations,L5,cost_bridges_engine,CostAnomaly.create,"create(tenant_id: str, anomaly_type: AnomalyType, entity_type: str, entity_id: str, current_value_cents: int, expected_value_cents: int, metadata: dict[str, Any] | None) -> 'CostAnomaly'",L5:__init__,cls | get | now | uuid4,__future__ | loop_events | orchestrator,pure,no,51,Factory method for creating cost anomalies.
integrations,L5,cost_bridges_engine,CostAnomaly.to_dict,"to_dict() -> dict[str, Any]",L5:__init__,isoformat,__future__ | loop_events | orchestrator,pure,no,16,Serialize for JSON.
integrations,L5,cost_bridges_engine,CostEstimationProbe.__init__,__init__(db_session),L5:__init__,,__future__ | loop_events | orchestrator,pure,no,2,
integrations,L5,cost_bridges_engine,CostEstimationProbe._calculate_cost,"_calculate_cost(model: str, prompt_tokens: int, output_tokens: int) -> int",L5:__init__,get | int,__future__ | loop_events | orchestrator,pure,no,9,Calculate cost in cents.
integrations,L5,cost_bridges_engine,CostEstimationProbe._find_cheaper_model,"_find_cheaper_model(current_model: str, prompt_tokens: int, output_tokens: int) -> Optional[dict[str, Any]]",L5:__init__,_calculate_cost,__future__ | loop_events | orchestrator,pure,no,25,Find a cheaper model that can handle the request.
integrations,L5,cost_bridges_engine,CostEstimationProbe.probe,"async probe(model: str, prompt_tokens: int, expected_output_tokens: int, tenant_id: str, cost_threshold_cents: int) -> dict[str, Any]",L5:__init__,_calculate_cost | _find_cheaper_model,__future__ | loop_events | orchestrator,pure,yes,48,Estimate cost and return routing decision.
integrations,L5,cost_bridges_engine,CostLoopBridge.__init__,__init__(db_session),L5:__init__,ValueError,__future__ | loop_events | orchestrator,pure,no,10,Initialize with database session.
integrations,L5,cost_bridges_engine,CostLoopBridge._map_severity_to_incident_severity,_map_severity_to_incident_severity(severity: AnomalySeverity) -> str,L5:__init__,get,__future__ | loop_events | orchestrator,pure,no,9,Map cost anomaly severity to incident severity.
integrations,L5,cost_bridges_engine,CostLoopBridge.on_anomaly_detected,on_anomaly_detected(anomaly: CostAnomaly) -> Optional[str],L5:__init__,create_incident_from_cost_anomaly_sync | info,__future__ | loop_events | orchestrator,pure,no,34,Create incident from cost anomaly if severity warrants.
integrations,L5,cost_bridges_engine,CostLoopOrchestrator.__init__,__init__(db_session),L5:__init__,CostLoopBridge | CostPatternMatcher | CostPolicyGenerator | CostRecoveryGenerator | CostRoutingAdjuster | ValueError,__future__ | loop_events | orchestrator,pure,no,17,Initialize orchestrator with database session.
integrations,L5,cost_bridges_engine,CostLoopOrchestrator.process_anomaly,"async process_anomaly(anomaly: CostAnomaly) -> dict[str, Any]",L5:__init__,append | generate_policy | generate_recovery | len | match_cost_pattern | max | on_anomaly_detected | on_cost_policy_created | to_dict,__future__ | loop_events | orchestrator,pure,yes,52,Process a cost anomaly through the full loop.
integrations,L5,cost_bridges_engine,CostPatternMatcher.__init__,__init__(db_session),L5:__init__,,__future__ | loop_events | orchestrator,pure,no,2,
integrations,L5,cost_bridges_engine,CostPatternMatcher._build_signature,"_build_signature(anomaly: CostAnomaly) -> dict[str, Any]",L5:__init__,_deviation_bucket,__future__ | loop_events | orchestrator,pure,no,8,Build pattern signature from anomaly.
integrations,L5,cost_bridges_engine,CostPatternMatcher._calculate_confidence,"_calculate_confidence(anomaly: CostAnomaly, pattern_name: str) -> float",L5:__init__,get,__future__ | loop_events | orchestrator,pure,no,15,Calculate match confidence based on pattern fit.
integrations,L5,cost_bridges_engine,CostPatternMatcher._deviation_bucket,_deviation_bucket(pct: float) -> str,L5:__init__,,__future__ | loop_events | orchestrator,pure,no,10,Bucket deviation percentages for pattern matching.
integrations,L5,cost_bridges_engine,CostPatternMatcher._find_predefined_match,_find_predefined_match(anomaly: CostAnomaly) -> Optional[str],L5:__init__,items,__future__ | loop_events | orchestrator,pure,no,10,Find matching pre-defined pattern.
integrations,L5,cost_bridges_engine,CostPatternMatcher._hash_signature,"_hash_signature(signature: dict[str, Any]) -> str",L5:__init__,dumps | encode | hexdigest | sha256,__future__ | loop_events | orchestrator,pure,no,7,Create hash of signature for matching.
integrations,L5,cost_bridges_engine,CostPatternMatcher.match_cost_pattern,"async match_cost_pattern(anomaly: CostAnomaly, incident_id: str) -> PatternMatchResult",L5:__init__,_build_signature | _calculate_confidence | _find_predefined_match | _hash_signature | from_match | uuid4,__future__ | loop_events | orchestrator,pure,yes,47,Match anomaly to existing or new cost pattern.
integrations,L5,cost_bridges_engine,CostPolicyGenerator.__init__,__init__(db_session),L5:__init__,,__future__ | loop_events | orchestrator,pure,no,2,
integrations,L5,cost_bridges_engine,CostPolicyGenerator.generate_policy,"async generate_policy(recovery: RecoverySuggestion, anomaly: CostAnomaly, pattern_result: PatternMatchResult) -> Optional[PolicyRule]",L5:__init__,create | format | get | info | isoformat | now | timedelta | warning,__future__ | loop_events | orchestrator,pure,yes,61,Generate policy from applied recovery.
integrations,L5,cost_bridges_engine,CostRecoveryGenerator.__init__,__init__(db_session),L5:__init__,,__future__ | loop_events | orchestrator,pure,no,2,
integrations,L5,cost_bridges_engine,CostRecoveryGenerator.generate_recovery,"async generate_recovery(anomaly: CostAnomaly, pattern_result: PatternMatchResult, incident_id: str) -> list[RecoverySuggestion]",L5:__init__,append | create | get | info,__future__ | loop_events | orchestrator,pure,yes,46,Generate recovery suggestions for cost incident.
integrations,L5,cost_bridges_engine,CostRoutingAdjuster.__init__,__init__(db_session),L5:__init__,,__future__ | loop_events | orchestrator,pure,no,2,
integrations,L5,cost_bridges_engine,CostRoutingAdjuster._create_budget_block_adjustment,_create_budget_block_adjustment(policy: PolicyRule) -> RoutingAdjustment,L5:__init__,create,__future__ | loop_events | orchestrator,pure,no,11,Create budget blocking adjustment (highest priority).
integrations,L5,cost_bridges_engine,CostRoutingAdjuster._create_escalation_adjustment,_create_escalation_adjustment(policy: PolicyRule) -> RoutingAdjustment,L5:__init__,create,__future__ | loop_events | orchestrator,pure,no,11,Create escalation adjustment (pause + notify admin).
integrations,L5,cost_bridges_engine,CostRoutingAdjuster._create_model_routing_adjustment,_create_model_routing_adjustment(policy: PolicyRule) -> RoutingAdjustment,L5:__init__,create,__future__ | loop_events | orchestrator,pure,no,12,Create cost-based model routing adjustment.
integrations,L5,cost_bridges_engine,CostRoutingAdjuster._create_notify_adjustment,_create_notify_adjustment(policy: PolicyRule) -> RoutingAdjustment,L5:__init__,create,__future__ | loop_events | orchestrator,pure,no,11,Create notification adjustment (minimal routing impact).
integrations,L5,cost_bridges_engine,CostRoutingAdjuster._create_rate_limit_adjustment,_create_rate_limit_adjustment(policy: PolicyRule) -> RoutingAdjustment,L5:__init__,create,__future__ | loop_events | orchestrator,pure,no,11,Create rate limiting adjustment.
integrations,L5,cost_bridges_engine,CostRoutingAdjuster._create_review_adjustment,_create_review_adjustment(policy: PolicyRule) -> RoutingAdjustment,L5:__init__,create,__future__ | loop_events | orchestrator,pure,no,11,Create review flag adjustment (minimal routing impact).
integrations,L5,cost_bridges_engine,CostRoutingAdjuster._create_throttle_adjustment,_create_throttle_adjustment(policy: PolicyRule) -> RoutingAdjustment,L5:__init__,create,__future__ | loop_events | orchestrator,pure,no,11,Create throttling adjustment.
integrations,L5,cost_bridges_engine,CostRoutingAdjuster._create_token_limit_adjustment,_create_token_limit_adjustment(policy: PolicyRule) -> RoutingAdjustment,L5:__init__,create,__future__ | loop_events | orchestrator,pure,no,11,Create token limiting adjustment.
integrations,L5,cost_bridges_engine,CostRoutingAdjuster.on_cost_policy_created,async on_cost_policy_created(policy: PolicyRule) -> list[RoutingAdjustment],L5:__init__,_create_budget_block_adjustment | _create_escalation_adjustment | _create_model_routing_adjustment | _create_notify_adjustment | _create_rate_limit_adjustment | _create_review_adjustment | _create_throttle_adjustment | _create_token_limit_adjustment | append | info,__future__ | loop_events | orchestrator,pure,yes,47,Adjust CARE routing based on new cost policy.
integrations,L5,cus_health_engine,CusHealthService.__init__,__init__(credential_service: Optional[CusCredentialService]),?:cus_health_service,CusCredentialService,cus_credential_service | cus_models | db | httpx | sqlmodel,pure,no,8,Initialize health service.
integrations,L5,cus_health_engine,CusHealthService._calculate_overall_health,"_calculate_overall_health(counts: Dict[str, int]) -> str",?:cus_health_service,,cus_credential_service | cus_models | db | httpx | sqlmodel,pure,no,22,Calculate overall health status from counts.
integrations,L5,cus_health_engine,CusHealthService._perform_health_check,"async _perform_health_check(integration: CusIntegration, tenant_id: str) -> Dict[str, Any]",?:cus_health_service,AsyncClient | Timeout | exception | get | int | lower | now | post | resolve_credential | str | total_seconds | update | warning,cus_credential_service | cus_models | db | httpx | sqlmodel,external_api,yes,183,Perform the actual health check call.
integrations,L5,cus_health_engine,CusHealthService.check_all_integrations,"async check_all_integrations(tenant_id: str, stale_threshold_minutes: int) -> List[Dict[str, Any]]",?:cus_health_service,Session | UUID | all | append | check_health | exec | get_engine | info | is_ | len | list | now | select | sleep | str,cus_credential_service | cus_models | db | httpx | sqlmodel,pure,yes,61,Check health of all integrations that need checking.
integrations,L5,cus_health_engine,CusHealthService.check_health,"async check_health(tenant_id: str, integration_id: str, force: bool) -> Dict[str, Any]",?:cus_health_service,Session | UUID | _perform_health_check | add | exec | first | get_engine | info | now | select | total_seconds | where,cus_credential_service | cus_models | db | httpx | sqlmodel,db_write,yes,72,Check health of a single integration.
integrations,L5,cus_health_engine,CusHealthService.get_health_summary,"async get_health_summary(tenant_id: str) -> Dict[str, Any]",?:cus_health_service,Session | UUID | _calculate_overall_health | all | exec | get_engine | len | list | lower | now | select | timedelta | where,cus_credential_service | cus_models | db | httpx | sqlmodel,pure,yes,51,Get health summary for all integrations.
integrations,L5,cus_integration_engine,get_cus_integration_engine,get_cus_integration_engine() -> CusIntegrationEngine,?:integrations_facade | ?:cus_integration_service | L5:integrations_facade,CusIntegrationEngine,,pure,no,6,Stub factory.
integrations,L5,cus_integration_engine,get_cus_integration_service,get_cus_integration_service() -> CusIntegrationService,?:integrations_facade | ?:cus_integration_service | L5:integrations_facade,get_cus_integration_engine,,pure,no,7,Get the CusIntegrationService instance.
integrations,L5,cus_schemas,CusIntegrationCreate.validate_not_raw_key,validate_not_raw_key(v: str) -> str,?:cus_telemetry | ?:aos_cus_integrations | ?:cus_telemetry_engine | ?:cus_integration_engine | L2:aos_cus_integrations | L2:cus_telemetry,ValueError | field_validator | startswith,cus_models | pydantic,pure,no,14,Ensure credential_ref is not a raw API key.
integrations,L5,cus_schemas,CusIntegrationUpdate.validate_not_raw_key,validate_not_raw_key(v: Optional[str]) -> Optional[str],?:cus_telemetry | ?:aos_cus_integrations | ?:cus_telemetry_engine | ?:cus_integration_engine | L2:aos_cus_integrations | L2:cus_telemetry,ValueError | field_validator | startswith,cus_models | pydantic,pure,no,12,Ensure credential_ref is not a raw API key.
integrations,L5,datasource_model,CustomerDataSource.activate,activate(now: Optional[datetime]) -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,now,,pure,no,6,Activate the data source.
integrations,L5,datasource_model,CustomerDataSource.add_tag,add_tag(tag: str) -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,append | now,,pure,no,5,Add a tag to the data source.
integrations,L5,datasource_model,CustomerDataSource.deactivate,deactivate(now: Optional[datetime]) -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,now,,pure,no,5,Deactivate the data source.
integrations,L5,datasource_model,CustomerDataSource.deprecate,deprecate(now: Optional[datetime]) -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,now,,pure,no,5,Mark data source as deprecated.
integrations,L5,datasource_model,CustomerDataSource.grant_access,grant_access(role: str) -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,append | now,,pure,no,5,Grant access to a role.
integrations,L5,datasource_model,CustomerDataSource.has_access,has_access(role: str) -> bool,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,len,,pure,no,3,Check if a role has access.
integrations,L5,datasource_model,CustomerDataSource.record_connection,record_connection(now: Optional[datetime]) -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,now,,pure,no,6,Record a successful connection.
integrations,L5,datasource_model,CustomerDataSource.record_error,"record_error(error: str, now: Optional[datetime]) -> None",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,now,,pure,no,7,Record a connection error.
integrations,L5,datasource_model,CustomerDataSource.remove_tag,remove_tag(tag: str) -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,now | remove,,pure,no,5,Remove a tag from the data source.
integrations,L5,datasource_model,CustomerDataSource.revoke_access,revoke_access(role: str) -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,now | remove,,pure,no,5,Revoke access from a role.
integrations,L5,datasource_model,CustomerDataSource.to_dict,"to_dict(include_secrets: bool) -> dict[str, Any]",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,isoformat | to_dict,,pure,no,23,Convert to dictionary.
integrations,L5,datasource_model,CustomerDataSource.update_config,"update_config(config: DataSourceConfig, now: Optional[datetime]) -> None",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,now,,pure,no,10,Update the data source configuration.
integrations,L5,datasource_model,DataSourceConfig.get_connection_url,get_connection_url() -> Optional[str],?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,,,pure,no,20,Build connection URL from components.
integrations,L5,datasource_model,DataSourceConfig.to_dict,"to_dict(include_secrets: bool) -> dict[str, Any]",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,,,pure,no,27,"Convert to dictionary, optionally masking secrets."
integrations,L5,datasource_model,DataSourceError.__init__,"__init__(message: str, source_id: Optional[str], source_type: Optional[DataSourceType])",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,__init__ | super,,pure,no,10,
integrations,L5,datasource_model,DataSourceError.to_dict,"to_dict() -> dict[str, Any]",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,,,pure,no,7,Convert to dictionary.
integrations,L5,datasource_model,DataSourceRegistry.__init__,__init__(),?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,,,pure,no,4,Initialize the registry.
integrations,L5,datasource_model,DataSourceRegistry.activate,activate(source_id: str) -> Optional[CustomerDataSource],?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,activate | get,,pure,no,6,Activate a data source.
integrations,L5,datasource_model,DataSourceRegistry.clear_tenant,clear_tenant(tenant_id: str) -> int,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,delete | get | len | list | set,,db_write,no,6,Clear all sources for a tenant.
integrations,L5,datasource_model,DataSourceRegistry.deactivate,deactivate(source_id: str) -> Optional[CustomerDataSource],?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,deactivate | get,,pure,no,6,Deactivate a data source.
integrations,L5,datasource_model,DataSourceRegistry.delete,delete(source_id: str) -> bool,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,discard | get,,pure,no,13,Delete a data source.
integrations,L5,datasource_model,DataSourceRegistry.get,get(source_id: str) -> Optional[CustomerDataSource],?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,get,,pure,no,3,Get a data source by ID.
integrations,L5,datasource_model,DataSourceRegistry.get_by_name,"get_by_name(tenant_id: str, name: str) -> Optional[CustomerDataSource]",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,values,,pure,no,10,Get a data source by name within a tenant.
integrations,L5,datasource_model,DataSourceRegistry.get_statistics,get_statistics(tenant_id: Optional[str]) -> DataSourceStats,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,DataSourceStats | get | values,,pure,no,32,Get registry statistics.
integrations,L5,datasource_model,DataSourceRegistry.list,"list(tenant_id: Optional[str], source_type: Optional[DataSourceType], status: Optional[DataSourceStatus], tag: Optional[str], limit: int, offset: int) -> list[CustomerDataSource]",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,list | sort | values,,pure,no,41,List data sources with optional filters.
integrations,L5,datasource_model,DataSourceRegistry.register,"register(tenant_id: str, name: str, source_type: DataSourceType, config: Optional[DataSourceConfig], description: Optional[str], tags: Optional[list[str]], owner_id: Optional[str], source_id: Optional[str]) -> CustomerDataSource",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,CustomerDataSource | DataSourceConfig | add | set | str | uuid4,,db_write,no,49,Register a new data source.
integrations,L5,datasource_model,DataSourceRegistry.reset,reset() -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,clear,,pure,no,4,Reset all state (for testing).
integrations,L5,datasource_model,DataSourceRegistry.update,"update(source_id: str, name: Optional[str], description: Optional[str], config: Optional[DataSourceConfig], metadata: Optional[dict[str, Any]]) -> Optional[CustomerDataSource]",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,get | now | update_config,,pure,no,24,Update a data source.
integrations,L5,datasource_model,DataSourceStats.to_dict,"to_dict() -> dict[str, Any]",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,,,pure,no,12,Convert to dictionary.
integrations,L5,datasource_model,_reset_registry,_reset_registry() -> None,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,reset,,pure,no,6,Reset the singleton (for testing).
integrations,L5,datasource_model,create_datasource,"create_datasource(tenant_id: str, name: str, source_type: DataSourceType, config: Optional[DataSourceConfig]) -> CustomerDataSource",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,get_datasource_registry | register,,pure,no,14,Create a new data source using the singleton registry.
integrations,L5,datasource_model,get_datasource,get_datasource(source_id: str) -> Optional[CustomerDataSource],?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,get | get_datasource_registry,,pure,no,4,Get a data source by ID using the singleton registry.
integrations,L5,datasource_model,get_datasource_registry,get_datasource_registry() -> DataSourceRegistry,?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,DataSourceRegistry,,pure,no,6,Get the singleton registry instance.
integrations,L5,datasource_model,list_datasources,"list_datasources(tenant_id: Optional[str], source_type: Optional[DataSourceType]) -> list[CustomerDataSource]",?:facade | ?:__init__ | L5:datasources_facade | ?:test_customer_datasource,get_datasource_registry | list,,pure,no,7,List data sources using the singleton registry.
integrations,L5,datasources_facade,DataSourcesFacade.__init__,__init__(),L4:integrations_handler,,datasource_model,pure,no,3,Initialize facade.
integrations,L5,datasources_facade,DataSourcesFacade.activate_source,"async activate_source(source_id: str, tenant_id: str) -> Optional[CustomerDataSource]",L4:integrations_handler,activate | get | info,datasource_model,pure,yes,21,Activate a data source.
integrations,L5,datasources_facade,DataSourcesFacade.deactivate_source,"async deactivate_source(source_id: str, tenant_id: str) -> Optional[CustomerDataSource]",L4:integrations_handler,deactivate | get | info,datasource_model,pure,yes,21,Deactivate a data source.
integrations,L5,datasources_facade,DataSourcesFacade.delete_source,"async delete_source(source_id: str, tenant_id: str) -> bool",L4:integrations_handler,delete | get | info,datasource_model,db_write,yes,21,Delete a data source.
integrations,L5,datasources_facade,DataSourcesFacade.get_source,"async get_source(source_id: str, tenant_id: str) -> Optional[CustomerDataSource]",L4:integrations_handler,get,datasource_model,pure,yes,19,Get a specific data source.
integrations,L5,datasources_facade,DataSourcesFacade.get_statistics,async get_statistics(tenant_id: str) -> DataSourceStats,L4:integrations_handler,get_statistics,datasource_model,pure,yes,14,Get data source statistics for a tenant.
integrations,L5,datasources_facade,DataSourcesFacade.list_sources,"async list_sources(tenant_id: str, source_type: Optional[str], status: Optional[str], tag: Optional[str], limit: int, offset: int) -> List[CustomerDataSource]",L4:integrations_handler,DataSourceStatus | DataSourceType | list,datasource_model,pure,yes,46,List data sources.
integrations,L5,datasources_facade,DataSourcesFacade.register_source,"async register_source(tenant_id: str, name: str, source_type: str, config: Optional[Dict[str, Any]], description: Optional[str], tags: Optional[List[str]], owner_id: Optional[str], metadata: Optional[Dict[str, Any]]) -> CustomerDataSource",L4:integrations_handler,DataSourceConfig | DataSourceType | get | info | register,datasource_model,pure,yes,68,Register a new data source.
integrations,L5,datasources_facade,DataSourcesFacade.registry,registry() -> DataSourceRegistry,L4:integrations_handler,get_datasource_registry,datasource_model,pure,no,5,Lazy load the registry.
integrations,L5,datasources_facade,DataSourcesFacade.test_connection,"async test_connection(source_id: str, tenant_id: str) -> Optional[TestConnectionResult]",L4:integrations_handler,TestConnectionResult | get | info | record_connection,datasource_model,pure,yes,34,Test a data source connection.
integrations,L5,datasources_facade,DataSourcesFacade.update_source,"async update_source(source_id: str, tenant_id: str, name: Optional[str], description: Optional[str], config: Optional[Dict[str, Any]], metadata: Optional[Dict[str, Any]]) -> Optional[CustomerDataSource]",L4:integrations_handler,DataSourceConfig | get | update,datasource_model,pure,yes,51,Update a data source.
integrations,L5,datasources_facade,TestConnectionResult.to_dict,"to_dict() -> Dict[str, Any]",L4:integrations_handler,,datasource_model,pure,no,8,Convert to dictionary.
integrations,L5,datasources_facade,get_datasources_facade,get_datasources_facade() -> DataSourcesFacade,L4:integrations_handler,DataSourcesFacade,datasource_model,pure,no,14,Get the data sources facade instance.
integrations,L5,dispatcher_engine,DispatcherConfig.from_env,from_env() -> 'DispatcherConfig',L5:bridges_engine,cls | float | getenv | int | lower,__future__ | loop_events | sqlalchemy,pure,no,14,Load config from environment variables.
integrations,L5,dispatcher_engine,IntegrationDispatcher.__init__,"__init__(redis_client: Any, db_session_factory: Callable, config: DispatcherConfig | None)",L5:bridges_engine,from_env | info | set,__future__ | loop_events | sqlalchemy,pure,no,26,
integrations,L5,dispatcher_engine,IntegrationDispatcher._check_db_idempotency,"async _check_db_idempotency(incident_id: str, stage: LoopStage) -> bool",L5:bridges_engine,db_factory | error | execute | fetchone | text,__future__ | loop_events | sqlalchemy,db_write,yes,25,HYGIENE #4: Check if this incident+stage was already processed.
integrations,L5,dispatcher_engine,IntegrationDispatcher._check_human_checkpoint_needed,async _check_human_checkpoint_needed(event: LoopEvent) -> Optional[HumanCheckpoint],L5:bridges_engine,create | get,__future__ | loop_events | sqlalchemy,pure,yes,39,Check if a human checkpoint is needed for this event.
integrations,L5,dispatcher_engine,IntegrationDispatcher._execute_handlers,async _execute_handlers(event: LoopEvent) -> LoopEvent,L5:bridges_engine,error | exception | get | handler | str | wait_for | warning,__future__ | loop_events | sqlalchemy,pure,yes,28,Execute all handlers for an event stage.
integrations,L5,dispatcher_engine,IntegrationDispatcher._get_or_create_loop_status,async _get_or_create_loop_status(event: LoopEvent) -> LoopStatus,L5:bridges_engine,LoopStatus | _load_loop_status | _persist_loop_status | uuid4,__future__ | loop_events | sqlalchemy,pure,yes,24,Get existing loop status or create new one.
integrations,L5,dispatcher_engine,IntegrationDispatcher._load_checkpoint,async _load_checkpoint(checkpoint_id: str) -> Optional[HumanCheckpoint],L5:bridges_engine,HumanCheckpoint | HumanCheckpointType | LoopStage | db_factory | error | execute | fetchone | isinstance | loads | text,__future__ | loop_events | sqlalchemy,db_write,yes,33,Load checkpoint from database.
integrations,L5,dispatcher_engine,IntegrationDispatcher._load_loop_status,async _load_loop_status(incident_id: str) -> Optional[LoopStatus],L5:bridges_engine,LoopStatus | db_factory | error | execute | fetchone | get | isinstance | loads | text,__future__ | loop_events | sqlalchemy,db_write,yes,35,Load loop status from database.
integrations,L5,dispatcher_engine,IntegrationDispatcher._persist_checkpoint,async _persist_checkpoint(checkpoint: HumanCheckpoint) -> None,L5:bridges_engine,db_factory | dumps | error | execute | text,__future__ | loop_events | sqlalchemy,db_write,yes,38,Persist human checkpoint to database.
integrations,L5,dispatcher_engine,IntegrationDispatcher._persist_event,async _persist_event(event: LoopEvent) -> None,L5:bridges_engine,db_factory | dumps | ensure_json_serializable | error | execute | text | to_dict,__future__ | loop_events | sqlalchemy,db_write,yes,38,Persist event to database for durability.
integrations,L5,dispatcher_engine,IntegrationDispatcher._persist_loop_status,async _persist_loop_status(status: LoopStatus) -> None,L5:bridges_engine,db_factory | dumps | error | execute | text,__future__ | loop_events | sqlalchemy,db_write,yes,35,Persist loop status to database.
integrations,L5,dispatcher_engine,IntegrationDispatcher._publish_checkpoint_needed,async _publish_checkpoint_needed(checkpoint: HumanCheckpoint) -> None,L5:bridges_engine,dumps | error | publish,__future__ | loop_events | sqlalchemy,pure,yes,18,Publish checkpoint needed event for console notification.
integrations,L5,dispatcher_engine,IntegrationDispatcher._publish_event,async _publish_event(event: LoopEvent) -> None,L5:bridges_engine,dumps | error | publish | to_dict,__future__ | loop_events | sqlalchemy,pure,yes,7,Publish event to Redis for real-time updates.
integrations,L5,dispatcher_engine,IntegrationDispatcher._trigger_next_stage,"async _trigger_next_stage(event: LoopEvent, loop_status: LoopStatus) -> Optional[LoopEvent]",L5:bridges_engine,create | error | index | len,__future__ | loop_events | sqlalchemy,pure,yes,30,Determine and create the next stage event if needed.
integrations,L5,dispatcher_engine,IntegrationDispatcher._update_loop_status,"async _update_loop_status(loop_status: LoopStatus, event: LoopEvent) -> None",L5:bridges_engine,_persist_loop_status | append | get | now | set,__future__ | loop_events | sqlalchemy,pure,yes,35,Update loop status after event processing.
integrations,L5,dispatcher_engine,IntegrationDispatcher.dispatch,async dispatch(event: LoopEvent) -> LoopEvent,L5:bridges_engine,_check_db_idempotency | _check_human_checkpoint_needed | _execute_handlers | _get_or_create_loop_status | _load_loop_status | _persist_checkpoint | _persist_event | _publish_checkpoint_needed | _publish_event | _trigger_next_stage | _update_loop_status | add | append | debug | dispatch,__future__ | loop_events | sqlalchemy,db_write,yes,98,Dispatch an event through the integration loop.
integrations,L5,dispatcher_engine,IntegrationDispatcher.get_loop_status,async get_loop_status(incident_id: str) -> Optional[LoopStatus],L5:bridges_engine,_load_loop_status,__future__ | loop_events | sqlalchemy,pure,yes,5,Get current loop status for an incident.
integrations,L5,dispatcher_engine,IntegrationDispatcher.get_pending_checkpoints,async get_pending_checkpoints(tenant_id: str) -> list[HumanCheckpoint],L5:bridges_engine,values,__future__ | loop_events | sqlalchemy,pure,yes,3,Get all pending human checkpoints for a tenant.
integrations,L5,dispatcher_engine,IntegrationDispatcher.is_bridge_enabled,is_bridge_enabled(stage: LoopStage) -> bool,L5:bridges_engine,get,__future__ | loop_events | sqlalchemy,pure,no,10,Check if a bridge is enabled by its stage.
integrations,L5,dispatcher_engine,IntegrationDispatcher.register_handler,"register_handler(stage: LoopStage, handler: Handler) -> None",L5:bridges_engine,append | debug,__future__ | loop_events | sqlalchemy,pure,no,4,Register a handler for a specific loop stage.
integrations,L5,dispatcher_engine,IntegrationDispatcher.resolve_checkpoint,"async resolve_checkpoint(checkpoint_id: str, user_id: str, resolution: str) -> Optional[LoopEvent]",L5:bridges_engine,ValueError | _load_checkpoint | _persist_checkpoint | create | dispatch | get | len | pop | remove | resolve,__future__ | loop_events | sqlalchemy,pure,yes,46,Resolve a pending human checkpoint.
integrations,L5,dispatcher_engine,IntegrationDispatcher.retry_failed_stage,"async retry_failed_stage(incident_id: str, stage: LoopStage, tenant_id: str) -> Optional[LoopEvent]",L5:bridges_engine,ValueError | create | dispatch | get_loop_status | remove,__future__ | loop_events | sqlalchemy,pure,yes,22,Retry a failed stage in the loop.
integrations,L5,dispatcher_engine,IntegrationDispatcher.revert_loop,"async revert_loop(incident_id: str, user_id: str, reason: str) -> None",L5:bridges_engine,ValueError | _persist_loop_status | clear | get_loop_status | rollback | warning,__future__ | loop_events | sqlalchemy,pure,yes,25,Revert all changes made by a loop.
integrations,L5,graduation_engine,CapabilityGates.can_auto_activate_policy,can_auto_activate_policy(status: ComputedGraduationStatus) -> bool,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,get,__future__,pure,no,9,Gate 2 required: Auto-activate policies only after self-correction proven.
integrations,L5,graduation_engine,CapabilityGates.can_auto_apply_recovery,can_auto_apply_recovery(status: ComputedGraduationStatus) -> bool,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,get,__future__,pure,no,9,Gate 1 required: Auto-apply recovery only after prevention proven.
integrations,L5,graduation_engine,CapabilityGates.can_full_auto_routing,can_full_auto_routing(status: ComputedGraduationStatus) -> bool,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,,__future__,pure,no,7,All gates required: Full autonomous routing only after proven.
integrations,L5,graduation_engine,CapabilityGates.get_blocked_capabilities,get_blocked_capabilities(status: ComputedGraduationStatus) -> list[str],?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,append | can_auto_activate_policy | can_auto_apply_recovery | can_full_auto_routing,__future__,pure,no,14,Get list of currently blocked capabilities.
integrations,L5,graduation_engine,CapabilityGates.get_unlocked_capabilities,get_unlocked_capabilities(status: ComputedGraduationStatus) -> list[str],?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,append | can_auto_activate_policy | can_auto_apply_recovery | can_full_auto_routing,__future__,pure,no,14,Get list of currently unlocked capabilities.
integrations,L5,graduation_engine,ComputedGraduationStatus.is_degraded,is_degraded() -> bool,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,,__future__,pure,no,2,
integrations,L5,graduation_engine,ComputedGraduationStatus.is_graduated,is_graduated() -> bool,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,,__future__,pure,no,2,
integrations,L5,graduation_engine,ComputedGraduationStatus.status_label,status_label() -> str,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,sum | upper | values,__future__,pure,no,8,
integrations,L5,graduation_engine,ComputedGraduationStatus.to_api_response,to_api_response() -> dict,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,isoformat | items,__future__,pure,no,26,Format for API response.
integrations,L5,graduation_engine,GraduationEngine.__init__,__init__(thresholds: Optional[GraduationThresholds]),?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,GraduationThresholds,__future__,pure,no,3,
integrations,L5,graduation_engine,GraduationEngine._check_degradation,"_check_degradation(previous_level: GraduationLevel, current_level: GraduationLevel, gates: dict[str, GateEvidence], evidence: GraduationEvidence) -> Optional[dict]",?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,items | now,__future__,pure,no,52,Check if graduation should be degraded.
integrations,L5,graduation_engine,GraduationEngine._evaluate_gate1,_evaluate_gate1(evidence: GraduationEvidence) -> GateEvidence,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,GateEvidence | get | isoformat | min | now,__future__,pure,no,43,Gate 1: Prevention Proof
integrations,L5,graduation_engine,GraduationEngine._evaluate_gate2,_evaluate_gate2(evidence: GraduationEvidence) -> GateEvidence,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,GateEvidence | get | min | now,__future__,pure,no,35,Gate 2: Regret Rollback
integrations,L5,graduation_engine,GraduationEngine._evaluate_gate3,_evaluate_gate3(evidence: GraduationEvidence) -> GateEvidence,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,GateEvidence | get | isoformat | min | now,__future__,pure,no,30,Gate 3: Console Timeline
integrations,L5,graduation_engine,GraduationEngine.compute,"compute(evidence: GraduationEvidence, previous_status: Optional[ComputedGraduationStatus]) -> ComputedGraduationStatus",?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,ComputedGraduationStatus | _check_degradation | _evaluate_gate1 | _evaluate_gate2 | _evaluate_gate3 | now | sum | values,__future__,pure,no,66,Compute graduation status from evidence.
integrations,L5,graduation_engine,SimulationState.is_demo_mode,is_demo_mode() -> bool,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,,__future__,pure,no,3,True if any simulation is active.
integrations,L5,graduation_engine,SimulationState.to_display,to_display() -> dict,?:M25_integrations | ?:graduation_evaluator | L2:M25_integrations | ?:test_m25_graduation_downgrade,,__future__,pure,no,11,Display format - clearly marked as simulation.
integrations,L5,http_connector_engine,HttpConnectorError.__init__,"__init__(message: str, status_code: Optional[int])",,__init__ | super,credentials | httpx,pure,no,3,
integrations,L5,http_connector_engine,HttpConnectorService.__init__,"__init__(config: HttpConnectorConfig, credential_service: Optional[CredentialService])",,,credentials | httpx,pure,no,8,
integrations,L5,http_connector_engine,HttpConnectorService._build_url,"_build_url(path: str, payload: Dict[str, Any]) -> str",,items | replace | rstrip | str,credentials | httpx,pure,no,10,Build URL from base URL and path.
integrations,L5,http_connector_engine,HttpConnectorService._check_rate_limit,_check_rate_limit(tenant_id: str),,RateLimitExceededError | len | now | timestamp | warning,credentials | httpx,pure,no,22,Check if rate limit exceeded (AC-059-09).
integrations,L5,http_connector_engine,HttpConnectorService._get_auth_headers,"async _get_auth_headers() -> Dict[str, str]",,b64encode | decode | encode | get | warning,credentials | httpx,pure,yes,22,Get auth headers from vault (machine-controlled).
integrations,L5,http_connector_engine,HttpConnectorService._record_request,_record_request(tenant_id: str),,append | now,credentials | httpx,pure,no,5,Record a request for rate limiting.
integrations,L5,http_connector_engine,HttpConnectorService._resolve_endpoint,_resolve_endpoint(action: str) -> EndpointConfig,,ValueError | keys | list,credentials | httpx,pure,no,8,Map action to endpoint (machine-controlled).
integrations,L5,http_connector_engine,HttpConnectorService.execute,"async execute(action: str, payload: Dict[str, Any], tenant_id: Optional[str]) -> Dict[str, Any]",,AsyncClient | HttpConnectorError | ValueError | _build_url | _check_rate_limit | _get_auth_headers | _record_request | _resolve_endpoint | decode | delete | error | get | json | len | patch,credentials | httpx,"db_write,external_api",yes,130,Execute a governed HTTP request.
integrations,L5,http_connector_engine,HttpConnectorService.id,id() -> str,,,credentials | httpx,pure,no,3,Connector ID for protocol compliance.
integrations,L5,http_connector_engine,RateLimitExceededError.__init__,__init__(retry_after_seconds: int),,__init__ | super,credentials | httpx,pure,no,3,
integrations,L5,iam_engine,AccessDecision.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,jwt,pure,no,12,Convert to dictionary.
integrations,L5,iam_engine,IAMService.__init__,__init__(),,_setup_default_roles,jwt,pure,no,5,
integrations,L5,iam_engine,IAMService._create_system_identity,"_create_system_identity(system_id: str, tenant_id: Optional[str]) -> Identity",,Identity | _expand_role_permissions,jwt,pure,no,16,Create a system identity for internal services.
integrations,L5,iam_engine,IAMService._expand_role_permissions,_expand_role_permissions(roles: Set[str]) -> Set[str],,set | update,jwt,pure,no,7,Expand roles into their constituent permissions.
integrations,L5,iam_engine,IAMService._resolve_api_key_identity,"async _resolve_api_key_identity(api_key: str, tenant_id: Optional[str]) -> Optional[Identity]",,Identity | _expand_role_permissions,jwt,pure,yes,19,Resolve identity from API key.
integrations,L5,iam_engine,IAMService._resolve_clerk_identity,"async _resolve_clerk_identity(token: str, tenant_id: Optional[str]) -> Optional[Identity]",,Identity | _expand_role_permissions | decode | get | set | warning,jwt,pure,yes,33,Resolve identity from Clerk JWT.
integrations,L5,iam_engine,IAMService._setup_default_roles,_setup_default_roles() -> None,,,jwt,pure,no,44,Set up default role-permission mappings.
integrations,L5,iam_engine,IAMService.check_access,"async check_access(identity: Identity, resource: str, action: str) -> AccessDecision",,AccessDecision | append | len | warning,jwt,pure,yes,47,Check if an identity can perform an action on a resource.
integrations,L5,iam_engine,IAMService.define_resource_permissions,"define_resource_permissions(resource: str, permissions: Set[str]) -> None",,,jwt,pure,no,7,Define permissions for a resource.
integrations,L5,iam_engine,IAMService.define_role,"define_role(role_name: str, permissions: Set[str]) -> None",,info | len,jwt,pure,no,8,Define or update a role's permissions.
integrations,L5,iam_engine,IAMService.get_access_log,"get_access_log(identity_id: Optional[str], resource: Optional[str], limit: int) -> List[AccessDecision]",,,jwt,pure,no,15,Get access decision log for auditing.
integrations,L5,iam_engine,IAMService.grant_role,"async grant_role(identity_id: str, role: str, granted_by: str) -> bool",,info,jwt,pure,yes,13,Grant a role to an identity.
integrations,L5,iam_engine,IAMService.list_resources,"list_resources() -> Dict[str, Set[str]]",,dict,jwt,pure,no,3,List all resources and their required permissions.
integrations,L5,iam_engine,IAMService.list_roles,"list_roles() -> Dict[str, Set[str]]",,dict,jwt,pure,no,3,List all defined roles and their permissions.
integrations,L5,iam_engine,IAMService.resolve_identity,"async resolve_identity(provider: IdentityProvider, token_or_key: str, tenant_id: Optional[str]) -> Optional[Identity]",,_create_system_identity | _resolve_api_key_identity | _resolve_clerk_identity | error | warning,jwt,pure,yes,31,Resolve an identity from a token or API key.
integrations,L5,iam_engine,IAMService.revoke_role,"async revoke_role(identity_id: str, role: str, revoked_by: str) -> bool",,info,jwt,pure,yes,13,Revoke a role from an identity.
integrations,L5,iam_engine,Identity.has_all_roles,has_all_roles(roles: List[str]) -> bool,,issubset | set,jwt,pure,no,3,Check if identity has all specified roles.
integrations,L5,iam_engine,Identity.has_any_role,has_any_role(roles: List[str]) -> bool,,bool | set,jwt,pure,no,3,Check if identity has any of the specified roles.
integrations,L5,iam_engine,Identity.has_permission,has_permission(permission: str) -> bool,,,jwt,pure,no,3,Check if identity has a specific permission.
integrations,L5,iam_engine,Identity.has_role,has_role(role: str) -> bool,,,jwt,pure,no,3,Check if identity has a specific role.
integrations,L5,iam_engine,Identity.to_dict,"to_dict() -> Dict[str, Any]",,isoformat | list,jwt,pure,no,16,Convert to dictionary.
integrations,L5,integrations_facade,IntegrationsFacade.__init__,__init__() -> None,?:aos_cus_integrations | L4:integrations_handler,CusIntegrationService,cus_integration_engine,pure,no,3,Initialize with CusIntegrationService.
integrations,L5,integrations_facade,IntegrationsFacade.create_integration,async create_integration(tenant_id: UUID) -> IntegrationDetailResult,?:aos_cus_integrations | L4:integrations_handler,IntegrationDetailResult | create_integration | str,cus_integration_engine,pure,yes,45,Create a new LLM integration.
integrations,L5,integrations_facade,IntegrationsFacade.delete_integration,"async delete_integration(tenant_id: UUID, integration_id: str) -> IntegrationDeleteResult",?:aos_cus_integrations | L4:integrations_handler,IntegrationDeleteResult | delete_integration,cus_integration_engine,pure,yes,15,Delete an integration (soft delete).
integrations,L5,integrations_facade,IntegrationsFacade.disable_integration,"async disable_integration(tenant_id: UUID, integration_id: str) -> Optional[IntegrationLifecycleResult]",?:aos_cus_integrations | L4:integrations_handler,IntegrationLifecycleResult | disable_integration | str,cus_integration_engine,pure,yes,19,Disable an integration.
integrations,L5,integrations_facade,IntegrationsFacade.enable_integration,"async enable_integration(tenant_id: UUID, integration_id: str) -> Optional[IntegrationLifecycleResult]",?:aos_cus_integrations | L4:integrations_handler,IntegrationLifecycleResult | enable_integration | str,cus_integration_engine,pure,yes,19,Enable an integration.
integrations,L5,integrations_facade,IntegrationsFacade.get_health_status,"async get_health_status(tenant_id: UUID, integration_id: str) -> Optional[HealthStatusResult]",?:aos_cus_integrations | L4:integrations_handler,HealthStatusResult | get_integration | str,cus_integration_engine,pure,yes,20,Get cached health status without running a new check.
integrations,L5,integrations_facade,IntegrationsFacade.get_integration,"async get_integration(tenant_id: UUID, integration_id: str) -> Optional[IntegrationDetailResult]",?:aos_cus_integrations | L4:integrations_handler,IntegrationDetailResult | get_integration | str,cus_integration_engine,pure,yes,31,Get full details for a specific integration.
integrations,L5,integrations_facade,IntegrationsFacade.get_limits_status,"async get_limits_status(tenant_id: UUID, integration_id: str) -> Optional[LimitsStatusResult]",?:aos_cus_integrations | L4:integrations_handler,LimitsStatusResult | get_limits_status,cus_integration_engine,pure,yes,28,Get current usage against configured limits.
integrations,L5,integrations_facade,IntegrationsFacade.list_integrations,async list_integrations(tenant_id: UUID) -> IntegrationListResult,?:aos_cus_integrations | L4:integrations_handler,IntegrationListResult | IntegrationSummaryResult | list_integrations | str,cus_integration_engine,pure,yes,32,List all integrations for the tenant.
integrations,L5,integrations_facade,IntegrationsFacade.test_credentials,"async test_credentials(tenant_id: UUID, integration_id: str) -> Optional[HealthCheckResult]",?:aos_cus_integrations | L4:integrations_handler,HealthCheckResult | get | test_credentials,cus_integration_engine,pure,yes,21,Test credentials and update health status.
integrations,L5,integrations_facade,IntegrationsFacade.update_integration,"async update_integration(tenant_id: UUID, integration_id: str, **update_data) -> Optional[IntegrationDetailResult]",?:aos_cus_integrations | L4:integrations_handler,IntegrationDetailResult | str | update_integration,cus_integration_engine,pure,yes,33,Update an existing integration.
integrations,L5,integrations_facade,get_integrations_facade,get_integrations_facade() -> IntegrationsFacade,?:aos_cus_integrations | L4:integrations_handler,IntegrationsFacade,cus_integration_engine,pure,no,6,Get the singleton IntegrationsFacade instance.
integrations,L5,loop_events,ConfidenceBand.allows_auto_apply,allows_auto_apply() -> bool,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,3,Only strong matches allow auto-apply.
integrations,L5,loop_events,ConfidenceBand.from_confidence,from_confidence(confidence: float) -> 'ConfidenceBand',L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,8,Classify confidence score into band.
integrations,L5,loop_events,ConfidenceBand.requires_human_review,requires_human_review() -> bool,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,3,Weak and novel patterns require human review.
integrations,L5,loop_events,ConfidenceCalculator.calculate_recovery_confidence,"calculate_recovery_confidence(base_confidence: float, occurrence_count: int, is_strong_match: bool) -> tuple[float, str, dict]",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,min,__future__,pure,no,32,Calculate recovery confidence with occurrence boosting.
integrations,L5,loop_events,ConfidenceCalculator.get_confirmation_level,get_confirmation_level(confidence: float) -> int,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,8,Get required confirmation level based on confidence.
integrations,L5,loop_events,ConfidenceCalculator.should_auto_apply,"should_auto_apply(confidence: float, occurrence_count: int) -> bool",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,3,Determine if recovery should auto-apply.
integrations,L5,loop_events,HumanCheckpoint.create,"create(checkpoint_type: HumanCheckpointType, incident_id: str, tenant_id: str, stage: LoopStage, target_id: str, description: str, options: list[str] | None) -> 'HumanCheckpoint'",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,cls | get | now | uuid4,__future__,pure,no,30,Factory for creating checkpoints.
integrations,L5,loop_events,HumanCheckpoint.is_pending,is_pending() -> bool,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,2,
integrations,L5,loop_events,HumanCheckpoint.resolve,"resolve(user_id: str, resolution: str) -> None",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,now,__future__,pure,no,5,Resolve checkpoint with user action.
integrations,L5,loop_events,LoopEvent.create,"create(incident_id: str, tenant_id: str, stage: LoopStage, details: dict[str, Any] | None, failure_state: LoopFailureState | None, confidence_band: ConfidenceBand | None) -> 'LoopEvent'",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,cls | now | uuid4,__future__,pure,no,21,Factory method to create events with auto-generated ID and timestamp.
integrations,L5,loop_events,LoopEvent.is_blocked,is_blocked() -> bool,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,6,Check if loop is blocked at this stage.
integrations,L5,loop_events,LoopEvent.is_success,is_success() -> bool,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,3,Check if event represents successful stage completion.
integrations,L5,loop_events,LoopEvent.to_dict,"to_dict() -> dict[str, Any]",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,isoformat,__future__,pure,no,14,Serialize for Redis/JSON.
integrations,L5,loop_events,LoopStatus._generate_narrative,"_generate_narrative() -> dict[str, str]",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,33,Generate narrative artifacts for storytelling.
integrations,L5,loop_events,LoopStatus.completion_pct,completion_pct() -> float,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,len,__future__,pure,no,3,Calculate loop completion percentage.
integrations,L5,loop_events,LoopStatus.to_console_display,"to_console_display() -> dict[str, Any]",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,_generate_narrative | append,__future__,pure,no,46,Format for console display:
integrations,L5,loop_events,LoopStatus.to_dict,"to_dict() -> dict[str, Any]",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,isoformat | to_dict,__future__,pure,no,22,Serialize for JSON/Redis storage.
integrations,L5,loop_events,PatternMatchResult.from_match,"from_match(incident_id: str, pattern_id: str, confidence: float, signature_hash: str, is_new: bool, details: dict[str, Any] | None) -> 'PatternMatchResult'",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,cls | from_confidence,__future__,pure,no,21,Create result from successful match.
integrations,L5,loop_events,PatternMatchResult.no_match,"no_match(incident_id: str, signature_hash: str) -> 'PatternMatchResult'",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,cls,__future__,pure,no,11,Create result for no match found.
integrations,L5,loop_events,PatternMatchResult.should_auto_proceed,should_auto_proceed() -> bool,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,3,Only proceed automatically for strong matches.
integrations,L5,loop_events,PatternMatchResult.to_dict,"to_dict() -> dict[str, Any]",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,13,Serialize for JSON/Redis.
integrations,L5,loop_events,PolicyRule.add_confirmation,add_confirmation() -> bool,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,now,__future__,pure,no,8,Add confirmation. Returns True if ready to activate.
integrations,L5,loop_events,PolicyRule.create,"create(name: str, description: str, category: str, condition: str, action: str, source_pattern_id: str, source_recovery_id: str, confidence: float, scope_type: str, scope_id: str | None, confirmations_required: int) -> 'PolicyRule'",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,cls | from_confidence | uuid4,__future__,pure,no,37,Factory method for creating policy rules.
integrations,L5,loop_events,PolicyRule.record_regret,record_regret() -> None,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,6,Record when this policy caused an incident (regret).
integrations,L5,loop_events,PolicyRule.record_shadow_evaluation,record_shadow_evaluation(would_block: bool) -> None,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,5,Track shadow mode evaluations for confidence building.
integrations,L5,loop_events,PolicyRule.shadow_block_rate,shadow_block_rate() -> float,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,5,Percentage of shadow evaluations that would have blocked.
integrations,L5,loop_events,PolicyRule.to_dict,"to_dict() -> dict[str, Any]",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,isoformat,__future__,pure,no,25,Serialize for JSON/Redis.
integrations,L5,loop_events,RecoverySuggestion.add_confirmation,add_confirmation() -> bool,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,7,Add a confirmation. Returns True if threshold reached.
integrations,L5,loop_events,RecoverySuggestion.create,"create(incident_id: str, pattern_id: str, action_type: str, action_params: dict[str, Any], confidence: float, suggestion_type: Literal['template', 'generated'], requires_confirmation: int) -> 'RecoverySuggestion'",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,cls | from_confidence | uuid4,__future__,pure,no,25,Factory method for creating recovery suggestions.
integrations,L5,loop_events,RecoverySuggestion.none_available,"none_available(incident_id: str, pattern_id: str) -> 'RecoverySuggestion'",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,cls | uuid4,__future__,pure,no,16,Create placeholder when no recovery is available.
integrations,L5,loop_events,RecoverySuggestion.to_dict,"to_dict() -> dict[str, Any]",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,17,Serialize for JSON/Redis.
integrations,L5,loop_events,RoutingAdjustment.check_kpi_regression,check_kpi_regression(current_kpi: float) -> bool,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,rollback,__future__,pure,no,16,Check if KPI has regressed past threshold.
integrations,L5,loop_events,RoutingAdjustment.create,"create(agent_id: str, adjustment_type: str, magnitude: float, reason: str, source_policy_id: str, capability: str | None, max_delta: float, decay_days: int) -> 'RoutingAdjustment'",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,cls | max | min | now | replace | uuid4,__future__,pure,no,33,Factory with guardrail enforcement.
integrations,L5,loop_events,RoutingAdjustment.effective_magnitude,effective_magnitude() -> float,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,now | total_seconds,__future__,pure,no,15,Calculate current magnitude with decay applied.
integrations,L5,loop_events,RoutingAdjustment.rollback,rollback(reason: str) -> None,L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,,__future__,pure,no,5,Rollback this adjustment.
integrations,L5,loop_events,RoutingAdjustment.to_dict,"to_dict() -> dict[str, Any]",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,isoformat,__future__,pure,no,22,Serialize for JSON/Redis.
integrations,L5,loop_events,ensure_json_serializable,"ensure_json_serializable(obj: Any, path: str) -> Any",L6:bridges_driver | L5s:__init__ | L5:bridges_engine | L5:dispatcher_engine | L5:cost_bridges_engine,TypeError | ensure_json_serializable | enumerate | hasattr | isinstance | isoformat | items | to_dict | type,__future__,pure,no,26,Guard function to ensure all objects stored in details are JSON-serializable.
integrations,L5,mcp_connector_engine,McpApprovalRequiredError.__init__,__init__(tool_name: str),,__init__ | super,credentials | httpx | jsonschema,pure,no,3,
integrations,L5,mcp_connector_engine,McpConnectorError.__init__,"__init__(message: str, code: Optional[int])",,__init__ | super,credentials | httpx | jsonschema,pure,no,3,
integrations,L5,mcp_connector_engine,McpConnectorService.__init__,"__init__(config: McpConnectorConfig, tool_registry: Dict[str, McpToolDefinition], credential_service: Optional[CredentialService])",,,credentials | httpx | jsonschema,pure,no,10,
integrations,L5,mcp_connector_engine,McpConnectorService._build_mcp_request,"_build_mcp_request(tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]",,,credentials | httpx | jsonschema,pure,no,15,Build MCP JSON-RPC request.
integrations,L5,mcp_connector_engine,McpConnectorService._check_rate_limit,_check_rate_limit(tenant_id: str),,McpRateLimitExceededError | len | now | timestamp | warning,credentials | httpx | jsonschema,pure,no,22,Check if rate limit exceeded.
integrations,L5,mcp_connector_engine,McpConnectorService._get_api_key,async _get_api_key() -> str,,McpConnectorError | get,credentials | httpx | jsonschema,pure,yes,7,Get API key from vault (machine-controlled).
integrations,L5,mcp_connector_engine,McpConnectorService._record_request,_record_request(tenant_id: str),,append | now,credentials | httpx | jsonschema,pure,no,5,Record a request for rate limiting.
integrations,L5,mcp_connector_engine,McpConnectorService._resolve_tool,_resolve_tool(tool_name: str) -> McpToolDefinition,,ValueError | keys | list,credentials | httpx | jsonschema,pure,no,15,Resolve tool by name (machine-controlled allowlist).
integrations,L5,mcp_connector_engine,McpConnectorService._validate_against_schema,"_validate_against_schema(schema: Dict[str, Any], payload: Dict[str, Any])",,Draft7Validator | McpSchemaValidationError | iter_errors | str | validate | warning,credentials | httpx | jsonschema,pure,no,25,Validate payload against JSON Schema.
integrations,L5,mcp_connector_engine,McpConnectorService.execute,"async execute(action: str, payload: Dict[str, Any], tenant_id: Optional[str]) -> Dict[str, Any]",,AsyncClient | McpApprovalRequiredError | McpConnectorError | _build_mcp_request | _check_rate_limit | _get_api_key | _record_request | _resolve_tool | _validate_against_schema | error | get | json | len | min | post,credentials | httpx | jsonschema,external_api,yes,139,Execute an MCP tool call.
integrations,L5,mcp_connector_engine,McpConnectorService.get_available_tools,"get_available_tools() -> List[Dict[str, Any]]",,append,credentials | httpx | jsonschema,pure,no,13,Get list of available tools with their schemas.
integrations,L5,mcp_connector_engine,McpConnectorService.id,id() -> str,,,credentials | httpx | jsonschema,pure,no,3,Connector ID for protocol compliance.
integrations,L5,mcp_connector_engine,McpRateLimitExceededError.__init__,__init__(retry_after_seconds: int),,__init__ | super,credentials | httpx | jsonschema,pure,no,3,
integrations,L5,mcp_connector_engine,McpSchemaValidationError.__init__,"__init__(message: str, errors: List[str])",,__init__ | super,credentials | httpx | jsonschema,pure,no,3,
integrations,L5,prevention_contract,PreventionContractViolation.__init__,"__init__(rule: str, details: str)",L5:__init__,__init__ | super,__future__,pure,no,4,
integrations,L5,prevention_contract,assert_no_deletion,assert_no_deletion(record_id: str) -> None,L5:__init__,PreventionContractViolation,__future__,pure,no,9,Assert that a prevention record cannot be deleted.
integrations,L5,prevention_contract,assert_prevention_immutable,"assert_prevention_immutable(record_id: str, existing_record: dict[str, Any]) -> None",L5:__init__,PreventionContractViolation,__future__,pure,no,10,Assert that a prevention record has not been modified.
integrations,L5,prevention_contract,validate_prevention_candidate,validate_prevention_candidate(candidate: PreventionCandidate) -> None,L5:__init__,PreventionContractViolation | info,__future__,pure,no,55,Validate that a prevention candidate satisfies the contract.
integrations,L5,prevention_contract,validate_prevention_for_graduation,"validate_prevention_for_graduation(prevention_record: dict[str, Any], policy_activated_at: datetime) -> bool",L5:__init__,debug | get | info | isinstance | replace,__future__,pure,no,32,Validate that a prevention record counts toward graduation.
integrations,L5,protocol,CredentialService.get,async get(credential_ref: str) -> Credential,L5:mcp_connector_engine | L5:sql_gateway | L5:http_connector_engine | L5:__init__ | ?:credential_service,,types,pure,yes,11,Get credential from vault.
integrations,L5,service,CredentialService.__init__,"__init__(vault: CredentialVault, audit_enabled: bool)",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,,vault,pure,no,8,
integrations,L5,service,CredentialService._audit,"_audit(credential_id: str, tenant_id: str, accessor_id: str, accessor_type: str, action: str, success: bool, error_message: Optional[str]) -> None",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,CredentialAccessRecord | append | len | now,vault,pure,no,29,Record an access for auditing.
integrations,L5,service,CredentialService._validate_name,_validate_name(name: str) -> None,?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,ValueError | len | strip,vault,pure,no,6,Validate credential name.
integrations,L5,service,CredentialService._validate_secret_data,"_validate_secret_data(credential_type: CredentialType, secret_data: Dict[str, str]) -> None",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,ValueError | issubset | keys,vault,pure,no,32,Validate secret data based on credential type.
integrations,L5,service,CredentialService._validate_tenant_id,_validate_tenant_id(tenant_id: str) -> None,?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,ValueError | len | strip,vault,pure,no,6,Validate tenant ID.
integrations,L5,service,CredentialService.delete_credential,"async delete_credential(tenant_id: str, credential_id: str, accessor_id: Optional[str], accessor_type: str) -> bool",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,_audit | delete_credential | str,vault,pure,yes,34,Delete a credential.
integrations,L5,service,CredentialService.get_access_log,"get_access_log(tenant_id: Optional[str], credential_id: Optional[str], limit: int) -> List[CredentialAccessRecord]",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,,vault,pure,no,15,Get credential access log (for auditing).
integrations,L5,service,CredentialService.get_credential,"async get_credential(tenant_id: str, credential_id: str, accessor_id: Optional[str], accessor_type: str) -> Optional[CredentialData]",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,_audit | get_credential | now | str | warning,vault,pure,yes,70,Get a credential with expiration checking.
integrations,L5,service,CredentialService.get_expiring_credentials,"async get_expiring_credentials(tenant_id: str, days_until_expiry: int) -> List[CredentialMetadata]",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,list_credentials | now | timedelta,vault,pure,yes,20,Get credentials expiring within specified days.
integrations,L5,service,CredentialService.get_rotatable_credentials,async get_rotatable_credentials(tenant_id: str) -> List[CredentialMetadata],?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,append | list_credentials | now | timedelta,vault,pure,yes,21,Get credentials that need rotation based on their schedule.
integrations,L5,service,CredentialService.get_secret_value,"async get_secret_value(tenant_id: str, credential_id: str, key: str, accessor_id: Optional[str], accessor_type: str) -> Optional[str]",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,get | get_credential,vault,pure,yes,32,Get a specific secret value from a credential.
integrations,L5,service,CredentialService.list_credentials,"async list_credentials(tenant_id: str, credential_type: Optional[CredentialType], tags: Optional[List[str]], include_expired: bool) -> List[CredentialMetadata]",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,list_credentials | now,vault,pure,yes,30,List credentials for a tenant.
integrations,L5,service,CredentialService.rotate_credential,"async rotate_credential(tenant_id: str, credential_id: str, new_secret_data: Dict[str, str], accessor_id: Optional[str], accessor_type: str) -> bool",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,_audit | rotate_credential | str,vault,pure,yes,39,Rotate a credential's secrets.
integrations,L5,service,CredentialService.store_credential,"async store_credential(tenant_id: str, name: str, credential_type: CredentialType, secret_data: Dict[str, str], description: Optional[str], tags: Optional[List[str]], expires_at: Optional[datetime], is_rotatable: bool, rotation_interval_days: Optional[int], metadata: Optional[Dict[str, Any]], accessor_id: Optional[str], accessor_type: str) -> str",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,_audit | _validate_name | _validate_secret_data | _validate_tenant_id | info | store_credential | str,vault,pure,yes,80,Store a credential with validation.
integrations,L5,service,CredentialService.update_credential,"async update_credential(tenant_id: str, credential_id: str, secret_data: Optional[Dict[str, str]], description: Optional[str], tags: Optional[List[str]], expires_at: Optional[datetime], metadata: Optional[Dict[str, Any]], accessor_id: Optional[str], accessor_type: str) -> bool",?:rbac_engine | ?:role_mapping | ?:__init__ | ?:agent_spawn | ?:definitions | ?:failure_intelligence | ?:datasets | L5:datasets_engine,_audit | str | update_credential,vault,pure,yes,47,Update a credential.
integrations,L5,sql_gateway,SqlGatewayService.__init__,"__init__(config: SqlGatewayConfig, template_registry: Dict[str, QueryTemplate], credential_service: Optional[CredentialService])",?:__init__ | ?:test_connectors,,asyncpg | credentials,pure,no,9,
integrations,L5,sql_gateway,SqlGatewayService._check_sql_injection,_check_sql_injection(value: str),?:__init__ | ?:test_connectors,SqlInjectionAttemptError | upper | warning,asyncpg | credentials,pure,no,19,Check for SQL injection patterns.
integrations,L5,sql_gateway,SqlGatewayService._coerce_parameter,"_coerce_parameter(value: Any, spec: ParameterSpec) -> Any",?:__init__ | ?:test_connectors,UUID | ValueError | _check_sql_injection | bool | date | float | fromisoformat | int | isinstance | len | lower | str | strptime,asyncpg | credentials,pure,no,73,Coerce and validate a single parameter.
integrations,L5,sql_gateway,SqlGatewayService._get_connection_string,async _get_connection_string() -> str,?:__init__ | ?:test_connectors,SqlGatewayError | get,asyncpg | credentials,pure,yes,9,Get connection string from vault (machine-controlled).
integrations,L5,sql_gateway,SqlGatewayService._resolve_template,_resolve_template(template_id: str) -> QueryTemplate,?:__init__ | ?:test_connectors,ValueError | keys | list,asyncpg | credentials,pure,no,17,Resolve template by ID (machine-controlled).
integrations,L5,sql_gateway,SqlGatewayService._validate_parameters,"_validate_parameters(template: QueryTemplate, payload: Dict[str, Any]) -> Dict[str, Any]",?:__init__ | ?:test_connectors,ValueError | _coerce_parameter | keys | list | set | warning,asyncpg | credentials,pure,no,43,Validate and type-coerce parameters.
integrations,L5,sql_gateway,SqlGatewayService.execute,"async execute(action: str, payload: Dict[str, Any], tenant_id: Optional[str]) -> Dict[str, Any]",?:__init__ | ?:test_connectors,SqlGatewayError | _get_connection_string | _resolve_template | _validate_parameters | close | connect | dict | encode | error | fetch | get | len | min | pop | str,asyncpg | credentials,pure,yes,118,Execute a templated SQL query.
integrations,L5,sql_gateway,SqlGatewayService.id,id() -> str,?:__init__ | ?:test_connectors,,asyncpg | credentials,pure,no,3,Connector ID for protocol compliance.
integrations,L5,vault,CredentialData.credential_id,credential_id() -> str,L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,no,2,
integrations,L5,vault,CredentialData.tenant_id,tenant_id() -> str,L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,no,2,
integrations,L5,vault,CredentialVault.delete_credential,"async delete_credential(tenant_id: str, credential_id: str) -> bool",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,yes,16,Delete a credential.
integrations,L5,vault,CredentialVault.get_credential,"async get_credential(tenant_id: str, credential_id: str) -> Optional[CredentialData]",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,yes,16,Get a credential by ID.
integrations,L5,vault,CredentialVault.get_metadata,"async get_metadata(tenant_id: str, credential_id: str) -> Optional[CredentialMetadata]",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,yes,16,Get credential metadata without secret values.
integrations,L5,vault,CredentialVault.list_credentials,"async list_credentials(tenant_id: str, credential_type: Optional[CredentialType], tags: Optional[list[str]]) -> list[CredentialMetadata]",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,yes,18,List credentials for a tenant (metadata only).
integrations,L5,vault,CredentialVault.rotate_credential,"async rotate_credential(tenant_id: str, credential_id: str, new_secret_data: Dict[str, str]) -> bool",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,yes,18,Rotate a credential's secret values.
integrations,L5,vault,CredentialVault.store_credential,"async store_credential(tenant_id: str, name: str, credential_type: CredentialType, secret_data: Dict[str, str], description: Optional[str], tags: Optional[list[str]], expires_at: Optional[datetime], is_rotatable: bool, rotation_interval_days: Optional[int], metadata: Optional[Dict[str, Any]]) -> str",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,yes,32,Store a credential and return its ID.
integrations,L5,vault,CredentialVault.update_credential,"async update_credential(tenant_id: str, credential_id: str, secret_data: Optional[Dict[str, str]], description: Optional[str], tags: Optional[list[str]], expires_at: Optional[datetime], metadata: Optional[Dict[str, Any]]) -> bool",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,yes,26,Update a credential.
integrations,L5,vault,EnvCredentialVault.__init__,__init__(),L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,no,3,
integrations,L5,vault,EnvCredentialVault.delete_credential,"async delete_credential(tenant_id: str, credential_id: str) -> bool",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,pop,httpx,pure,yes,12,Delete a credential.
integrations,L5,vault,EnvCredentialVault.get_credential,"async get_credential(tenant_id: str, credential_id: str) -> Optional[CredentialData]",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,CredentialData | CredentialMetadata | get | getenv | now | replace | upper,httpx,pure,yes,29,Get credential from memory or environment.
integrations,L5,vault,EnvCredentialVault.get_metadata,"async get_metadata(tenant_id: str, credential_id: str) -> Optional[CredentialMetadata]",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,get,httpx,pure,yes,8,Get credential metadata.
integrations,L5,vault,EnvCredentialVault.list_credentials,"async list_credentials(tenant_id: str, credential_type: Optional[CredentialType], tags: Optional[list[str]]) -> list[CredentialMetadata]",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,all | append | items | startswith,httpx,pure,yes,17,List credentials for tenant.
integrations,L5,vault,EnvCredentialVault.rotate_credential,"async rotate_credential(tenant_id: str, credential_id: str, new_secret_data: Dict[str, str]) -> bool",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,update_credential,httpx,pure,yes,12,Rotate credential secrets.
integrations,L5,vault,EnvCredentialVault.store_credential,"async store_credential(tenant_id: str, name: str, credential_type: CredentialType, secret_data: Dict[str, str], description: Optional[str], tags: Optional[list[str]], expires_at: Optional[datetime], is_rotatable: bool, rotation_interval_days: Optional[int], metadata: Optional[Dict[str, Any]]) -> str",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,CredentialMetadata | info | now | str | uuid4,httpx,pure,yes,40,Store credential in memory.
integrations,L5,vault,EnvCredentialVault.update_credential,"async update_credential(tenant_id: str, credential_id: str, secret_data: Optional[Dict[str, str]], description: Optional[str], tags: Optional[list[str]], expires_at: Optional[datetime], metadata: Optional[Dict[str, Any]]) -> bool",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,now,httpx,pure,yes,30,Update a credential.
integrations,L5,vault,HashiCorpVault.__init__,"__init__(vault_url: str, token: str, mount_path: str)",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,,httpx,pure,no,10,
integrations,L5,vault,HashiCorpVault.delete_credential,"async delete_credential(tenant_id: str, credential_id: str) -> bool",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,AsyncClient | delete | pop,httpx,"db_write,external_api",yes,25,Delete a credential.
integrations,L5,vault,HashiCorpVault.get_credential,"async get_credential(tenant_id: str, credential_id: str) -> Optional[CredentialData]",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,AsyncClient | CredentialData | CredentialMetadata | CredentialType | fromisoformat | get | json | now | pop | raise_for_status,httpx,external_api,yes,53,Get credential from Vault.
integrations,L5,vault,HashiCorpVault.get_metadata,"async get_metadata(tenant_id: str, credential_id: str) -> Optional[CredentialMetadata]",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,get_credential,httpx,pure,yes,15,Get credential metadata without secrets.
integrations,L5,vault,HashiCorpVault.list_credentials,"async list_credentials(tenant_id: str, credential_type: Optional[CredentialType], tags: Optional[list[str]]) -> list[CredentialMetadata]",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,AsyncClient | all | append | get | get_metadata | json | raise_for_status | request,httpx,external_api,yes,44,List credentials for tenant.
integrations,L5,vault,HashiCorpVault.rotate_credential,"async rotate_credential(tenant_id: str, credential_id: str, new_secret_data: Dict[str, str]) -> bool",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,update_credential,httpx,pure,yes,12,Rotate credential secrets.
integrations,L5,vault,HashiCorpVault.store_credential,"async store_credential(tenant_id: str, name: str, credential_type: CredentialType, secret_data: Dict[str, str], description: Optional[str], tags: Optional[list[str]], expires_at: Optional[datetime], is_rotatable: bool, rotation_interval_days: Optional[int], metadata: Optional[Dict[str, Any]]) -> str",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,AsyncClient | CredentialMetadata | info | isoformat | now | post | raise_for_status | str | uuid4,httpx,external_api,yes,65,Store credential in Vault.
integrations,L5,vault,HashiCorpVault.update_credential,"async update_credential(tenant_id: str, credential_id: str, secret_data: Optional[Dict[str, str]], description: Optional[str], tags: Optional[list[str]], expires_at: Optional[datetime], metadata: Optional[Dict[str, Any]]) -> bool",L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,AsyncClient | get_credential | isoformat | now | post | raise_for_status,httpx,external_api,yes,60,Update a credential.
integrations,L5,vault,create_credential_vault,create_credential_vault() -> CredentialVault,L7:cus_models | ?:cus_schemas | ?:__init__ | ?:service | L5s:cus_schemas,EnvCredentialVault | HashiCorpVault | getenv | info | warning,httpx,pure,no,21,Factory function to create appropriate vault based on configuration.
integrations,L6,bridges_driver,record_policy_activation,"async record_policy_activation(db_factory, policy_id: str, source_pattern_id: str, source_recovery_id: str, confidence: float, approval_path: str, loop_trace_id: str, tenant_id: str) -> PolicyActivationAudit",L6:__init__ | L5:bridges_engine,PolicyActivationAudit | commit | db_factory | execute | info | now | text,__future__ | audit_schemas | loop_events | sqlalchemy,db_write,yes,63,Record policy activation for audit trail.
integrations,L6,connector_registry_driver,BaseConnector.__init__,"__init__(connector_id: str, tenant_id: str, name: str, connector_type: ConnectorType, config: Optional[ConnectorConfig])",L5:connectors_facade,ConnectorConfig | now,,pure,no,25,
integrations,L6,connector_registry_driver,BaseConnector.connect,connect() -> bool,L5:connectors_facade,,,pure,no,3,Establish connection to the service.
integrations,L6,connector_registry_driver,BaseConnector.disconnect,disconnect() -> bool,L5:connectors_facade,,,pure,no,3,Disconnect from the service.
integrations,L6,connector_registry_driver,BaseConnector.health_check,health_check() -> bool,L5:connectors_facade,,,pure,no,3,Check if connector is healthy.
integrations,L6,connector_registry_driver,BaseConnector.record_connection,record_connection(now: Optional[datetime]) -> None,L5:connectors_facade,now,,pure,no,7,Record a successful connection.
integrations,L6,connector_registry_driver,BaseConnector.record_error,"record_error(error: str, now: Optional[datetime]) -> None",L5:connectors_facade,now,,pure,no,7,Record a connection error.
integrations,L6,connector_registry_driver,BaseConnector.to_dict,"to_dict(include_secrets: bool) -> dict[str, Any]",L5:connectors_facade,isoformat | to_dict,,pure,no,20,Convert to dictionary.
integrations,L6,connector_registry_driver,ConnectorConfig.to_dict,"to_dict(include_secrets: bool) -> dict[str, Any]",L5:connectors_facade,,,pure,no,18,Convert to dictionary.
integrations,L6,connector_registry_driver,ConnectorError.__init__,"__init__(message: str, connector_id: Optional[str], connector_type: Optional[ConnectorType])",L5:connectors_facade,__init__ | super,,pure,no,10,
integrations,L6,connector_registry_driver,ConnectorError.to_dict,"to_dict() -> dict[str, Any]",L5:connectors_facade,,,pure,no,9,Convert to dictionary.
integrations,L6,connector_registry_driver,ConnectorRegistry.__init__,__init__(),L5:connectors_facade,,,pure,no,4,Initialize the registry.
integrations,L6,connector_registry_driver,ConnectorRegistry.clear_tenant,clear_tenant(tenant_id: str) -> int,L5:connectors_facade,delete | get | len | list | set,,db_write,no,6,Clear all connectors for a tenant.
integrations,L6,connector_registry_driver,ConnectorRegistry.create_file_connector,"create_file_connector(tenant_id: str, name: str, config: Optional[ConnectorConfig], storage_type: str, base_path: str, connector_id: Optional[str]) -> FileConnector",L5:connectors_facade,FileConnector | register | str | uuid4,,pure,no,19,Create and register a file connector.
integrations,L6,connector_registry_driver,ConnectorRegistry.create_serverless_connector,"create_serverless_connector(tenant_id: str, name: str, config: Optional[ConnectorConfig], platform: str, region: str, connector_id: Optional[str]) -> ServerlessConnector",L5:connectors_facade,ServerlessConnector | register | str | uuid4,,pure,no,19,Create and register a serverless connector.
integrations,L6,connector_registry_driver,ConnectorRegistry.create_vector_connector,"create_vector_connector(tenant_id: str, name: str, config: Optional[ConnectorConfig], vector_dimension: int, connector_id: Optional[str]) -> VectorConnector",L5:connectors_facade,VectorConnector | register | str | uuid4,,pure,no,17,Create and register a vector connector.
integrations,L6,connector_registry_driver,ConnectorRegistry.delete,delete(connector_id: str) -> bool,L5:connectors_facade,discard | get,,pure,no,13,Delete a connector.
integrations,L6,connector_registry_driver,ConnectorRegistry.get,get(connector_id: str) -> Optional[BaseConnector],L5:connectors_facade,get,,pure,no,3,Get a connector by ID.
integrations,L6,connector_registry_driver,ConnectorRegistry.get_by_name,"get_by_name(tenant_id: str, name: str) -> Optional[BaseConnector]",L5:connectors_facade,values,,pure,no,10,Get a connector by name within a tenant.
integrations,L6,connector_registry_driver,ConnectorRegistry.get_statistics,get_statistics(tenant_id: Optional[str]) -> ConnectorStats,L5:connectors_facade,ConnectorStats | get | values,,pure,no,28,Get registry statistics.
integrations,L6,connector_registry_driver,ConnectorRegistry.list,"list(tenant_id: Optional[str], connector_type: Optional[ConnectorType], status: Optional[ConnectorStatus], limit: int, offset: int) -> list[BaseConnector]",L5:connectors_facade,list | sort | values,,pure,no,23,List connectors with optional filters.
integrations,L6,connector_registry_driver,ConnectorRegistry.register,register(connector: BaseConnector) -> BaseConnector,L5:connectors_facade,add | set,,db_write,no,14,Register a connector.
integrations,L6,connector_registry_driver,ConnectorRegistry.reset,reset() -> None,L5:connectors_facade,clear,,pure,no,4,Reset all state (for testing).
integrations,L6,connector_registry_driver,ConnectorStats.to_dict,"to_dict() -> dict[str, Any]",L5:connectors_facade,,,pure,no,11,Convert to dictionary.
integrations,L6,connector_registry_driver,FileConnector.__init__,"__init__(connector_id: str, tenant_id: str, name: str, config: Optional[ConnectorConfig], storage_type: str, base_path: str)",L5:connectors_facade,__init__ | super,,pure,no,24,
integrations,L6,connector_registry_driver,FileConnector.connect,connect() -> bool,L5:connectors_facade,record_connection | record_error | str,,pure,no,9,Connect to file storage.
integrations,L6,connector_registry_driver,FileConnector.delete_file,"delete_file(path: str) -> dict[str, Any]",L5:connectors_facade,ConnectorError,,pure,no,13,Delete a file.
integrations,L6,connector_registry_driver,FileConnector.disconnect,disconnect() -> bool,L5:connectors_facade,now,,pure,no,6,Disconnect from file storage.
integrations,L6,connector_registry_driver,FileConnector.health_check,health_check() -> bool,L5:connectors_facade,,,pure,no,3,Check file storage health.
integrations,L6,connector_registry_driver,FileConnector.list_files,"list_files(path: str, recursive: bool) -> list[dict[str, Any]]",L5:connectors_facade,ConnectorError | range,,pure,no,18,List files in a directory.
integrations,L6,connector_registry_driver,FileConnector.read_file,read_file(path: str) -> bytes,L5:connectors_facade,ConnectorError,,pure,no,11,Read a file.
integrations,L6,connector_registry_driver,FileConnector.to_dict,"to_dict(include_secrets: bool) -> dict[str, Any]",L5:connectors_facade,super | to_dict,,pure,no,6,Convert to dictionary with file-specific fields.
integrations,L6,connector_registry_driver,FileConnector.write_file,"write_file(path: str, content: bytes) -> dict[str, Any]",L5:connectors_facade,ConnectorError | len,,pure,no,14,Write a file.
integrations,L6,connector_registry_driver,ServerlessConnector.__init__,"__init__(connector_id: str, tenant_id: str, name: str, config: Optional[ConnectorConfig], platform: str, region: str)",L5:connectors_facade,__init__ | super,,pure,no,23,
integrations,L6,connector_registry_driver,ServerlessConnector.connect,connect() -> bool,L5:connectors_facade,record_connection | record_error | str,,pure,no,9,Connect to serverless platform.
integrations,L6,connector_registry_driver,ServerlessConnector.disconnect,disconnect() -> bool,L5:connectors_facade,now,,pure,no,6,Disconnect from serverless platform.
integrations,L6,connector_registry_driver,ServerlessConnector.get_result,"get_result(request_id: str) -> dict[str, Any]",L5:connectors_facade,ConnectorError,,pure,no,15,Get async invocation result.
integrations,L6,connector_registry_driver,ServerlessConnector.health_check,health_check() -> bool,L5:connectors_facade,,,pure,no,3,Check serverless platform health.
integrations,L6,connector_registry_driver,ServerlessConnector.invoke,"invoke(function_name: str, payload: dict[str, Any], async_invoke: bool) -> dict[str, Any]",L5:connectors_facade,ConnectorError | str | uuid4,,pure,no,21,Invoke a serverless function.
integrations,L6,connector_registry_driver,ServerlessConnector.list_functions,"list_functions() -> list[dict[str, Any]]",L5:connectors_facade,ConnectorError | range,,pure,no,14,List available functions.
integrations,L6,connector_registry_driver,ServerlessConnector.to_dict,"to_dict(include_secrets: bool) -> dict[str, Any]",L5:connectors_facade,super | to_dict,,pure,no,6,Convert to dictionary with serverless-specific fields.
integrations,L6,connector_registry_driver,VectorConnector.__init__,"__init__(connector_id: str, tenant_id: str, name: str, config: Optional[ConnectorConfig], vector_dimension: int, distance_metric: str)",L5:connectors_facade,__init__ | super,,pure,no,25,
integrations,L6,connector_registry_driver,VectorConnector.connect,connect() -> bool,L5:connectors_facade,record_connection | record_error | str,,pure,no,10,Connect to vector database.
integrations,L6,connector_registry_driver,VectorConnector.delete_vectors,"delete_vectors(ids: list[str]) -> dict[str, Any]",L5:connectors_facade,ConnectorError | len,,pure,no,13,Delete vectors by ID.
integrations,L6,connector_registry_driver,VectorConnector.disconnect,disconnect() -> bool,L5:connectors_facade,now,,pure,no,6,Disconnect from vector database.
integrations,L6,connector_registry_driver,VectorConnector.health_check,health_check() -> bool,L5:connectors_facade,,,pure,no,3,Check vector database health.
integrations,L6,connector_registry_driver,VectorConnector.search,"search(query_vector: list[float], top_k: int, filter_metadata: Optional[dict[str, Any]]) -> list[dict[str, Any]]",L5:connectors_facade,ConnectorError | min | range,,pure,no,19,Search for similar vectors.
integrations,L6,connector_registry_driver,VectorConnector.to_dict,"to_dict(include_secrets: bool) -> dict[str, Any]",L5:connectors_facade,super | to_dict,,pure,no,6,Convert to dictionary with vector-specific fields.
integrations,L6,connector_registry_driver,VectorConnector.upsert_vectors,"upsert_vectors(vectors: list[dict[str, Any]]) -> dict[str, Any]",L5:connectors_facade,ConnectorError | len,,pure,no,17,Upsert vectors to the database.
integrations,L6,connector_registry_driver,_reset_registry,_reset_registry() -> None,L5:connectors_facade,reset,,pure,no,6,Reset the singleton (for testing).
integrations,L6,connector_registry_driver,get_connector,get_connector(connector_id: str) -> Optional[BaseConnector],L5:connectors_facade,get | get_connector_registry,,pure,no,4,Get a connector by ID using the singleton registry.
integrations,L6,connector_registry_driver,get_connector_registry,get_connector_registry() -> ConnectorRegistry,L5:connectors_facade,ConnectorRegistry,,pure,no,6,Get the singleton registry instance.
integrations,L6,connector_registry_driver,list_connectors,"list_connectors(tenant_id: Optional[str], connector_type: Optional[ConnectorType]) -> list[BaseConnector]",L5:connectors_facade,get_connector_registry | list,,pure,no,7,List connectors using the singleton registry.
integrations,L6,connector_registry_driver,register_connector,register_connector(connector: BaseConnector) -> BaseConnector,L5:connectors_facade,get_connector_registry | register,,pure,no,4,Register a connector using the singleton registry.
integrations,L6,external_response_driver,ExternalResponseService.__init__,__init__(session: Session),,,external_response | orm | sqlalchemy,pure,no,2,
integrations,L6,external_response_driver,ExternalResponseService.get_interpreted,get_interpreted(response_id: UUID) -> Optional[InterpretedResponse],,InterpretedResponse | and_ | execute | is_not | scalar_one_or_none | select | where,external_response | orm | sqlalchemy,db_write,no,35,Get interpreted response for consumers (L5/L2  L6 read).
integrations,L6,external_response_driver,ExternalResponseService.get_pending_interpretations,"get_pending_interpretations(interpretation_owner: str, limit: int) -> list[ExternalResponse]",,all | and_ | execute | is_ | limit | list | order_by | scalars | select | where,external_response | orm | sqlalchemy,db_write,no,29,Get responses pending interpretation by owner.
integrations,L6,external_response_driver,ExternalResponseService.get_raw_for_interpretation,"get_raw_for_interpretation(response_id: UUID, expected_owner: str) -> Optional[ExternalResponse]",,and_ | execute | scalar_one_or_none | select | where,external_response | orm | sqlalchemy,db_write,no,25,Get raw response for L4 interpretation.
integrations,L6,external_response_driver,ExternalResponseService.interpret,"interpret(response_id: UUID, interpreted_value: dict, interpreted_by: str) -> ExternalResponse",,execute | flush | now | returning | scalar_one | update | values | where,external_response | orm | sqlalchemy,db_write,no,36,Record L4 engine interpretation (L4  L6 write).
integrations,L6,external_response_driver,ExternalResponseService.record_raw_response,"record_raw_response(source: str, raw_response: dict, interpretation_owner: str, interpretation_contract: Optional[str], request_id: Optional[str], run_id: Optional[str]) -> ExternalResponse",,ExternalResponse | add | flush | now,external_response | orm | sqlalchemy,db_write,no,42,Record a raw external response (L3  L6 write).
integrations,L6,external_response_driver,get_interpreted_response,"get_interpreted_response(session: Session, response_id: UUID) -> Optional[InterpretedResponse]",,ExternalResponseService | get_interpreted,external_response | orm | sqlalchemy,pure,no,7,Get interpreted response for consumers (L5/L2  L6).
integrations,L6,external_response_driver,interpret_response,"interpret_response(session: Session, response_id: UUID, interpreted_value: dict, interpreted_by: str) -> ExternalResponse",,ExternalResponseService | interpret,external_response | orm | sqlalchemy,pure,no,13,Record L4 engine interpretation (L4  L6).
integrations,L6,external_response_driver,record_external_response,"record_external_response(session: Session, source: str, raw_response: dict, interpretation_owner: str, interpretation_contract: Optional[str], request_id: Optional[str], run_id: Optional[str]) -> ExternalResponse",,ExternalResponseService | record_raw_response,external_response | orm | sqlalchemy,pure,no,19,Record a raw external response (L3  L6).
integrations,L6,worker_registry_driver,WorkerRegistryService.__init__,__init__(session: Session),,,sqlmodel | tenant,pure,no,2,
integrations,L6,worker_registry_driver,WorkerRegistryService.deprecate_worker,deprecate_worker(worker_id: str) -> WorkerRegistry,,update_worker_status,sqlmodel | tenant,pure,no,3,Mark a worker as deprecated.
integrations,L6,worker_registry_driver,WorkerRegistryService.get_effective_worker_config,"get_effective_worker_config(tenant_id: str, worker_id: str) -> Dict[str, Any]",,get_tenant_worker_config | get_worker_or_raise | loads | update,sqlmodel | tenant,pure,no,44,"Get effective worker configuration, merging tenant overrides with defaults."
integrations,L6,worker_registry_driver,WorkerRegistryService.get_tenant_worker_config,"get_tenant_worker_config(tenant_id: str, worker_id: str) -> Optional[WorkerConfig]",,exec | first | select | where,sqlmodel | tenant,pure,no,11,Get tenant-specific worker configuration.
integrations,L6,worker_registry_driver,WorkerRegistryService.get_worker,get_worker(worker_id: str) -> Optional[WorkerRegistry],,get,sqlmodel | tenant,pure,no,3,Get a worker by ID.
integrations,L6,worker_registry_driver,WorkerRegistryService.get_worker_details,"get_worker_details(worker_id: str) -> Dict[str, Any]",,get_worker_or_raise | isoformat | loads,sqlmodel | tenant,pure,no,49,Get detailed worker information including schemas.
integrations,L6,worker_registry_driver,WorkerRegistryService.get_worker_or_raise,get_worker_or_raise(worker_id: str) -> WorkerRegistry,,WorkerNotFoundError | get_worker,sqlmodel | tenant,pure,no,6,"Get a worker by ID, raising if not found."
integrations,L6,worker_registry_driver,WorkerRegistryService.get_worker_summary,"get_worker_summary(worker_id: str) -> Dict[str, Any]",,get_worker_or_raise | loads,sqlmodel | tenant,pure,no,21,Get summary worker information (for listings).
integrations,L6,worker_registry_driver,WorkerRegistryService.get_workers_for_tenant,"get_workers_for_tenant(tenant_id: str, include_disabled: bool) -> List[Dict[str, Any]]",,append | bool | get_effective_worker_config | list_available_workers | loads,sqlmodel | tenant,pure,no,41,Get all workers available to a tenant with their effective configs.
integrations,L6,worker_registry_driver,WorkerRegistryService.is_worker_available,is_worker_available(worker_id: str) -> bool,,get_worker,sqlmodel | tenant,pure,no,6,Check if a worker is available for execution.
integrations,L6,worker_registry_driver,WorkerRegistryService.is_worker_enabled_for_tenant,"is_worker_enabled_for_tenant(tenant_id: str, worker_id: str) -> bool",,get_tenant_worker_config,sqlmodel | tenant,pure,no,7,Check if a worker is enabled for a tenant.
integrations,L6,worker_registry_driver,WorkerRegistryService.list_available_workers,list_available_workers() -> List[WorkerRegistry],,list_workers,sqlmodel | tenant,pure,no,3,List only available (runnable) workers.
integrations,L6,worker_registry_driver,WorkerRegistryService.list_tenant_worker_configs,list_tenant_worker_configs(tenant_id: str) -> List[WorkerConfig],,exec | list | select | where,sqlmodel | tenant,pure,no,4,List all worker configurations for a tenant.
integrations,L6,worker_registry_driver,WorkerRegistryService.list_worker_summaries,"list_worker_summaries(status: Optional[str], public_only: bool) -> List[Dict[str, Any]]",,get_worker_summary | list_workers,sqlmodel | tenant,pure,no,8,List worker summaries.
integrations,L6,worker_registry_driver,WorkerRegistryService.list_workers,"list_workers(status: Optional[str], public_only: bool) -> List[WorkerRegistry]",,exec | list | order_by | select | where,sqlmodel | tenant,pure,no,13,"List all workers, optionally filtered."
integrations,L6,worker_registry_driver,WorkerRegistryService.register_worker,"register_worker(worker_id: str, name: str, description: Optional[str], version: str, status: str, is_public: bool, moats: Optional[List[str]], default_config: Optional[Dict], input_schema: Optional[Dict], output_schema: Optional[Dict], tokens_per_run_estimate: Optional[int], cost_per_run_cents: Optional[int]) -> WorkerRegistry",,WorkerRegistry | WorkerRegistryError | add | dumps | flush | get_worker | info | refresh,sqlmodel | tenant,db_write,no,41,Register a new worker.
integrations,L6,worker_registry_driver,WorkerRegistryService.set_tenant_worker_config,"set_tenant_worker_config(tenant_id: str, worker_id: str, enabled: bool, config: Optional[Dict], brand: Optional[Dict], max_runs_per_day: Optional[int], max_tokens_per_run: Optional[int]) -> WorkerConfig",,WorkerConfig | add | dumps | flush | get_tenant_worker_config | get_worker_or_raise | refresh,sqlmodel | tenant,db_write,no,42,Set or update tenant-specific worker configuration.
integrations,L6,worker_registry_driver,WorkerRegistryService.update_worker_status,"update_worker_status(worker_id: str, status: str) -> WorkerRegistry",,add | flush | get_worker_or_raise | info | refresh,sqlmodel | tenant,db_write,no,9,Update worker status.
integrations,L6,worker_registry_driver,get_worker_registry_service,get_worker_registry_service(session: Session) -> WorkerRegistryService,,WorkerRegistryService,sqlmodel | tenant,pure,no,3,Get a WorkerRegistryService instance.
logs,L5,audit_engine,AuditChecks._is_health_degraded,"_is_health_degraded(before: Any, after: Any) -> bool",L4:__init__,get | str,contract,pure,no,9,Check if health status degraded.
logs,L5,audit_engine,AuditChecks.check_execution_fidelity,check_execution_fidelity(audit_input: AuditInput) -> AuditCheck,L4:__init__,AuditCheck | add | get | isinstance | list | set,contract,db_write,no,60,A-003: Execution Fidelity
logs,L5,audit_engine,AuditChecks.check_health_preservation,check_health_preservation(audit_input: AuditInput) -> AuditCheck,L4:__init__,AuditCheck | _is_health_degraded | append | get | items,contract,pure,no,63,A-002: Health Preservation
logs,L5,audit_engine,AuditChecks.check_no_unauthorized_mutations,check_no_unauthorized_mutations(audit_input: AuditInput) -> AuditCheck,L4:__init__,AuditCheck | append | get | keys | list | set,contract,pure,no,51,A-007: No Unauthorized Mutations
logs,L5,audit_engine,AuditChecks.check_rollback_availability,check_rollback_availability(audit_input: AuditInput) -> AuditCheck,L4:__init__,AuditCheck | append | enumerate | get,contract,pure,no,63,A-005: Rollback Availability
logs,L5,audit_engine,AuditChecks.check_scope_compliance,check_scope_compliance(audit_input: AuditInput) -> AuditCheck,L4:__init__,AuditCheck | add | get | list | set,contract,db_write,no,46,A-001: Scope Compliance
logs,L5,audit_engine,AuditChecks.check_signal_consistency,check_signal_consistency(audit_input: AuditInput) -> AuditCheck,L4:__init__,AuditCheck,contract,pure,no,33,A-006: Signal Consistency
logs,L5,audit_engine,AuditChecks.check_timing_compliance,check_timing_compliance(audit_input: AuditInput) -> AuditCheck,L4:__init__,AuditCheck | isoformat,contract,pure,no,62,A-004: Timing Compliance
logs,L5,audit_engine,AuditService.__init__,__init__(auditor_version: str),L4:__init__,,contract,pure,no,11,Initialize Audit Service.
logs,L5,audit_engine,AuditService._determine_verdict,"_determine_verdict(checks: list[AuditCheck]) -> tuple[AuditVerdict, str]",L4:__init__,join,contract,pure,no,36,Determine final verdict from check results.
logs,L5,audit_engine,AuditService._run_all_checks,_run_all_checks(audit_input: AuditInput) -> list[AuditCheck],L4:__init__,check_execution_fidelity | check_health_preservation | check_no_unauthorized_mutations | check_rollback_availability | check_scope_compliance | check_signal_consistency | check_timing_compliance,contract,pure,no,11,Run all audit checks and return results.
logs,L5,audit_engine,AuditService.audit,audit(audit_input: AuditInput) -> AuditResult,L4:__init__,AuditResult | _determine_verdict | _run_all_checks | int | len | now | str | sum | total_seconds | tuple | uuid4,contract,pure,no,59,Perform audit on completed job.
logs,L5,audit_engine,AuditService.version,version() -> str,L4:__init__,,contract,pure,no,3,Return auditor version.
logs,L5,audit_engine,RolloutGate.get_rollout_status,"get_rollout_status(verdict: AuditVerdict) -> dict[str, Any]",L4:__init__,,contract,pure,no,35,Get rollout status details.
logs,L5,audit_engine,RolloutGate.is_rollout_authorized,is_rollout_authorized(verdict: AuditVerdict) -> bool,L4:__init__,,contract,pure,no,13,Check if rollout is authorized based on verdict.
logs,L5,audit_engine,audit_result_to_record,"audit_result_to_record(result: AuditResult) -> dict[str, Any]",L4:__init__,isoformat | str,contract,pure,no,33,Convert AuditResult to database record format.
logs,L5,audit_engine,create_audit_input_from_evidence,"create_audit_input_from_evidence(job_id: UUID, contract_id: UUID, job_status: str, contract_scope: list[str], proposed_changes: dict[str, Any], execution_result: dict[str, Any], activation_window_start: Optional[datetime], activation_window_end: Optional[datetime]) -> AuditInput",L4:__init__,AuditInput | fromisoformat | get | now | replace,contract,pure,no,51,Create AuditInput from job execution evidence.
logs,L5,audit_evidence,MCPAuditEmitter.__init__,__init__(publisher: Optional[Any]),?:__init__,,events,pure,no,10,Initialize audit emitter.
logs,L5,audit_evidence,MCPAuditEmitter._emit,async _emit(event: MCPAuditEvent) -> None,?:__init__,_get_publisher | debug | error | info | publish | str | to_dict,events,pure,yes,48,Emit event to event bus and update chain.
logs,L5,audit_evidence,MCPAuditEmitter._generate_event_id,_generate_event_id() -> str,?:__init__,uuid4,events,pure,no,5,Generate unique event ID.
logs,L5,audit_evidence,MCPAuditEmitter._get_publisher,_get_publisher() -> Optional[Any],?:__init__,debug | get_publisher,events,pure,no,11,Get event publisher (lazy initialization).
logs,L5,audit_evidence,MCPAuditEmitter.emit_server_registered,"async emit_server_registered(tenant_id: str, server_id: str, server_name: str, url: str, tool_count: int) -> MCPAuditEvent",?:__init__,MCPAuditEvent | _emit | _generate_event_id | isoformat | now,events,pure,yes,39,Emit audit event when MCP server is registered.
logs,L5,audit_evidence,MCPAuditEmitter.emit_server_unregistered,"async emit_server_unregistered(tenant_id: str, server_id: str) -> MCPAuditEvent",?:__init__,MCPAuditEvent | _emit | _generate_event_id | isoformat | now,events,pure,yes,28,Emit audit event when MCP server is unregistered.
logs,L5,audit_evidence,MCPAuditEmitter.emit_tool_allowed,"async emit_tool_allowed(tenant_id: str, server_id: str, tool_name: str, run_id: str, policy_id: Optional[str], trace_id: Optional[str]) -> MCPAuditEvent",?:__init__,MCPAuditEvent | _emit | _generate_event_id | isoformat | now,events,pure,yes,39,Emit audit event when tool invocation is allowed.
logs,L5,audit_evidence,MCPAuditEmitter.emit_tool_completed,"async emit_tool_completed(tenant_id: str, server_id: str, tool_name: str, run_id: str, output: Optional[Any], duration_ms: Optional[float], span_id: Optional[str], trace_id: Optional[str]) -> MCPAuditEvent",?:__init__,MCPAuditEvent | _emit | _generate_event_id | _hash_value | isoformat | now,events,pure,yes,44,Emit audit event when tool execution completes successfully.
logs,L5,audit_evidence,MCPAuditEmitter.emit_tool_denied,"async emit_tool_denied(tenant_id: str, server_id: str, tool_name: str, run_id: str, deny_reason: str, policy_id: Optional[str], message: Optional[str], trace_id: Optional[str]) -> MCPAuditEvent",?:__init__,MCPAuditEvent | _emit | _generate_event_id | isoformat | now,events,pure,yes,45,Emit audit event when tool invocation is denied.
logs,L5,audit_evidence,MCPAuditEmitter.emit_tool_failed,"async emit_tool_failed(tenant_id: str, server_id: str, tool_name: str, run_id: str, error_message: str, duration_ms: Optional[float], span_id: Optional[str], trace_id: Optional[str]) -> MCPAuditEvent",?:__init__,MCPAuditEvent | _emit | _generate_event_id | isoformat | now,events,pure,yes,44,Emit audit event when tool execution fails.
logs,L5,audit_evidence,MCPAuditEmitter.emit_tool_requested,"async emit_tool_requested(tenant_id: str, server_id: str, tool_name: str, run_id: str, input_params: Optional[Dict[str, Any]], trace_id: Optional[str]) -> MCPAuditEvent",?:__init__,MCPAuditEvent | _emit | _generate_event_id | _hash_value | _redact_sensitive | isoformat | now,events,pure,yes,39,Emit audit event when tool invocation is requested.
logs,L5,audit_evidence,MCPAuditEmitter.emit_tool_started,"async emit_tool_started(tenant_id: str, server_id: str, tool_name: str, run_id: str, span_id: Optional[str], trace_id: Optional[str]) -> MCPAuditEvent",?:__init__,MCPAuditEvent | _emit | _generate_event_id | isoformat | now,events,pure,yes,38,Emit audit event when tool execution starts.
logs,L5,audit_evidence,MCPAuditEvent.__post_init__,__post_init__(),?:__init__,_compute_integrity_hash,events,pure,no,4,Compute integrity hash after initialization.
logs,L5,audit_evidence,MCPAuditEvent._compute_integrity_hash,_compute_integrity_hash() -> str,?:__init__,encode | hexdigest | sha256,events,pure,no,9,Compute tamper-evident integrity hash.
logs,L5,audit_evidence,MCPAuditEvent.to_dict,"to_dict() -> Dict[str, Any]",?:__init__,,events,pure,no,24,Convert to dictionary for serialization.
logs,L5,audit_evidence,MCPAuditEvent.verify_integrity,verify_integrity() -> bool,?:__init__,_compute_integrity_hash,events,pure,no,4,Verify the integrity hash is valid.
logs,L5,audit_evidence,_contains_sensitive,_contains_sensitive(key: str) -> bool,?:__init__,any | lower,events,pure,no,4,Check if key name suggests sensitive data.
logs,L5,audit_evidence,_hash_value,_hash_value(value: Any) -> str,?:__init__,encode | hexdigest | sha256 | str,events,pure,no,6,Hash a value for audit purposes.
logs,L5,audit_evidence,_redact_sensitive,"_redact_sensitive(data: Dict[str, Any]) -> Dict[str, Any]",?:__init__,_contains_sensitive | _redact_sensitive | isinstance | items,events,pure,no,19,Redact sensitive fields from data for logging.
logs,L5,audit_evidence,configure_mcp_audit_emitter,configure_mcp_audit_emitter(publisher: Optional[Any]) -> MCPAuditEmitter,?:__init__,MCPAuditEmitter | info,events,pure,no,22,Configure the singleton MCPAuditEmitter.
logs,L5,audit_evidence,get_mcp_audit_emitter,get_mcp_audit_emitter() -> MCPAuditEmitter,?:__init__,MCPAuditEmitter | info,events,pure,no,14,Get or create the singleton MCPAuditEmitter.
logs,L5,audit_evidence,reset_mcp_audit_emitter,reset_mcp_audit_emitter() -> None,?:__init__,,events,pure,no,4,Reset the singleton (for testing).
logs,L5,audit_ledger_engine,AuditLedgerService.__init__,__init__(session: 'Session'),?:incident_write_engine | ?:test_import_surface_sentinels,,audit_ledger | sqlmodel,pure,no,3,Initialize with sync database session.
logs,L5,audit_ledger_engine,AuditLedgerService._emit,"_emit(tenant_id: str, event_type: AuditEventType, entity_type: AuditEntityType, entity_id: str, actor_type: ActorType, actor_id: Optional[str], reason: Optional[str], before_state: Optional[Dict[str, Any]], after_state: Optional[Dict[str, Any]]) -> AuditLedger",?:incident_write_engine | ?:test_import_surface_sentinels,AuditLedger | add | flush | info,audit_ledger | sqlmodel,db_write,no,56,Emit an audit event to the ledger.
logs,L5,audit_ledger_engine,AuditLedgerService.incident_acknowledged,"incident_acknowledged(tenant_id: str, incident_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], incident_state: Optional[Dict[str, Any]]) -> AuditLedger",?:incident_write_engine | ?:test_import_surface_sentinels,_emit,audit_ledger | sqlmodel,pure,no,20,Record an incident acknowledgment event.
logs,L5,audit_ledger_engine,AuditLedgerService.incident_manually_closed,"incident_manually_closed(tenant_id: str, incident_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], incident_state: Optional[Dict[str, Any]]) -> AuditLedger",?:incident_write_engine | ?:test_import_surface_sentinels,_emit,audit_ledger | sqlmodel,pure,no,20,Record an incident manual closure event.
logs,L5,audit_ledger_engine,AuditLedgerService.incident_resolved,"incident_resolved(tenant_id: str, incident_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], before_state: Optional[Dict[str, Any]], after_state: Optional[Dict[str, Any]]) -> AuditLedger",?:incident_write_engine | ?:test_import_surface_sentinels,_emit,audit_ledger | sqlmodel,pure,no,22,Record an incident resolution event.
logs,L5,audit_ledger_engine,get_audit_ledger_service,get_audit_ledger_service(session: 'Session') -> AuditLedgerService,?:incident_write_engine | ?:test_import_surface_sentinels,AuditLedgerService,audit_ledger | sqlmodel,pure,no,11,Get an AuditLedgerService instance.
logs,L5,audit_reconciler,AuditReconciler.__init__,__init__(store: Optional[AuditStore]),?:run_orchestration_kernel,get_audit_store,audit_store | prometheus_client | rac_models,pure,no,8,Initialize the reconciler.
logs,L5,audit_reconciler,AuditReconciler._record_metrics,_record_metrics(result: ReconciliationResult) -> None,?:run_orchestration_kernel,inc | labels,audit_store | prometheus_client | rac_models,pure,no,16,Record Prometheus metrics for reconciliation.
logs,L5,audit_reconciler,AuditReconciler.check_deadline_violations,check_deadline_violations(run_id: UUID) -> List[AuditExpectation],?:run_orchestration_kernel,append | get_expectations | len | now | str | timestamp | warning,audit_store | prometheus_client | rac_models,pure,no,33,Check for expectations that have exceeded their deadlines.
logs,L5,audit_reconciler,AuditReconciler.get_run_audit_summary,get_run_audit_summary(run_id: UUID) -> dict,?:run_orchestration_kernel,get_acks | get_expectations | len | str,audit_store | prometheus_client | rac_models,pure,no,38,Get a summary of the audit state for a run.
logs,L5,audit_reconciler,AuditReconciler.reconcile,reconcile(run_id: UUID) -> ReconciliationResult,?:run_orchestration_kernel,ReconciliationResult | _record_metrics | any | get_acks | get_expectations | info | key | len | list | now | str | total_seconds,audit_store | prometheus_client | rac_models,pure,no,91,Reconcile expectations against acknowledgments for a run.
logs,L5,audit_reconciler,get_audit_reconciler,get_audit_reconciler(store: Optional[AuditStore]) -> AuditReconciler,?:run_orchestration_kernel,AuditReconciler,audit_store | prometheus_client | rac_models,pure,no,14,Get the audit reconciler singleton.
logs,L5,certificate,Certificate.from_dict,"from_dict(data: Dict[str, Any]) -> 'Certificate'",?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,CertificatePayload | CertificateType | cls | get,replay_determinism,pure,no,27,Create certificate from dict.
logs,L5,certificate,Certificate.to_dict,"to_dict() -> Dict[str, Any]",?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,to_dict,replay_determinism,pure,no,7,Convert to full certificate dict.
logs,L5,certificate,Certificate.to_json,to_json() -> str,?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,dumps | to_dict,replay_determinism,pure,no,3,Convert to JSON string.
logs,L5,certificate,CertificatePayload.canonical_json,canonical_json() -> str,?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,dumps | to_dict,replay_determinism,pure,no,3,Canonical JSON for deterministic signing.
logs,L5,certificate,CertificatePayload.to_dict,"to_dict() -> Dict[str, Any]",?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,,replay_determinism,pure,no,21,Convert to dictionary for signing.
logs,L5,certificate,CertificateService.__init__,__init__(secret: Optional[str]),?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,getenv | token_hex,replay_determinism,pure,no,14,Initialize certificate service.
logs,L5,certificate,CertificateService._sign,_sign(content: str) -> str,?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,encode | hexdigest | new,replay_determinism,pure,no,4,Sign content with HMAC-SHA256.
logs,L5,certificate,CertificateService._verify_signature,"_verify_signature(content: str, signature: str) -> bool",?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,_sign | compare_digest,replay_determinism,pure,no,4,Verify HMAC signature.
logs,L5,certificate,CertificateService.create_policy_audit_certificate,"create_policy_audit_certificate(incident_id: str, policy_decisions: List[Dict[str, Any]], tenant_id: Optional[str]) -> Certificate",?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,Certificate | CertificatePayload | _sign | canonical_json | get | len | str | sum | uuid4,replay_determinism,pure,no,40,Create a certificate proving policy evaluation at a point in time.
logs,L5,certificate,CertificateService.create_replay_certificate,"create_replay_certificate(call_id: str, validation_result: ReplayResult, level: DeterminismLevel, tenant_id: Optional[str], user_id: Optional[str], request_hash: Optional[str], response_hash: Optional[str]) -> Certificate",?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,Certificate | CertificatePayload | _sign | canonical_json | len | str | sum | uuid4,replay_determinism,pure,no,56,Create a certificate proving deterministic replay.
logs,L5,certificate,CertificateService.export_certificate,"export_certificate(certificate: Certificate, format: str) -> str",?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,ValueError | dumps | join | to_dict | to_json,replay_determinism,pure,no,39,Export certificate in various formats.
logs,L5,certificate,CertificateService.verify_certificate,"verify_certificate(certificate: Certificate) -> Dict[str, Any]",?:guard | ?:certificate | L4:logs_handler | ?:apply | ?:mypy_zones,_verify_signature | canonical_json | fromisoformat | now | replace,replay_determinism,pure,no,28,Verify a certificate's signature and validity.
logs,L5,completeness_checker,CompletenessCheckResponse.to_dict,"to_dict() -> Dict[str, Any]",?:__init__,sorted,,pure,no,12,Convert to dictionary for API responses.
logs,L5,completeness_checker,EvidenceCompletenessChecker.__init__,"__init__(validation_enabled: bool, strict_mode: bool)",?:__init__,,,pure,no,14,Initialize the completeness checker.
logs,L5,completeness_checker,EvidenceCompletenessChecker.check,"check(bundle: Any, export_type: str) -> CompletenessCheckResponse",?:__init__,CompletenessCheckResponse | add | get_required_fields | is_field_present | len | set | sorted,,db_write,no,89,Check if a bundle is complete for PDF generation.
logs,L5,completeness_checker,EvidenceCompletenessChecker.ensure_complete,"ensure_complete(bundle: Any, export_type: str) -> None",?:__init__,EvidenceCompletenessError | check | len | sorted,,pure,no,31,Ensure bundle is complete or raise error.
logs,L5,completeness_checker,EvidenceCompletenessChecker.from_governance_config,from_governance_config(config: Any) -> 'EvidenceCompletenessChecker',?:__init__,cls | getattr,,pure,no,13,Create checker from GovernanceConfig.
logs,L5,completeness_checker,EvidenceCompletenessChecker.get_completeness_summary,"get_completeness_summary(bundle: Any, export_type: str) -> Dict[str, Any]",?:__init__,get_required_fields | is_field_present | len | sum | values,,pure,no,40,Get a summary of bundle completeness for reporting.
logs,L5,completeness_checker,EvidenceCompletenessChecker.get_field_value,"get_field_value(bundle: Any, field_name: str) -> Any",?:__init__,get | getattr | isinstance,,pure,no,14,Get field value from bundle (dict or object).
logs,L5,completeness_checker,EvidenceCompletenessChecker.get_required_fields,get_required_fields(export_type: str) -> FrozenSet[str],?:__init__,frozenset,,pure,no,29,Get required fields for an export type.
logs,L5,completeness_checker,EvidenceCompletenessChecker.is_field_present,"is_field_present(bundle: Any, field_name: str) -> bool",?:__init__,get_field_value | isinstance | len | strip,,pure,no,26,Check if a field is present and non-empty.
logs,L5,completeness_checker,EvidenceCompletenessChecker.should_allow_export,"should_allow_export(bundle: Any, export_type: str) -> tuple[bool, str]",?:__init__,check,,pure,no,26,Check if an export should be allowed.
logs,L5,completeness_checker,EvidenceCompletenessChecker.strict_mode,strict_mode() -> bool,?:__init__,,,pure,no,3,Check if strict mode is enabled.
logs,L5,completeness_checker,EvidenceCompletenessChecker.validation_enabled,validation_enabled() -> bool,?:__init__,,,pure,no,3,Check if validation is enabled.
logs,L5,completeness_checker,EvidenceCompletenessError.__init__,"__init__(message: str, missing_fields: Set[str], export_type: str, validation_enabled: bool)",?:__init__,__init__ | super,,pure,no,11,
logs,L5,completeness_checker,EvidenceCompletenessError.to_dict,"to_dict() -> Dict[str, Any]",?:__init__,sorted | str,,pure,no,9,Convert to dictionary for logging/API responses.
logs,L5,completeness_checker,check_evidence_completeness,"check_evidence_completeness(bundle: Any, export_type: str, validation_enabled: bool, strict_mode: bool) -> CompletenessCheckResponse",?:__init__,EvidenceCompletenessChecker | check,,pure,no,23,Quick helper to check evidence completeness.
logs,L5,completeness_checker,ensure_evidence_completeness,"ensure_evidence_completeness(bundle: Any, export_type: str, validation_enabled: bool, strict_mode: bool) -> None",?:__init__,EvidenceCompletenessChecker | ensure_complete,,pure,no,23,Quick helper to ensure evidence completeness or raise error.
logs,L5,evidence_facade,EvidenceChain.to_dict,"to_dict() -> Dict[str, Any]",L4:logs_handler | L5:__init__,to_dict,,pure,no,12,Convert to dictionary.
logs,L5,evidence_facade,EvidenceExport.to_dict,"to_dict() -> Dict[str, Any]",L4:logs_handler | L5:__init__,,,pure,no,13,Convert to dictionary.
logs,L5,evidence_facade,EvidenceFacade.__init__,__init__(),L4:logs_handler | L5:__init__,,,pure,no,5,Initialize facade.
logs,L5,evidence_facade,EvidenceFacade._create_link,"_create_link(evidence_type: str, data: Dict[str, Any], previous_hash: Optional[str]) -> EvidenceLink",L4:logs_handler | L5:__init__,EvidenceLink | _hash_data | isoformat | now | str | uuid4,,pure,no,26,Create a new evidence link.
logs,L5,evidence_facade,EvidenceFacade._hash_data,"_hash_data(data: Dict[str, Any]) -> str",L4:logs_handler | L5:__init__,dumps | encode | hexdigest | sha256,,pure,no,4,Create deterministic hash of data.
logs,L5,evidence_facade,EvidenceFacade.add_evidence,"async add_evidence(chain_id: str, tenant_id: str, evidence_type: str, data: Dict[str, Any]) -> Optional[EvidenceChain]",L4:logs_handler | L5:__init__,_create_link | append | get | info | len,,pure,yes,42,Add evidence to a chain.
logs,L5,evidence_facade,EvidenceFacade.create_chain,"async create_chain(tenant_id: str, run_id: Optional[str], initial_evidence: Optional[Dict[str, Any]]) -> EvidenceChain",L4:logs_handler | L5:__init__,EvidenceChain | _create_link | _hash_data | append | get | info | isoformat | len | now | str | uuid4,,pure,yes,49,Create a new evidence chain.
logs,L5,evidence_facade,EvidenceFacade.create_export,"async create_export(tenant_id: str, chain_id: str, format: str) -> EvidenceExport",L4:logs_handler | L5:__init__,EvidenceExport | get | info | isoformat | now | str | uuid4,,pure,yes,54,Create evidence export request.
logs,L5,evidence_facade,EvidenceFacade.get_chain,"async get_chain(chain_id: str, tenant_id: str) -> Optional[EvidenceChain]",L4:logs_handler | L5:__init__,get,,pure,yes,19,Get a specific evidence chain.
logs,L5,evidence_facade,EvidenceFacade.get_export,"async get_export(export_id: str, tenant_id: str) -> Optional[EvidenceExport]",L4:logs_handler | L5:__init__,get,,pure,yes,19,Get export status.
logs,L5,evidence_facade,EvidenceFacade.list_chains,"async list_chains(tenant_id: str, run_id: Optional[str], limit: int, offset: int) -> List[EvidenceChain]",L4:logs_handler | L5:__init__,append | sort | values,,pure,yes,31,List evidence chains.
logs,L5,evidence_facade,EvidenceFacade.list_exports,"async list_exports(tenant_id: str, chain_id: Optional[str], limit: int, offset: int) -> List[EvidenceExport]",L4:logs_handler | L5:__init__,append | sort | values,,pure,yes,31,List exports.
logs,L5,evidence_facade,EvidenceFacade.verify_chain,"async verify_chain(chain_id: str, tenant_id: str) -> VerificationResult",L4:logs_handler | L5:__init__,VerificationResult | _hash_data | append | enumerate | get | len,,pure,yes,55,Verify chain integrity.
logs,L5,evidence_facade,EvidenceLink.to_dict,"to_dict() -> Dict[str, Any]",L4:logs_handler | L5:__init__,,,pure,no,10,Convert to dictionary.
logs,L5,evidence_facade,VerificationResult.to_dict,"to_dict() -> Dict[str, Any]",L4:logs_handler | L5:__init__,,,pure,no,8,Convert to dictionary.
logs,L5,evidence_facade,get_evidence_facade,get_evidence_facade() -> EvidenceFacade,L4:logs_handler | L5:__init__,EvidenceFacade,,pure,no,14,Get the evidence facade instance.
logs,L5,evidence_report,EvidenceReportGenerator.__init__,__init__(is_demo: bool),?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,_setup_custom_styles | getSampleStyleSheet,enums | lib | pagesizes | platypus | styles | units,pure,no,4,
logs,L5,evidence_report,EvidenceReportGenerator._add_footer,"_add_footer(canvas, doc)",?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,Color | HexColor | drawCentredString | drawRightString | getPageNumber | restoreState | rotate | saveState | setFillColor | setFont,enums | lib | pagesizes | platypus | styles | units,pure,no,22,Add footer to every page.
logs,L5,evidence_report,EvidenceReportGenerator._build_certificate_section,_build_certificate_section(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HRFlowable | HexColor | Paragraph | Spacer | Table | TableStyle | append | setStyle,enums | lib | pagesizes | platypus | styles | units,pure,no,99,M23: Build cryptographic certificate section - HMAC-signed proof.
logs,L5,evidence_report,EvidenceReportGenerator._build_cover_page,_build_cover_page(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HexColor | Paragraph | Spacer | Table | TableStyle | append | setStyle | strftime | utcnow,enums | lib | pagesizes | platypus | styles | units,pure,no,51,Build cover page with metadata.
logs,L5,evidence_report,EvidenceReportGenerator._build_decision_timeline,_build_decision_timeline(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HRFlowable | HexColor | Paragraph | Spacer | Table | TableStyle | append | get | setStyle,enums | lib | pagesizes | platypus | styles | units,pure,no,43,Build decision timeline section - deterministic trace.
logs,L5,evidence_report,EvidenceReportGenerator._build_executive_summary,_build_executive_summary(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HRFlowable | HexColor | Paragraph | Spacer | Table | TableStyle | append | get | setStyle,enums | lib | pagesizes | platypus | styles | units,pure,no,86,Build executive summary section.
logs,L5,evidence_report,EvidenceReportGenerator._build_factual_reconstruction,_build_factual_reconstruction(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HRFlowable | HexColor | Paragraph | Spacer | Table | TableStyle | append | items | join | setStyle | str,enums | lib | pagesizes | platypus | styles | units,pure,no,58,"Build factual reconstruction section - pure facts, no opinions."
logs,L5,evidence_report,EvidenceReportGenerator._build_incident_snapshot,_build_incident_snapshot(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HexColor | Paragraph | Spacer | Table | TableStyle | append | get | getattr | setStyle,enums | lib | pagesizes | platypus | styles | units,pure,no,120,Build 1-page Incident Snapshot - scannable summary for executives.
logs,L5,evidence_report,EvidenceReportGenerator._build_legal_attestation,_build_legal_attestation(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HRFlowable | HexColor | Paragraph | Spacer | Table | TableStyle | _compute_report_hash | append | setStyle | strftime | utcnow,enums | lib | pagesizes | platypus | styles | units,pure,no,43,Build legal attestation section.
logs,L5,evidence_report,EvidenceReportGenerator._build_policy_evaluation,_build_policy_evaluation(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HRFlowable | HexColor | Paragraph | Spacer | Table | TableStyle | append | enumerate | get | setStyle,enums | lib | pagesizes | platypus | styles | units,pure,no,94,Build policy evaluation record.
logs,L5,evidence_report,EvidenceReportGenerator._build_prevention_proof,_build_prevention_proof(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HRFlowable | HexColor | Paragraph | Spacer | append | get | lower | str,enums | lib | pagesizes | platypus | styles | units,pure,no,44,Build counterfactual prevention proof.
logs,L5,evidence_report,EvidenceReportGenerator._build_remediation,_build_remediation(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HRFlowable | HexColor | Paragraph | Spacer | append,enums | lib | pagesizes | platypus | styles | units,pure,no,34,Build remediation & controls section.
logs,L5,evidence_report,EvidenceReportGenerator._build_replay_verification,_build_replay_verification(evidence: IncidentEvidence) -> List,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HRFlowable | HexColor | Paragraph | Spacer | Table | TableStyle | _compute_hash | append | get | setStyle,enums | lib | pagesizes | platypus | styles | units,pure,no,63,Build replay verification section - the hard moat.
logs,L5,evidence_report,EvidenceReportGenerator._compute_hash,_compute_hash(content: str) -> str,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,encode | hexdigest | sha256,enums | lib | pagesizes | platypus | styles | units,pure,no,3,Compute SHA-256 hash of content.
logs,L5,evidence_report,EvidenceReportGenerator._compute_report_hash,_compute_report_hash(evidence: IncidentEvidence) -> str,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,_compute_hash,enums | lib | pagesizes | platypus | styles | units,pure,no,4,Compute verification hash for the entire report.
logs,L5,evidence_report,EvidenceReportGenerator._setup_custom_styles,_setup_custom_styles(),?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,HexColor | ParagraphStyle | add,enums | lib | pagesizes | platypus | styles | units,db_write,no,125,Create custom paragraph styles.
logs,L5,evidence_report,EvidenceReportGenerator.generate,generate(evidence: IncidentEvidence) -> bytes,?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,BytesIO | PageBreak | SimpleDocTemplate | _build_certificate_section | _build_cover_page | _build_decision_timeline | _build_executive_summary | _build_factual_reconstruction | _build_incident_snapshot | _build_legal_attestation | _build_policy_evaluation | _build_prevention_proof | _build_remediation | _build_replay_verification | append,enums | lib | pagesizes | platypus | styles | units,file_io,no,64,Generate the complete PDF evidence report.
logs,L5,evidence_report,generate_evidence_report,"generate_evidence_report(incident_id: str, tenant_id: str, tenant_name: str, user_id: str, product_name: str, model_id: str, timestamp: str, user_input: str, context_data: Dict[str, Any], ai_output: str, policy_results: List[Dict[str, Any]], timeline_events: List[Dict[str, Any]], replay_result: Optional[Dict[str, Any]], prevention_result: Optional[Dict[str, Any]], root_cause: str, impact_assessment: Optional[List[str]], certificate: Optional[Dict[str, Any]], severity: str, status: str, is_demo: bool) -> bytes",?:guard | L4:logs_handler | ?:apply | ?:mypy_zones,CertificateEvidence | EvidenceReportGenerator | IncidentEvidence | generate | get,enums | lib | pagesizes | platypus | styles | units,pure,no,76,Convenience function to generate an evidence report.
logs,L5,logs_facade,LogsFacade.__init__,__init__(store: Optional[LogsDomainStore]),?:logs | L4:logs_handler | L5:__init__,get_logs_domain_store,logs_domain_store,pure,no,3,Initialize with optional store (for testing).
logs,L5,logs_facade,LogsFacade._snapshot_to_record_result,_snapshot_to_record_result(s: LLMRunSnapshot) -> LLMRunRecordResult,?:logs | L4:logs_handler | L5:__init__,LLMRunRecordResult,logs_domain_store,pure,no,18,Convert snapshot to result type.
logs,L5,logs_facade,LogsFacade.get_audit_access,"async get_audit_access(session: Any, tenant_id: str) -> AuditAccessResult",?:logs | L4:logs_handler | L5:__init__,AccessEventResult | AuditAccessResult | EvidenceMetadataResult | len | list_audit_entries | now,logs_domain_store,pure,yes,38,O3: Get log access audit.
logs,L5,logs_facade,LogsFacade.get_audit_authorization,"async get_audit_authorization(session: Any, tenant_id: str) -> AuditAuthorizationResult",?:logs | L4:logs_handler | L5:__init__,AuditAuthorizationResult | AuthorizationDecisionResult | EvidenceMetadataResult | len | list_audit_entries | now,logs_domain_store,pure,yes,38,O2: Get authorization decisions.
logs,L5,logs_facade,LogsFacade.get_audit_entry,"async get_audit_entry(session: Any, tenant_id: str, entry_id: str) -> Optional[AuditLedgerDetailResult]",?:logs | L4:logs_handler | L5:__init__,AuditLedgerDetailResult | get_audit_entry,logs_domain_store,pure,yes,25,Get audit entry detail with state snapshots.
logs,L5,logs_facade,LogsFacade.get_audit_exports,"async get_audit_exports(session: Any, tenant_id: str) -> AuditExportsResult",?:logs | L4:logs_handler | L5:__init__,AuditExportsResult | EvidenceMetadataResult | ExportRecordResult | list_log_exports | now,logs_domain_store,pure,yes,42,O5: Get compliance exports.
logs,L5,logs_facade,LogsFacade.get_audit_identity,"async get_audit_identity(session: Any, tenant_id: str) -> AuditIdentityResult",?:logs | L4:logs_handler | L5:__init__,AuditIdentityResult | EvidenceMetadataResult | IdentityEventResult | len | list_audit_entries | now,logs_domain_store,pure,yes,36,O1: Get identity lifecycle events.
logs,L5,logs_facade,LogsFacade.get_audit_integrity,get_audit_integrity(tenant_id: str) -> AuditIntegrityResult,?:logs | L4:logs_handler | L5:__init__,AuditIntegrityResult | EvidenceMetadataResult | IntegrityCheckResult | now,logs_domain_store,pure,no,21,O4: Get tamper detection status.
logs,L5,logs_facade,LogsFacade.get_llm_run_envelope,"async get_llm_run_envelope(session: Any, tenant_id: str, run_id: str) -> Optional[LLMRunEnvelopeResult]",?:logs | L4:logs_handler | L5:__init__,EvidenceMetadataResult | LLMRunEnvelopeResult | get_llm_run,logs_domain_store,pure,yes,40,O1: Get canonical immutable run record.
logs,L5,logs_facade,LogsFacade.get_llm_run_export,"async get_llm_run_export(session: Any, tenant_id: str, run_id: str) -> Optional[LLMRunExportResult]",?:logs | L4:logs_handler | L5:__init__,EvidenceMetadataResult | LLMRunExportResult | get_llm_run | now,logs_domain_store,pure,yes,31,O5: Get export information.
logs,L5,logs_facade,LogsFacade.get_llm_run_governance,"async get_llm_run_governance(session: Any, tenant_id: str, run_id: str) -> LLMRunGovernanceResult",?:logs | L4:logs_handler | L5:__init__,EvidenceMetadataResult | GovernanceEventResult | LLMRunGovernanceResult | get_governance_events | len | now,logs_domain_store,pure,yes,39,O3: Get policy interaction trace.
logs,L5,logs_facade,LogsFacade.get_llm_run_replay,"async get_llm_run_replay(session: Any, tenant_id: str, run_id: str) -> Optional[LLMRunReplayResult]",?:logs | L4:logs_handler | L5:__init__,EvidenceMetadataResult | LLMRunReplayResult | ReplayEventResult | get_llm_run | get_replay_window_events | now | timedelta,logs_domain_store,pure,yes,51,O4: Get 60-second replay window.
logs,L5,logs_facade,LogsFacade.get_llm_run_trace,"async get_llm_run_trace(session: Any, tenant_id: str, run_id: str) -> Optional[LLMRunTraceResult]",?:logs | L4:logs_handler | L5:__init__,EvidenceMetadataResult | LLMRunTraceResult | TraceStepResult | get_trace_id_for_run | get_trace_steps | len | now,logs_domain_store,pure,yes,46,O2: Get step-by-step execution trace.
logs,L5,logs_facade,LogsFacade.get_system_audit,"async get_system_audit(session: Any, tenant_id: str) -> SystemAuditResult",?:logs | L4:logs_handler | L5:__init__,EvidenceMetadataResult | SystemAuditResult | SystemEventResult | list_system_records | now,logs_domain_store,pure,yes,42,O5: Get infra attribution.
logs,L5,logs_facade,LogsFacade.get_system_events,"async get_system_events(session: Any, tenant_id: str, run_id: str) -> SystemEventsResult",?:logs | L4:logs_handler | L5:__init__,EvidenceMetadataResult | SystemEventResult | SystemEventsResult | len | list_system_records | now,logs_domain_store,pure,yes,42,O3: Get infra events affecting run.
logs,L5,logs_facade,LogsFacade.get_system_replay,"async get_system_replay(session: Any, tenant_id: str, run_id: str) -> Optional[SystemReplayResult]",?:logs | L4:logs_handler | L5:__init__,EvidenceMetadataResult | ReplayEventResult | SystemReplayResult | get_llm_run | get_system_records_in_window | now | timedelta,logs_domain_store,pure,yes,48,O4: Get infra replay window.
logs,L5,logs_facade,LogsFacade.get_system_snapshot,"async get_system_snapshot(session: Any, tenant_id: str, run_id: str) -> SystemSnapshotResult",?:logs | L4:logs_handler | L5:__init__,EvidenceMetadataResult | SystemSnapshotResult | get_system_record_by_correlation | now,logs_domain_store,pure,yes,55,O1: Get environment baseline snapshot.
logs,L5,logs_facade,LogsFacade.get_system_telemetry,get_system_telemetry(run_id: str) -> TelemetryStubResult,?:logs | L4:logs_handler | L5:__init__,TelemetryStubResult,logs_domain_store,pure,no,3,O2: Telemetry stub - producer not implemented.
logs,L5,logs_facade,LogsFacade.list_audit_entries,"async list_audit_entries(session: Any, tenant_id: str) -> AuditLedgerListResult",?:logs | L4:logs_handler | L5:__init__,AuditLedgerItemResult | AuditLedgerListResult | isoformat | list_audit_entries,logs_domain_store,pure,yes,59,List audit entries with optional filters.
logs,L5,logs_facade,LogsFacade.list_llm_run_records,"async list_llm_run_records(session: Any, tenant_id: str) -> LLMRunRecordsResult",?:logs | L4:logs_handler | L5:__init__,LLMRunRecordsResult | _snapshot_to_record_result | isoformat | list_llm_runs,logs_domain_store,pure,yes,56,List LLM run records with optional filters.
logs,L5,logs_facade,LogsFacade.list_system_records,"async list_system_records(session: Any, tenant_id: str) -> SystemRecordsResult",?:logs | L4:logs_handler | L5:__init__,SystemRecordResult | SystemRecordsResult | isoformat | list_system_records,logs_domain_store,pure,yes,60,List system records with optional filters.
logs,L5,logs_facade,get_logs_facade,get_logs_facade() -> LogsFacade,?:logs | L4:logs_handler | L5:__init__,LogsFacade,logs_domain_store,pure,no,6,Get the singleton LogsFacade instance.
logs,L5,logs_read_engine,LogsReadService.__init__,__init__(store: Optional[PostgresTraceStore]),L4:logs_bridge | L3:customer_logs_adapter,,models | pg_store,pure,no,4,Initialize with trace store (lazy loaded if not provided).
logs,L5,logs_read_engine,LogsReadService._get_store,async _get_store() -> PostgresTraceStore,L4:logs_bridge | L3:customer_logs_adapter,get_postgres_trace_store,models | pg_store,pure,yes,6,Get the L6 PostgresTraceStore (lazy loaded).
logs,L5,logs_read_engine,LogsReadService.get_trace,"async get_trace(trace_id: str, tenant_id: str) -> Optional[TraceRecord]",L4:logs_bridge | L3:customer_logs_adapter,_get_store | get_trace,models | pg_store,pure,yes,20,Get a single trace by ID with tenant isolation.
logs,L5,logs_read_engine,LogsReadService.get_trace_by_root_hash,"async get_trace_by_root_hash(root_hash: str, tenant_id: str) -> Optional[TraceRecord]",L4:logs_bridge | L3:customer_logs_adapter,_get_store | get_trace_by_root_hash,models | pg_store,pure,yes,20,Get trace by deterministic root hash with tenant isolation.
logs,L5,logs_read_engine,LogsReadService.get_trace_count,async get_trace_count(tenant_id: str) -> int,L4:logs_bridge | L3:customer_logs_adapter,_get_store | get_trace_count,models | pg_store,pure,yes,15,Get total trace count for a tenant.
logs,L5,logs_read_engine,LogsReadService.list_traces,"async list_traces(tenant_id: str, limit: int, offset: int) -> List[TraceSummary]",L4:logs_bridge | L3:customer_logs_adapter,_get_store | list_traces | min,models | pg_store,pure,yes,26,List traces for a tenant.
logs,L5,logs_read_engine,LogsReadService.search_traces,"async search_traces(tenant_id: str, agent_id: Optional[str], status: Optional[str], from_date: Optional[str], to_date: Optional[str], limit: int, offset: int) -> List[TraceSummary]",L4:logs_bridge | L3:customer_logs_adapter,_get_store | min | search_traces,models | pg_store,pure,yes,38,Search traces for a tenant with optional filters.
logs,L5,logs_read_engine,get_logs_read_service,get_logs_read_service() -> LogsReadService,L4:logs_bridge | L3:customer_logs_adapter,LogsReadService,models | pg_store,pure,no,10,Factory function to get LogsReadService instance.
logs,L5,mapper,SOC2ControlMapper.__init__,__init__(registry: Optional[SOC2ControlRegistry]),?:__init__ | ?:control_registry | L4:control_registry,get_control_registry,control_registry | time,pure,no,3,Initialize mapper with control registry.
logs,L5,mapper,SOC2ControlMapper._create_mapping,"_create_mapping(control: SOC2Control, incident_data: dict[str, Any]) -> SOC2ControlMapping",?:__init__ | ?:control_registry | L4:control_registry,SOC2ControlMapping | _determine_compliance_status | append | format | get | utc_now,control_registry | time,pure,no,54,Create a mapping with evidence for a control.
logs,L5,mapper,SOC2ControlMapper._determine_compliance_status,"_determine_compliance_status(control: SOC2Control, incident_data: dict[str, Any]) -> SOC2ComplianceStatus",?:__init__ | ?:control_registry | L4:control_registry,bool | get | startswith,control_registry | time,pure,no,38,Determine compliance status based on control and evidence.
logs,L5,mapper,SOC2ControlMapper.get_all_applicable_controls,get_all_applicable_controls(incident_category: str) -> list[SOC2Control],?:__init__ | ?:control_registry | L4:control_registry,append | get | get_control,control_registry | time,pure,no,17,Get all controls applicable to an incident category.
logs,L5,mapper,SOC2ControlMapper.map_incident_to_controls,"map_incident_to_controls(incident_category: str, incident_data: dict[str, Any]) -> list[SOC2ControlMapping]",?:__init__ | ?:control_registry | L4:control_registry,_create_mapping | append | get | get_control,control_registry | time,pure,no,39,Map an incident to relevant SOC2 controls.
logs,L5,mapper,get_control_mappings_for_incident,"get_control_mappings_for_incident(incident_category: str, incident_data: dict[str, Any]) -> list[dict[str, Any]]",?:__init__ | ?:control_registry | L4:control_registry,SOC2ControlMapper | map_incident_to_controls | to_dict,control_registry | time,pure,no,20,Get SOC2 control mappings for an incident (GAP-025 main entry point).
logs,L5,panel_response_assembler,PanelResponseAssembler.__init__,"__init__(adapter_version: str, schema_version: str)",?:__init__ | ?:ai_console_panel_engine | L5:ai_console_panel_engine,,panel_consistency_driver | panel_types,pure,no,7,
logs,L5,panel_response_assembler,PanelResponseAssembler._aggregate_verification,"_aggregate_verification(slot_results: List[PanelSlotResult]) -> Dict[str, int]",?:__init__ | ?:ai_console_panel_engine | L5:ai_console_panel_engine,,panel_consistency_driver | panel_types,pure,no,15,Aggregate verification signals across all slots.
logs,L5,panel_response_assembler,PanelResponseAssembler._determine_panel_authority,"_determine_panel_authority(slot_results: List[PanelSlotResult], consistency: ConsistencyCheckResult) -> str",?:__init__ | ?:ai_console_panel_engine | L5:ai_console_panel_engine,all | any,panel_consistency_driver | panel_types,pure,no,22,Determine overall panel authority.
logs,L5,panel_response_assembler,PanelResponseAssembler._determine_panel_state,_determine_panel_state(slot_results: List[PanelSlotResult]) -> str,?:__init__ | ?:ai_console_panel_engine | L5:ai_console_panel_engine,any,panel_consistency_driver | panel_types,pure,no,12,Determine overall panel state from slots.
logs,L5,panel_response_assembler,PanelResponseAssembler._slot_to_dict,"_slot_to_dict(slot: PanelSlotResult) -> Dict[str, Any]",?:__init__ | ?:ai_console_panel_engine | L5:ai_console_panel_engine,isoformat,panel_consistency_driver | panel_types,pure,no,38,Convert PanelSlotResult to dict.
logs,L5,panel_response_assembler,PanelResponseAssembler.assemble,"assemble(panel_id: str, panel_contract_id: str, slot_results: List[PanelSlotResult], consistency: ConsistencyCheckResult, evaluation_time_ms: float, request_params: Optional[Dict[str, Any]]) -> Dict[str, Any]",?:__init__ | ?:ai_console_panel_engine | L5:ai_console_panel_engine,_aggregate_verification | _determine_panel_authority | _determine_panel_state | _slot_to_dict | isoformat | len | now | round,panel_consistency_driver | panel_types,pure,no,63,Assemble complete panel response.
logs,L5,panel_response_assembler,PanelResponseAssembler.assemble_error,"assemble_error(panel_id: str, error: str, request_params: Optional[Dict[str, Any]]) -> Dict[str, Any]",?:__init__ | ?:ai_console_panel_engine | L5:ai_console_panel_engine,isoformat | now,panel_consistency_driver | panel_types,pure,no,41,Assemble error response envelope.
logs,L5,panel_response_assembler,create_response_assembler,"create_response_assembler(adapter_version: Optional[str], schema_version: Optional[str]) -> PanelResponseAssembler",?:__init__ | ?:ai_console_panel_engine | L5:ai_console_panel_engine,PanelResponseAssembler,panel_consistency_driver | panel_types,pure,no,9,Create response assembler.
logs,L5,pdf_renderer,PDFRenderer.__init__,__init__(),?:incidents | L7:export_bundles | L4:logs_handler,_setup_styles | getSampleStyleSheet,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,4,Initialize PDF renderer with styles.
logs,L5,pdf_renderer,PDFRenderer._build_attestation,_build_attestation(bundle: SOC2Bundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,HRFlowable | Paragraph | Spacer | append,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,14,Build attestation statement section.
logs,L5,pdf_renderer,PDFRenderer._build_control_mappings,_build_control_mappings(bundle: SOC2Bundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,HRFlowable | Paragraph | Spacer | append,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,30,Build SOC2 control mappings section.
logs,L5,pdf_renderer,PDFRenderer._build_evidence_cover,_build_evidence_cover(bundle: EvidenceBundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,Paragraph | Spacer | append | strftime,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,20,Build evidence cover page.
logs,L5,pdf_renderer,PDFRenderer._build_evidence_summary,_build_evidence_summary(bundle: EvidenceBundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,HRFlowable | Paragraph | Spacer | Table | TableStyle | append | setStyle | str,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,38,Build evidence summary section.
logs,L5,pdf_renderer,PDFRenderer._build_exec_cover,_build_exec_cover(bundle: ExecutiveDebriefBundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,Paragraph | Spacer | append | hexval | strftime | upper,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,30,Build executive debrief cover page.
logs,L5,pdf_renderer,PDFRenderer._build_exec_metrics,_build_exec_metrics(bundle: ExecutiveDebriefBundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,HRFlowable | Paragraph | Spacer | Table | TableStyle | append | setStyle,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,35,Build executive metrics section.
logs,L5,pdf_renderer,PDFRenderer._build_exec_summary,_build_exec_summary(bundle: ExecutiveDebriefBundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,HRFlowable | Paragraph | Spacer | Table | TableStyle | append | setStyle | strftime | upper,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,44,Build executive summary section.
logs,L5,pdf_renderer,PDFRenderer._build_integrity_section,_build_integrity_section(bundle: EvidenceBundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,HRFlowable | Paragraph | Spacer | append,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,21,Build integrity verification section.
logs,L5,pdf_renderer,PDFRenderer._build_policy_section,_build_policy_section(bundle: EvidenceBundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,HRFlowable | Paragraph | Spacer | Table | TableStyle | append | setStyle,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,33,Build policy context section.
logs,L5,pdf_renderer,PDFRenderer._build_recommendations,_build_recommendations(bundle: ExecutiveDebriefBundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,HRFlowable | Paragraph | Spacer | append | enumerate,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,12,Build recommendations section.
logs,L5,pdf_renderer,PDFRenderer._build_soc2_cover,_build_soc2_cover(bundle: SOC2Bundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,Paragraph | Spacer | append | strftime,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,29,Build SOC2 cover page.
logs,L5,pdf_renderer,PDFRenderer._build_trace_timeline,_build_trace_timeline(bundle: EvidenceBundle) -> list,?:incidents | L7:export_bundles | L4:logs_handler,HRFlowable | Paragraph | Spacer | Table | TableStyle | append | len | setStyle | str | strftime | upper,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,52,Build trace timeline section.
logs,L5,pdf_renderer,PDFRenderer._setup_styles,_setup_styles() -> None,?:incidents | L7:export_bundles | L4:logs_handler,ParagraphStyle | add,enums | export_bundles | lib | pagesizes | platypus | styles | units,db_write,no,59,Configure custom paragraph styles.
logs,L5,pdf_renderer,PDFRenderer.render_evidence_pdf,render_evidence_pdf(bundle: EvidenceBundle) -> bytes,?:incidents | L7:export_bundles | L4:logs_handler,BytesIO | PageBreak | SimpleDocTemplate | _build_evidence_cover | _build_evidence_summary | _build_integrity_section | _build_policy_section | _build_trace_timeline | append | build | extend | getvalue | info | len | seek,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,53,Render evidence bundle to PDF bytes.
logs,L5,pdf_renderer,PDFRenderer.render_executive_debrief_pdf,render_executive_debrief_pdf(bundle: ExecutiveDebriefBundle) -> bytes,?:incidents | L7:export_bundles | L4:logs_handler,BytesIO | PageBreak | SimpleDocTemplate | _build_exec_cover | _build_exec_metrics | _build_exec_summary | _build_recommendations | append | build | extend | getvalue | info | seek,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,47,Render executive debrief to PDF.
logs,L5,pdf_renderer,PDFRenderer.render_soc2_pdf,render_soc2_pdf(bundle: SOC2Bundle) -> bytes,?:incidents | L7:export_bundles | L4:logs_handler,BytesIO | PageBreak | SimpleDocTemplate | _build_attestation | _build_control_mappings | _build_evidence_summary | _build_soc2_cover | _build_trace_timeline | append | build | extend | getvalue | info | len | seek,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,54,Render SOC2 bundle to PDF with attestation.
logs,L5,pdf_renderer,get_pdf_renderer,get_pdf_renderer() -> PDFRenderer,?:incidents | L7:export_bundles | L4:logs_handler,PDFRenderer,enums | export_bundles | lib | pagesizes | platypus | styles | units,pure,no,6,Get or create PDFRenderer singleton.
logs,L5,redact,add_redaction_pattern,"add_redaction_pattern(pattern: str, replacement: str) -> None",?:traces | ?:pg_store | ?:__init__ | L6:pg_store | L2:traces | ?:mypy_zones,append | compile,,pure,no,3,Add a custom redaction pattern.
logs,L5,redact,add_sensitive_field,add_sensitive_field(field_name: str) -> None,?:traces | ?:pg_store | ?:__init__ | L6:pg_store | L2:traces | ?:mypy_zones,add | lower,,db_write,no,3,Add a custom field name to the sensitive fields set.
logs,L5,redact,is_sensitive_field,is_sensitive_field(field_name: str) -> bool,?:traces | ?:pg_store | ?:__init__ | L6:pg_store | L2:traces | ?:mypy_zones,lower,,pure,no,3,Check if a field name indicates sensitive data.
logs,L5,redact,redact_dict,"redact_dict(data: dict[str, Any], depth: int, max_depth: int) -> dict[str, Any]",?:traces | ?:pg_store | ?:__init__ | L6:pg_store | L2:traces | ?:mypy_zones,isinstance | items | lower | redact_dict | redact_list | redact_string_value,,pure,no,31,Recursively redact sensitive fields in a dictionary.
logs,L5,redact,redact_json_string,redact_json_string(json_str: str) -> str,?:traces | ?:pg_store | ?:__init__ | L6:pg_store | L2:traces | ?:mypy_zones,sub,,pure,no,14,Apply PII redaction patterns to a JSON string.
logs,L5,redact,redact_list,"redact_list(data: list[Any], depth: int, max_depth: int) -> list[Any]",?:traces | ?:pg_store | ?:__init__ | L6:pg_store | L2:traces | ?:mypy_zones,append | isinstance | redact_dict | redact_list | redact_string_value,,pure,no,17,Recursively redact sensitive fields in a list.
logs,L5,redact,redact_string_value,redact_string_value(value: str) -> str,?:traces | ?:pg_store | ?:__init__ | L6:pg_store | L2:traces | ?:mypy_zones,lower | search | sub,,pure,no,22,Redact sensitive patterns in a string value.
logs,L5,redact,redact_trace_data,"redact_trace_data(trace: dict[str, Any]) -> dict[str, Any]",?:traces | ?:pg_store | ?:__init__ | L6:pg_store | L2:traces | ?:mypy_zones,deepcopy | isinstance | redact_dict | redact_list,,pure,no,53,Redact PII from a complete trace object.
logs,L5,replay_determinism,CallRecord.to_dict,"to_dict() -> Dict[str, Any]",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,isoformat | to_dict,determinism_types,pure,no,11,
logs,L5,replay_determinism,ModelVersion.from_dict,"from_dict(data: Dict[str, Any]) -> 'ModelVersion'",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,cls | fromisoformat | get | now,determinism_types,pure,no,9,
logs,L5,replay_determinism,ModelVersion.to_dict,"to_dict() -> Dict[str, Any]",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,isoformat,determinism_types,pure,no,9,
logs,L5,replay_determinism,PolicyDecision.to_dict,"to_dict() -> Dict[str, Any]",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,,determinism_types,pure,no,9,
logs,L5,replay_determinism,ReplayContextBuilder.__init__,__init__(),?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,ReplayValidator,determinism_types,pure,no,2,
logs,L5,replay_determinism,ReplayContextBuilder.build_call_record,"build_call_record(call_id: str, request: Dict[str, Any], response: Dict[str, Any], model_info: Dict[str, Any], policy_decisions: List[Dict[str, Any]], duration_ms: Optional[int]) -> CallRecord",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,CallRecord | ModelVersion | PolicyDecision | dumps | get | hash_content,determinism_types,pure,no,58,Build a CallRecord from raw API data.
logs,L5,replay_determinism,ReplayResult.to_dict,"to_dict() -> Dict[str, Any]",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,to_dict,determinism_types,pure,no,12,
logs,L5,replay_determinism,ReplayValidator.__init__,__init__(),?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,,determinism_types,pure,no,2,
logs,L5,replay_determinism,ReplayValidator._compare_policies,"_compare_policies(original: List[PolicyDecision], replay: List[PolicyDecision]) -> bool",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,items | keys | len | set,determinism_types,pure,no,25,Compare policy decisions for logical equivalence.
logs,L5,replay_determinism,ReplayValidator._detect_model_drift,"_detect_model_drift(original: ModelVersion, replay: ModelVersion) -> bool",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,abs,determinism_types,pure,no,23,Detect if the model has drifted between original and replay.
logs,L5,replay_determinism,ReplayValidator._level_meets_requirement,"_level_meets_requirement(achieved: ReplayMatch, required: DeterminismLevel) -> bool",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,,determinism_types,pure,no,17,Check if achieved match level meets the required determinism level.
logs,L5,replay_determinism,ReplayValidator._semantic_equivalent,"_semantic_equivalent(original: CallRecord, replay: CallRecord) -> bool",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,isinstance | keys | len | loads | max | set,determinism_types,pure,no,31,Check if two responses are semantically equivalent.
logs,L5,replay_determinism,ReplayValidator.hash_content,hash_content(content: str) -> str,?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,encode | hexdigest | sha256,determinism_types,pure,no,3,Create a deterministic hash of content.
logs,L5,replay_determinism,ReplayValidator.validate_replay,"validate_replay(original: CallRecord, replay: CallRecord, level: DeterminismLevel) -> ReplayResult",?:guard | ?:certificate | ?:replay_determinism | L4:logs_handler | L5s:determinism_types | L5:certificate,ReplayResult | _compare_policies | _detect_model_drift | _level_meets_requirement | _semantic_equivalent,determinism_types,pure,no,67,Validate a replay against the original call.
logs,L5,trace_facade,TraceFacade.__init__,__init__(trace_store),?:transaction_coordinator | ?:__init__ | ?:trace_facade | L5:__init__,,audit_store | rac_models | trace_store,pure,no,9,Initialize facade with optional trace store.
logs,L5,trace_facade,TraceFacade._emit_ack,"_emit_ack(run_id: str, action: str, result_id: Optional[str], error: Optional[str]) -> None",?:transaction_coordinator | ?:__init__ | ?:trace_facade | L5:__init__,DomainAck | UUID | add_ack | debug | get | get_audit_store | warning,audit_store | rac_models | trace_store,pure,no,51,Emit RAC acknowledgment for trace operations.
logs,L5,trace_facade,TraceFacade._store,_store(),?:transaction_coordinator | ?:__init__ | ?:trace_facade | L5:__init__,PostgresTraceStore,audit_store | rac_models | trace_store,pure,no,6,Lazy-load trace store.
logs,L5,trace_facade,TraceFacade.add_step,"async add_step(trace_id: str, step_type: str, data: dict) -> bool",?:transaction_coordinator | ?:__init__ | ?:trace_facade | L5:__init__,add_step | warning,audit_store | rac_models | trace_store,pure,yes,33,Add a step to a trace.
logs,L5,trace_facade,TraceFacade.complete_trace,"async complete_trace(trace_id: str, run_id: str, status: str) -> bool",?:transaction_coordinator | ?:__init__ | ?:trace_facade | L5:__init__,_emit_ack | complete_trace | debug | error | str,audit_store | rac_models | trace_store,pure,yes,50,Complete a trace.
logs,L5,trace_facade,TraceFacade.start_trace,"async start_trace(run_id: str, tenant_id: str, agent_id: Optional[str]) -> Optional[str]",?:transaction_coordinator | ?:__init__ | ?:trace_facade | L5:__init__,_emit_ack | debug | error | start_trace | str,audit_store | rac_models | trace_store,pure,yes,50,Start a trace for a run.
logs,L5,trace_facade,get_trace_facade,get_trace_facade(trace_store) -> TraceFacade,?:transaction_coordinator | ?:__init__ | ?:trace_facade | L5:__init__,TraceFacade,audit_store | rac_models | trace_store,pure,no,14,Get the trace facade singleton.
logs,L5,traces_metrics,TracesMetrics.__init__,__init__(),?:__init__,,prometheus_client,pure,no,2,
logs,L5,traces_metrics,TracesMetrics.measure_request,"measure_request(operation: str, tenant_id: str)",?:__init__,inc | labels | observe | perf_counter,prometheus_client,pure,no,13,Context manager to measure request duration.
logs,L5,traces_metrics,TracesMetrics.measure_storage,"measure_storage(operation: str, backend: str)",?:__init__,labels | observe | perf_counter,prometheus_client,pure,no,8,Context manager to measure storage operation duration.
logs,L5,traces_metrics,TracesMetrics.record_idempotency_check,"record_idempotency_check(result: str, tenant_id: str)",?:__init__,inc | labels,prometheus_client,pure,no,3,Record idempotency check result.
logs,L5,traces_metrics,TracesMetrics.record_parity_check,"record_parity_check(trace_id: str, check_type: str, passed: bool)",?:__init__,inc | labels | set,prometheus_client,pure,no,9,Record parity check result.
logs,L5,traces_metrics,TracesMetrics.record_replay_enforcement,"record_replay_enforcement(behavior: str, outcome: str, tenant_id: str)",?:__init__,inc | labels,prometheus_client,pure,no,3,Record replay enforcement outcome.
logs,L5,traces_metrics,TracesMetrics.record_trace_stored,"record_trace_stored(trace_id: str, step_count: int, size_bytes: int, tenant_id: str)",?:__init__,labels | observe,prometheus_client,pure,no,4,Record trace storage metrics.
logs,L5,traces_metrics,get_traces_metrics,get_traces_metrics() -> TracesMetrics,?:__init__,TracesMetrics,prometheus_client,pure,no,6,Get or create global traces metrics instance.
logs,L5,traces_metrics,instrument_parity_check,instrument_parity_check(func: Callable) -> Callable,?:__init__,func | get | get_traces_metrics | getattr | record_parity_check | wraps,prometheus_client,pure,no,17,Decorator to instrument parity checks.
logs,L5,traces_metrics,instrument_replay_check,instrument_replay_check(func: Callable) -> Callable,?:__init__,func | get | get_traces_metrics | getattr | record_replay_enforcement | str | wraps,prometheus_client,pure,no,18,Decorator to instrument replay enforcement.
logs,L5,traces_metrics,instrument_trace_request,instrument_trace_request(operation: str),?:__init__,func | get | get_traces_metrics | measure_request | wraps,prometheus_client,pure,no,16,Decorator to instrument trace API endpoints.
logs,L5,traces_models,ParityResult.to_dict,"to_dict() -> dict[str, Any]",,,,pure,no,9,Convert to dictionary.
logs,L5,traces_models,TraceRecord.determinism_signature,determinism_signature() -> str,,determinism_hash | encode | hexdigest | join | sha256,,pure,no,9,Compute signature of all determinism-relevant fields.
logs,L5,traces_models,TraceRecord.failure_count,failure_count() -> int,,sum,,pure,no,3,Count of failed steps.
logs,L5,traces_models,TraceRecord.from_dict,"from_dict(data: dict[str, Any]) -> 'TraceRecord'",,cls | from_dict | fromisoformat | get,,pure,no,18,Create from dictionary.
logs,L5,traces_models,TraceRecord.success_count,success_count() -> int,,sum,,pure,no,3,Count of successful steps.
logs,L5,traces_models,TraceRecord.to_dict,"to_dict() -> dict[str, Any]",,determinism_signature | isoformat | to_dict,,pure,no,22,Convert to dictionary for storage.
logs,L5,traces_models,TraceRecord.to_summary,to_summary() -> TraceSummary,,TraceSummary | len,,pure,no,16,Create a summary from this record.
logs,L5,traces_models,TraceRecord.total_cost_cents,total_cost_cents() -> float,,sum,,pure,no,3,Sum of all step costs.
logs,L5,traces_models,TraceRecord.total_duration_ms,total_duration_ms() -> float,,sum,,pure,no,3,Sum of all step durations.
logs,L5,traces_models,TraceStep.determinism_hash,determinism_hash() -> str,,_normalize_for_determinism | dumps | encode | getattr | hexdigest | sha256,,pure,no,15,Compute hash of determinism-relevant fields only.
logs,L5,traces_models,TraceStep.from_dict,"from_dict(data: dict[str, Any]) -> 'TraceStep'",,TraceStatus | cls | fromisoformat | get,,pure,no,15,Create from dictionary.
logs,L5,traces_models,TraceStep.to_dict,"to_dict() -> dict[str, Any]",,isoformat,,pure,no,15,Convert to dictionary for storage.
logs,L5,traces_models,TraceSummary.to_dict,"to_dict() -> dict[str, Any]",,isoformat,,pure,no,21,Convert to dictionary.
logs,L5,traces_models,_normalize_for_determinism,_normalize_for_determinism(value: Any) -> Any,,_normalize_for_determinism | isinstance | items | round,,pure,no,17,Normalize a value for deterministic hashing.
logs,L5,traces_models,compare_traces,"compare_traces(original: TraceRecord, replay: TraceRecord) -> ParityResult",,ParityResult | append | determinism_hash | determinism_signature | dumps | enumerate | join | len | min | zip,,pure,no,68,Compare two traces to verify replay parity.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync.__init__,__init__(session: AsyncSession),?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,,asyncio | audit_ledger,pure,no,3,Initialize with async database session.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync._emit,"async _emit(tenant_id: str, event_type: AuditEventType, entity_type: AuditEntityType, entity_id: str, actor_type: ActorType, actor_id: Optional[str], reason: Optional[str], before_state: Optional[Dict[str, Any]], after_state: Optional[Dict[str, Any]]) -> AuditLedger",?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,AuditLedger | add | flush | info,asyncio | audit_ledger,db_write,yes,56,Emit an audit event to the ledger.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync.limit_breached,"async limit_breached(tenant_id: str, limit_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], breach_details: Optional[Dict[str, Any]]) -> AuditLedger",?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,_emit,asyncio | audit_ledger,pure,yes,20,Record a limit breach event.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync.limit_created,"async limit_created(tenant_id: str, limit_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], limit_state: Optional[Dict[str, Any]]) -> AuditLedger",?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,_emit,asyncio | audit_ledger,pure,yes,20,Record a limit creation event.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync.limit_updated,"async limit_updated(tenant_id: str, limit_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], before_state: Optional[Dict[str, Any]], after_state: Optional[Dict[str, Any]]) -> AuditLedger",?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,_emit,asyncio | audit_ledger,pure,yes,22,Record a limit update event.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync.policy_proposal_approved,"async policy_proposal_approved(tenant_id: str, proposal_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], proposal_state: Optional[Dict[str, Any]]) -> AuditLedger",?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,_emit,asyncio | audit_ledger,pure,yes,20,Record a policy proposal approval event.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync.policy_proposal_rejected,"async policy_proposal_rejected(tenant_id: str, proposal_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], proposal_state: Optional[Dict[str, Any]]) -> AuditLedger",?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,_emit,asyncio | audit_ledger,pure,yes,20,Record a policy proposal rejection event.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync.policy_rule_created,"async policy_rule_created(tenant_id: str, rule_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], rule_state: Optional[Dict[str, Any]]) -> AuditLedger",?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,_emit,asyncio | audit_ledger,pure,yes,20,Record a policy rule creation event.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync.policy_rule_modified,"async policy_rule_modified(tenant_id: str, rule_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], before_state: Optional[Dict[str, Any]], after_state: Optional[Dict[str, Any]]) -> AuditLedger",?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,_emit,asyncio | audit_ledger,pure,yes,22,Record a policy rule modification event.
logs,L6,audit_ledger_driver,AuditLedgerServiceAsync.policy_rule_retired,"async policy_rule_retired(tenant_id: str, rule_id: str, actor_id: Optional[str], actor_type: ActorType, reason: Optional[str], rule_state: Optional[Dict[str, Any]]) -> AuditLedger",?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,_emit,asyncio | audit_ledger,pure,yes,20,Record a policy rule retirement event.
logs,L6,audit_ledger_driver,get_audit_ledger_service_async,get_audit_ledger_service_async(session: AsyncSession) -> AuditLedgerServiceAsync,?:policy_proposal | ?:policy_rules_service | ?:policy_limits_service,AuditLedgerServiceAsync,asyncio | audit_ledger,pure,no,11,Get an AuditLedgerServiceAsync instance.
logs,L6,bridges_driver,record_policy_activation,"async record_policy_activation(db_factory, policy_id: str, source_pattern_id: str, source_recovery_id: str, confidence: float, approval_path: str, loop_trace_id: str, tenant_id: str) -> PolicyActivationAudit",L6:__init__ | L5:bridges_engine,PolicyActivationAudit | db_factory | execute | info | now | text,__future__ | audit_schemas | loop_events | sqlalchemy,db_write,yes,63,Record policy activation for audit trail.
logs,L6,capture_driver,EvidenceContextError.__init__,"__init__(evidence_type: str, message: str)",,__init__ | super,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,pure,no,4,
logs,L6,capture_driver,_assert_context_exists,"_assert_context_exists(ctx: ExecutionContext, evidence_type: str) -> None",,EvidenceContextError | error,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,pure,no,25,Hard guard: Fail fast if context is None.
logs,L6,capture_driver,_hash_content,_hash_content(content: str) -> str,,encode | hexdigest | sha256,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,pure,no,3,Generate SHA256 fingerprint of content.
logs,L6,capture_driver,_record_capture_failure,"_record_capture_failure(session: Session, run_id: str, evidence_type: str, failure_reason: str, error_message: Optional[str], resolution: Optional[str]) -> None",,debug | execute | get | now | text | uuid4,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,db_write,no,61,Record an evidence capture failure for later integrity reporting.
logs,L6,capture_driver,capture_activity_evidence,"capture_activity_evidence(session: Session, ctx: ExecutionContext) -> Optional[str]",,_assert_context_exists | _record_capture_failure | assert_valid_for_evidence | debug | execute | now | str | text | warning,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,db_write,no,118,Capture activity evidence (Class B) before/after LLM calls.
logs,L6,capture_driver,capture_environment_evidence,"capture_environment_evidence(session: Session, ctx: ExecutionContext) -> Optional[str]",,_assert_context_exists | _record_capture_failure | assert_valid_for_evidence | execute | info | now | str | text | warning,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,db_write,no,93,Capture environment evidence (Class H) at run creation.
logs,L6,capture_driver,capture_integrity_evidence,"capture_integrity_evidence(session: Session, run_id: str) -> Optional[str]",,compute_integrity | copy | dumps | execute | get | info | now | str | text | warning,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,db_write,no,86,Capture integrity evidence (Class J) at terminal state.
logs,L6,capture_driver,capture_policy_decision_evidence,"capture_policy_decision_evidence(session: Session, ctx: ExecutionContext) -> Optional[str]",,_assert_context_exists | _record_capture_failure | assert_valid_for_evidence | debug | execute | now | str | text | uuid4 | warning,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,db_write,no,110,Capture policy decision evidence (Class D) during policy evaluation.
logs,L6,capture_driver,capture_provider_evidence,"capture_provider_evidence(session: Session, ctx: ExecutionContext) -> Optional[str]",,_assert_context_exists | _record_capture_failure | assert_valid_for_evidence | debug | execute | now | str | text | warning,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,db_write,no,120,Capture provider evidence (Class G) after LLM provider response.
logs,L6,capture_driver,compute_integrity,"compute_integrity(run_id: str) -> Dict[str, Any]",,compute_integrity_v2 | debug,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,pure,no,22,Compute integrity payload by examining evidence tables.
logs,L6,capture_driver,hash_prompt,hash_prompt(prompt: str) -> str,,_hash_content,__future__ | exc | execution_context | integrity | sqlalchemy | sqlmodel,pure,no,3,Generate SHA256 fingerprint of prompt content.
logs,L6,export_bundle_store,ExportBundleStore.__init__,__init__(trace_store: Optional[TraceStore]),L3:export_bundle_adapter | L6:__init__,,db | killswitch | sqlmodel | store,pure,no,3,Initialize with optional trace store for testing.
logs,L6,export_bundle_store,ExportBundleStore.get_incident,get_incident(incident_id: str) -> Optional[IncidentSnapshot],L3:export_bundle_adapter | L6:__init__,IncidentSnapshot | Session | get | getattr,db | killswitch | sqlmodel | store,pure,no,17,Get incident by ID.
logs,L6,export_bundle_store,ExportBundleStore.get_run_by_run_id,get_run_by_run_id(run_id: str) -> Optional[RunSnapshot],L3:export_bundle_adapter | L6:__init__,RunSnapshot | Session | exec | first | getattr | select | where,db | killswitch | sqlmodel | store,pure,no,21,Get run by run_id.
logs,L6,export_bundle_store,ExportBundleStore.get_trace_steps,"async get_trace_steps(trace_id: str, tenant_id: str) -> list[TraceStepSnapshot]",L3:export_bundle_adapter | L6:__init__,TraceStepSnapshot | enumerate | get_trace_steps,db | killswitch | sqlmodel | store,pure,yes,24,Get trace steps for a trace.
logs,L6,export_bundle_store,ExportBundleStore.get_trace_summary,"async get_trace_summary(run_id: str, tenant_id: str) -> Optional[TraceSummarySnapshot]",L3:export_bundle_adapter | L6:__init__,TraceSummarySnapshot | get_trace_summary,db | killswitch | sqlmodel | store,pure,yes,21,Get trace summary for a run.
logs,L6,export_bundle_store,ExportBundleStore.trace_store,trace_store() -> TraceStore,L3:export_bundle_adapter | L6:__init__,TraceStore,db | killswitch | sqlmodel | store,pure,no,5,Get or create TraceStore instance.
logs,L6,export_bundle_store,get_export_bundle_store,get_export_bundle_store() -> ExportBundleStore,L3:export_bundle_adapter | L6:__init__,ExportBundleStore,db | killswitch | sqlmodel | store,pure,no,6,Get the singleton ExportBundleStore instance.
logs,L6,idempotency_driver,IdempotencyResponse.is_conflict,is_conflict() -> bool,,,asyncio,pure,no,2,
logs,L6,idempotency_driver,IdempotencyResponse.is_duplicate,is_duplicate() -> bool,,,asyncio,pure,no,2,
logs,L6,idempotency_driver,IdempotencyResponse.is_new,is_new() -> bool,,,asyncio,pure,no,2,
logs,L6,idempotency_driver,InMemoryIdempotencyStore.__init__,__init__(default_ttl: int),,,asyncio,pure,no,3,
logs,L6,idempotency_driver,InMemoryIdempotencyStore._make_key,"_make_key(idempotency_key: str, tenant_id: str) -> str",,,asyncio,pure,no,2,
logs,L6,idempotency_driver,InMemoryIdempotencyStore.check,"async check(idempotency_key: str, request_data: Dict[str, Any], tenant_id: str, trace_id: str, ttl: Optional[int]) -> IdempotencyResponse",,IdempotencyResponse | _make_key | get | hash_request,asyncio,pure,yes,26,
logs,L6,idempotency_driver,InMemoryIdempotencyStore.delete,"async delete(idempotency_key: str, tenant_id: str) -> bool",,_make_key | pop,asyncio,pure,yes,4,
logs,L6,idempotency_driver,InMemoryIdempotencyStore.get_status,"async get_status(idempotency_key: str, tenant_id: str) -> Optional[Dict[str, str]]",,_make_key | get,asyncio,pure,yes,3,
logs,L6,idempotency_driver,InMemoryIdempotencyStore.mark_completed,"async mark_completed(idempotency_key: str, trace_id: str, tenant_id: str, response_data: Optional[Dict[str, Any]]) -> bool",,_make_key,asyncio,pure,yes,12,
logs,L6,idempotency_driver,InMemoryIdempotencyStore.mark_failed,"async mark_failed(idempotency_key: str, tenant_id: str, error: str) -> bool",,_make_key,asyncio,pure,yes,6,
logs,L6,idempotency_driver,RedisIdempotencyStore.__init__,"__init__(redis_client: Any, key_prefix: str, default_ttl: int)",,,asyncio,pure,no,10,
logs,L6,idempotency_driver,RedisIdempotencyStore._ensure_script_loaded,async _ensure_script_loaded() -> str,,_load_lua_script | script_load,asyncio,pure,yes,6,Ensure Lua script is loaded in Redis.
logs,L6,idempotency_driver,RedisIdempotencyStore._make_key,"_make_key(idempotency_key: str, tenant_id: str) -> str",,,asyncio,pure,no,3,Construct Redis key.
logs,L6,idempotency_driver,RedisIdempotencyStore.check,"async check(idempotency_key: str, request_data: Dict[str, Any], tenant_id: str, trace_id: str, ttl: Optional[int]) -> IdempotencyResponse",,IdempotencyResponse | IdempotencyResult | _ensure_script_loaded | _make_key | decode | error | evalsha | hash_request | isinstance | str,asyncio,pure,yes,47,Check idempotency key atomically.
logs,L6,idempotency_driver,RedisIdempotencyStore.delete,"async delete(idempotency_key: str, tenant_id: str) -> bool",,_make_key | delete | error,asyncio,db_write,yes,9,Delete idempotency key (admin operation).
logs,L6,idempotency_driver,RedisIdempotencyStore.get_status,"async get_status(idempotency_key: str, tenant_id: str) -> Optional[Dict[str, str]]",,_make_key | decode | error | hgetall | isinstance | items,asyncio,pure,yes,14,Get current status of idempotency key.
logs,L6,idempotency_driver,RedisIdempotencyStore.mark_completed,"async mark_completed(idempotency_key: str, trace_id: str, tenant_id: str, response_data: Optional[Dict[str, Any]]) -> bool",,_make_key | error | hash_request | hset,asyncio,pure,yes,23,Mark idempotency key as completed with trace result.
logs,L6,idempotency_driver,RedisIdempotencyStore.mark_failed,"async mark_failed(idempotency_key: str, tenant_id: str, error: str) -> bool",,_make_key | error | expire | hset,asyncio,pure,yes,18,Mark idempotency key as failed (allows retry).
logs,L6,idempotency_driver,_load_lua_script,_load_lua_script() -> str,,exists | read_text,asyncio,pure,no,49,Load Lua script from file.
logs,L6,idempotency_driver,canonical_json,canonical_json(obj: Any) -> str,,dumps,asyncio,pure,no,3,"Produce canonical JSON (sorted keys, compact format)."
logs,L6,idempotency_driver,get_idempotency_store,async get_idempotency_store() -> Any,,InMemoryIdempotencyStore | RedisIdempotencyStore | from_url | getenv | info | ping | warning,asyncio,pure,yes,25,Get or create idempotency store based on environment.
logs,L6,idempotency_driver,hash_request,"hash_request(data: Dict[str, Any]) -> str",,canonical_json | encode | hexdigest | sha256,asyncio,pure,no,4,Hash request data for idempotency comparison.
logs,L6,integrity_driver,CaptureFailure.to_dict,"to_dict() -> Dict[str, Any]",,,__future__ | exc | sqlalchemy,pure,no,7,
logs,L6,integrity_driver,IntegrityAssembler.__init__,__init__(database_url: Optional[str]),,,__future__ | exc | sqlalchemy,pure,no,2,
logs,L6,integrity_driver,IntegrityAssembler._count_evidence,"_count_evidence(conn, run_id: str, table: str) -> int",,execute | scalar | text,__future__ | exc | sqlalchemy,pure,no,25,Count evidence records for a table.
logs,L6,integrity_driver,IntegrityAssembler._gather_failures,"_gather_failures(conn, run_id: str) -> List[CaptureFailure]",,CaptureFailure | _string_to_class | append | execute | text,__future__ | exc | sqlalchemy,pure,no,22,Gather capture failures from the failures table.
logs,L6,integrity_driver,IntegrityAssembler._resolve_superseded,_resolve_superseded(facts: IntegrityFacts) -> None,,,__future__ | exc | sqlalchemy,pure,no,5,Mark failures as superseded if evidence was later captured.
logs,L6,integrity_driver,IntegrityAssembler._string_to_class,_string_to_class(value: str) -> EvidenceClass,,,__future__ | exc | sqlalchemy,pure,no,6,Convert string to EvidenceClass.
logs,L6,integrity_driver,IntegrityAssembler._table_to_class,_table_to_class(table: str) -> EvidenceClass,,get,__future__ | exc | sqlalchemy,pure,no,11,Map table name to EvidenceClass.
logs,L6,integrity_driver,IntegrityAssembler.gather,gather(run_id: str) -> IntegrityFacts,,IntegrityFacts | _count_evidence | _gather_failures | _resolve_superseded | _table_to_class | append | connect | create_engine | dispose | warning,__future__ | exc | sqlalchemy,pure,no,43,Gather integrity facts for a run.
logs,L6,integrity_driver,IntegrityEvaluation.integrity_status,integrity_status() -> str,,,__future__ | exc | sqlalchemy,pure,no,8,Legacy status for backward compatibility.
logs,L6,integrity_driver,IntegrityEvaluator._build_explanation,"_build_explanation(facts: IntegrityFacts, grade: IntegrityGrade) -> str",,append | join | len,__future__ | exc | sqlalchemy,pure,no,19,Build human-readable explanation.
logs,L6,integrity_driver,IntegrityEvaluator._compute_grade,"_compute_grade(facts: IntegrityFacts, score: float) -> IntegrityGrade",,,__future__ | exc | sqlalchemy,pure,no,34,Compute integrity grade based on policy.
logs,L6,integrity_driver,IntegrityEvaluator._find_failure,"_find_failure(facts: IntegrityFacts, evidence_class: EvidenceClass) -> Optional[CaptureFailure]",,,__future__ | exc | sqlalchemy,pure,no,7,Find a capture failure for an evidence class.
logs,L6,integrity_driver,IntegrityEvaluator.evaluate,evaluate(facts: IntegrityFacts) -> IntegrityEvaluation,,IntegrityEvaluation | _build_explanation | _compute_grade | _find_failure | len | to_dict,__future__ | exc | sqlalchemy,pure,no,47,Evaluate integrity facts and produce a grade.
logs,L6,integrity_driver,IntegrityFacts.has_capture_failures,has_capture_failures() -> bool,,len,__future__ | exc | sqlalchemy,pure,no,3,Check if any capture failures were recorded.
logs,L6,integrity_driver,IntegrityFacts.has_required_evidence,has_required_evidence() -> bool,,all,__future__ | exc | sqlalchemy,pure,no,3,Check if all required evidence is present.
logs,L6,integrity_driver,IntegrityFacts.unresolved_failures,unresolved_failures() -> List[CaptureFailure],,,__future__ | exc | sqlalchemy,pure,no,4,Get failures that are not superseded.
logs,L6,integrity_driver,compute_integrity_v2,"compute_integrity_v2(run_id: str) -> Dict[str, Any]",,IntegrityAssembler | IntegrityEvaluator | evaluate | gather | to_dict,__future__ | exc | sqlalchemy,pure,no,24,Compute integrity using the new split architecture.
logs,L6,job_execution_driver,JobAuditEmitter.__init__,__init__(publisher: Optional[Any]),,,events | random,pure,no,10,Initialize the audit emitter.
logs,L6,job_execution_driver,JobAuditEmitter._emit,async _emit(event: JobAuditEvent) -> None,,_get_publisher | debug | error | publish | str | to_dict,events | random,pure,yes,26,Emit event and update chain.
logs,L6,job_execution_driver,JobAuditEmitter._generate_event_id,_generate_event_id() -> str,,uuid4,events | random,pure,no,5,Generate unique event ID.
logs,L6,job_execution_driver,JobAuditEmitter._get_publisher,_get_publisher() -> Optional[Any],,get_publisher,events | random,pure,no,10,Get event publisher.
logs,L6,job_execution_driver,JobAuditEmitter.emit_completed,"async emit_completed(job_id: str, tenant_id: str, duration_ms: int, result: Optional[Dict[str, Any]]) -> JobAuditEvent",,JobAuditEvent | _emit | _generate_event_id | _hash_value | isoformat | now,events | random,pure,yes,21,Emit job completed event.
logs,L6,job_execution_driver,JobAuditEmitter.emit_created,"async emit_created(job_id: str, tenant_id: str, handler: str, payload: Optional[Dict[str, Any]]) -> JobAuditEvent",,JobAuditEvent | _emit | _generate_event_id | _hash_value | isoformat | now,events | random,pure,yes,21,Emit job created event.
logs,L6,job_execution_driver,JobAuditEmitter.emit_failed,"async emit_failed(job_id: str, tenant_id: str, error: str, duration_ms: Optional[int], attempt_number: int) -> JobAuditEvent",,JobAuditEvent | _emit | _generate_event_id | isoformat | now,events | random,pure,yes,23,Emit job failed event.
logs,L6,job_execution_driver,JobAuditEmitter.emit_retried,"async emit_retried(job_id: str, tenant_id: str, attempt_number: int, delay_seconds: int, error: str) -> JobAuditEvent",,JobAuditEvent | _emit | _generate_event_id | isoformat | now,events | random,pure,yes,23,Emit job retried event.
logs,L6,job_execution_driver,JobAuditEmitter.emit_started,"async emit_started(job_id: str, tenant_id: str, handler: str, attempt_number: int) -> JobAuditEvent",,JobAuditEvent | _emit | _generate_event_id | isoformat | now,events | random,pure,yes,21,Emit job started event.
logs,L6,job_execution_driver,JobAuditEvent.__post_init__,__post_init__(),,_compute_integrity_hash,events | random,pure,no,4,Compute integrity hash.
logs,L6,job_execution_driver,JobAuditEvent._compute_integrity_hash,_compute_integrity_hash() -> str,,encode | hexdigest | sha256,events | random,pure,no,7,Compute tamper-evident integrity hash.
logs,L6,job_execution_driver,JobAuditEvent.to_dict,"to_dict() -> Dict[str, Any]",,,events | random,pure,no,18,Convert to dictionary.
logs,L6,job_execution_driver,JobAuditEvent.verify_integrity,verify_integrity() -> bool,,_compute_integrity_hash,events | random,pure,no,4,Verify integrity hash is valid.
logs,L6,job_execution_driver,JobProgressTracker.__init__,__init__(publisher: Optional[Any]),,,events | random,pure,no,10,Initialize the progress tracker.
logs,L6,job_execution_driver,JobProgressTracker._calculate_eta,_calculate_eta(update: ProgressUpdate) -> Optional[int],,fromisoformat | int | now | total_seconds,events | random,pure,no,22,Calculate estimated time to completion.
logs,L6,job_execution_driver,JobProgressTracker._emit_progress,async _emit_progress(update: ProgressUpdate) -> None,,_get_publisher | callback | get | publish | str | to_dict | warning,events | random,pure,yes,26,Emit progress event.
logs,L6,job_execution_driver,JobProgressTracker._get_publisher,_get_publisher() -> Optional[Any],,get_publisher,events | random,pure,no,10,Get event publisher.
logs,L6,job_execution_driver,JobProgressTracker.complete,"async complete(job_id: str, message: Optional[str]) -> Optional[ProgressUpdate]",,_emit_progress | debug | get | isoformat | now,events | random,pure,yes,29,Mark a job as complete.
logs,L6,job_execution_driver,JobProgressTracker.fail,"async fail(job_id: str, message: Optional[str]) -> Optional[ProgressUpdate]",,_emit_progress | debug | get | isoformat | now,events | random,pure,yes,28,Mark a job as failed.
logs,L6,job_execution_driver,JobProgressTracker.get_progress,get_progress(job_id: str) -> Optional[ProgressUpdate],,get,events | random,pure,no,3,Get current progress for a job.
logs,L6,job_execution_driver,JobProgressTracker.register_callback,"register_callback(job_id: str, callback: Callable[[ProgressUpdate], None]) -> None",,append,events | random,pure,no,9,Register a callback for progress updates.
logs,L6,job_execution_driver,JobProgressTracker.start,"async start(job_id: str, total_steps: Optional[int], metadata: Optional[Dict[str, Any]]) -> ProgressUpdate",,ProgressUpdate | _emit_progress | debug | isoformat | now,events | random,pure,yes,35,Start tracking progress for a job.
logs,L6,job_execution_driver,JobProgressTracker.update,"async update(job_id: str, percentage: Optional[float], stage: Optional[ProgressStage], message: Optional[str], current_step: Optional[int], metadata: Optional[Dict[str, Any]]) -> Optional[ProgressUpdate]",,_calculate_eta | _emit_progress | get | isoformat | max | min | now | update,events | random,pure,yes,60,Update progress for a job.
logs,L6,job_execution_driver,JobRetryManager.__init__,__init__(config: Optional[RetryConfig]),,RetryConfig,events | random,pure,no,9,Initialize the retry manager.
logs,L6,job_execution_driver,JobRetryManager.calculate_delay,calculate_delay(attempt_number: int) -> int,,int | len | min | random,events | random,pure,no,37,Calculate retry delay based on strategy.
logs,L6,job_execution_driver,JobRetryManager.clear_history,clear_history(job_id: str) -> None,,pop,events | random,pure,no,3,Clear retry history for a job.
logs,L6,job_execution_driver,JobRetryManager.get_retry_history,get_retry_history(job_id: str) -> List[RetryAttempt],,get,events | random,pure,no,3,Get retry history for a job.
logs,L6,job_execution_driver,JobRetryManager.record_retry,"record_retry(job_id: str, attempt_number: int, error: str, will_retry: bool) -> RetryAttempt",,RetryAttempt | append | calculate_delay | info | isoformat | now | timedelta,events | random,pure,no,51,Record a retry attempt.
logs,L6,job_execution_driver,JobRetryManager.should_retry,"should_retry(job_id: str, error: str, attempt_number: int) -> bool",,any | info,events | random,pure,no,38,Determine if a job should be retried.
logs,L6,job_execution_driver,ProgressUpdate.to_dict,"to_dict() -> Dict[str, Any]",,,events | random,pure,no,14,Convert to dictionary.
logs,L6,job_execution_driver,_hash_value,_hash_value(value: Any) -> str,,encode | hexdigest | sha256 | str,events | random,pure,no,6,Hash a value for audit purposes.
logs,L6,job_execution_driver,get_job_audit_emitter,get_job_audit_emitter() -> JobAuditEmitter,,JobAuditEmitter,events | random,pure,no,6,Get the singleton JobAuditEmitter.
logs,L6,job_execution_driver,get_job_progress_tracker,get_job_progress_tracker() -> JobProgressTracker,,JobProgressTracker,events | random,pure,no,6,Get the singleton JobProgressTracker.
logs,L6,job_execution_driver,get_job_retry_manager,get_job_retry_manager() -> JobRetryManager,,JobRetryManager,events | random,pure,no,6,Get the singleton JobRetryManager.
logs,L6,job_execution_driver,reset_job_execution_services,reset_job_execution_services() -> None,,,events | random,pure,no,6,Reset all singletons (for testing).
logs,L6,logs_domain_store,LogsDomainStore._to_audit_snapshot,_to_audit_snapshot(entry: AuditLedger) -> AuditLedgerSnapshot,L6:__init__ | L5:logs_facade,AuditLedgerSnapshot | getattr,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,pure,no,16,Transform ORM model to immutable snapshot.
logs,L6,logs_domain_store,LogsDomainStore._to_export_snapshot,_to_export_snapshot(entry: LogExport) -> LogExportSnapshot,L6:__init__ | L5:logs_facade,LogExportSnapshot,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,pure,no,13,Transform ORM model to immutable snapshot.
logs,L6,logs_domain_store,LogsDomainStore._to_llm_run_snapshot,_to_llm_run_snapshot(entry: LLMRunRecord) -> LLMRunSnapshot,L6:__init__ | L5:logs_facade,LLMRunSnapshot,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,pure,no,19,Transform ORM model to immutable snapshot.
logs,L6,logs_domain_store,LogsDomainStore._to_system_record_snapshot,_to_system_record_snapshot(entry: SystemRecord) -> SystemRecordSnapshot,L6:__init__ | L5:logs_facade,SystemRecordSnapshot,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,pure,no,14,Transform ORM model to immutable snapshot.
logs,L6,logs_domain_store,LogsDomainStore.get_audit_entry,"async get_audit_entry(session: AsyncSession, tenant_id: str, entry_id: str) -> Optional[AuditLedgerSnapshot]",L6:__init__ | L5:logs_facade,_to_audit_snapshot | execute | scalar_one_or_none | select | where,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,18,Get single audit ledger entry.
logs,L6,logs_domain_store,LogsDomainStore.get_governance_events,"async get_governance_events(session: AsyncSession, tenant_id: str, limit: int) -> list[AuditLedgerSnapshot]",L6:__init__ | L5:logs_facade,_to_audit_snapshot | all | desc | execute | in_ | limit | order_by | scalars | select | where,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,20,Get governance (policy) related audit events.
logs,L6,logs_domain_store,LogsDomainStore.get_llm_run,"async get_llm_run(session: AsyncSession, tenant_id: str, run_id: str) -> Optional[LLMRunSnapshot]",L6:__init__ | L5:logs_facade,_to_llm_run_snapshot | execute | scalar_one_or_none | select | where,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,18,Get single LLM run record by run_id.
logs,L6,logs_domain_store,LogsDomainStore.get_replay_window_events,"async get_replay_window_events(session: AsyncSession, tenant_id: str, trace_id: str, window_start: datetime, window_end: datetime) -> list[dict]",L6:__init__ | L5:logs_facade,execute | fetchall | text,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,56,Get replay window events from multiple sources.
logs,L6,logs_domain_store,LogsDomainStore.get_system_record_by_correlation,"async get_system_record_by_correlation(session: AsyncSession, tenant_id: str, correlation_id: str) -> Optional[SystemRecordSnapshot]",L6:__init__ | L5:logs_facade,_to_system_record_snapshot | asc | execute | is_ | limit | order_by | scalar_one_or_none | select | where,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,23,Get first system record for a correlation_id.
logs,L6,logs_domain_store,LogsDomainStore.get_system_records_in_window,"async get_system_records_in_window(session: AsyncSession, tenant_id: str, window_start: datetime, window_end: datetime) -> list[SystemRecordSnapshot]",L6:__init__ | L5:logs_facade,_to_system_record_snapshot | all | between | execute | is_ | order_by | scalars | select | where,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,20,Get system records in a time window.
logs,L6,logs_domain_store,LogsDomainStore.get_trace_id_for_run,"async get_trace_id_for_run(session: AsyncSession, tenant_id: str, run_id: str) -> Optional[str]",L6:__init__ | L5:logs_facade,execute | fetchone | text,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,14,Get trace_id from aos_traces for a run.
logs,L6,logs_domain_store,LogsDomainStore.get_trace_steps,"async get_trace_steps(session: AsyncSession, trace_id: str) -> list[TraceStepSnapshot]",L6:__init__ | L5:logs_facade,TraceStepSnapshot | execute | fetchall | int | text,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,28,Get trace steps for a trace_id.
logs,L6,logs_domain_store,LogsDomainStore.list_audit_entries,"async list_audit_entries(session: AsyncSession, tenant_id: str) -> QueryResult",L6:__init__ | L5:logs_facade,QueryResult | _to_audit_snapshot | all | count | desc | execute | len | limit | offset | order_by | scalar | scalars | select | where,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,48,Query audit ledger entries with filters.
logs,L6,logs_domain_store,LogsDomainStore.list_llm_runs,"async list_llm_runs(session: AsyncSession, tenant_id: str) -> QueryResult",L6:__init__ | L5:logs_facade,QueryResult | _to_llm_run_snapshot | all | count | desc | execute | len | limit | offset | order_by | scalar | scalars | select | where,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,56,Query LLM run records with filters.
logs,L6,logs_domain_store,LogsDomainStore.list_log_exports,"async list_log_exports(session: AsyncSession, tenant_id: str) -> QueryResult",L6:__init__ | L5:logs_facade,QueryResult | _to_export_snapshot | all | count | desc | execute | len | limit | offset | order_by | scalar | scalars | select | where,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,32,Query log export records.
logs,L6,logs_domain_store,LogsDomainStore.list_system_records,"async list_system_records(session: AsyncSession, tenant_id: str) -> QueryResult",L6:__init__ | L5:logs_facade,QueryResult | _to_system_record_snapshot | all | count | desc | execute | is_ | len | limit | offset | order_by | scalar | scalars | select | where,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,db_write,yes,54,Query system records with filters.
logs,L6,logs_domain_store,get_logs_domain_store,get_logs_domain_store() -> LogsDomainStore,L6:__init__ | L5:logs_facade,LogsDomainStore,asyncio | audit_ledger | log_exports | logs_records | sqlalchemy,pure,no,6,Get the singleton LogsDomainStore instance.
logs,L6,panel_consistency_driver,PanelConsistencyChecker.__init__,"__init__(rules: Optional[List[Dict[str, Any]]])",L5:panel_response_assembler,_default_rules,panel_types,pure,no,2,
logs,L6,panel_consistency_driver,PanelConsistencyChecker._check_rule,"_check_rule(rule: Dict[str, Any], signals: Dict[str, Any], sources: Dict[str, str]) -> Optional[ConsistencyViolation]",L5:panel_response_assembler,ConsistencyViolation | _evaluate_condition | append | get | warning,panel_types,pure,no,40,Check a single consistency rule.
logs,L6,panel_consistency_driver,PanelConsistencyChecker._default_rules,"_default_rules() -> List[Dict[str, Any]]",L5:panel_response_assembler,,panel_types,pure,no,36,Default consistency rules from spec.
logs,L6,panel_consistency_driver,PanelConsistencyChecker._eval_expr,"_eval_expr(expr: str, signal_a: Any, signal_b: Any) -> bool",L5:panel_response_assembler,bool | isinstance | strip,panel_types,pure,no,22,Evaluate a simple expression.
logs,L6,panel_consistency_driver,PanelConsistencyChecker._evaluate_condition,"_evaluate_condition(condition: str, signal_a: Any, signal_b: Any) -> bool",L5:panel_response_assembler,_eval_expr | len | split | strip | warning,panel_types,pure,no,28,Evaluate a condition expression.
logs,L6,panel_consistency_driver,PanelConsistencyChecker.check,"check(panel_id: str, slot_results: List[PanelSlotResult]) -> ConsistencyCheckResult",L5:panel_response_assembler,ConsistencyCheckResult | ConsistencyViolation | _check_rule | append | items | len,panel_types,pure,no,45,Check consistency across all slots in a panel.
logs,L6,panel_consistency_driver,create_consistency_checker,"create_consistency_checker(rules: Optional[List[Dict[str, Any]]]) -> PanelConsistencyChecker",L5:panel_response_assembler,PanelConsistencyChecker,panel_types,pure,no,5,Create consistency checker with optional custom rules.
logs,L6,pg_store,PostgresTraceStore.__init__,__init__(database_url: str | None),?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,getenv,asyncpg | models | redact,pure,no,5,
logs,L6,pg_store,PostgresTraceStore._get_pool,async _get_pool(),?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,create_pool,asyncpg | models | redact,pure,yes,13,Get or create connection pool.
logs,L6,pg_store,PostgresTraceStore.check_idempotency_key,"async check_idempotency_key(idempotency_key: str, tenant_id: str) -> dict | None",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,_get_pool | acquire | fetchrow | loads,asyncpg | models | redact,pure,yes,35,Check if an idempotency key has been executed.
logs,L6,pg_store,PostgresTraceStore.cleanup_old_traces,async cleanup_old_traces(days: int) -> int,?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,_get_pool | acquire | execute | int | split,asyncpg | models | redact,pure,yes,28,Archive and delete traces older than specified days.
logs,L6,pg_store,PostgresTraceStore.close,async close(),?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,close,asyncpg | models | redact,pure,yes,5,Close connection pool.
logs,L6,pg_store,PostgresTraceStore.complete_trace,"async complete_trace(run_id: str, status: str, metadata: dict[str, Any] | None) -> None",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,_get_pool | acquire | dumps | execute | now,asyncpg | models | redact,pure,yes,22,Mark a trace as completed (for replay compatibility).
logs,L6,pg_store,PostgresTraceStore.delete_trace,"async delete_trace(trace_id: str, tenant_id: str | None) -> bool",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,_get_pool | acquire | bool | execute,asyncpg | models | redact,pure,yes,23,Delete trace by ID.
logs,L6,pg_store,PostgresTraceStore.get_trace,"async get_trace(trace_id: str, tenant_id: str | None) -> TraceRecord | None",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,TraceRecord | TraceStatus | TraceStep | _get_pool | acquire | fetch | fetchrow | loads | now,asyncpg | models | redact,pure,yes,68,Get trace by ID with optional tenant check.
logs,L6,pg_store,PostgresTraceStore.get_trace_by_root_hash,"async get_trace_by_root_hash(root_hash: str, tenant_id: str | None) -> TraceRecord | None",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,_get_pool | acquire | fetchrow | get_trace,asyncpg | models | redact,pure,yes,21,Get trace by deterministic root hash.
logs,L6,pg_store,PostgresTraceStore.get_trace_count,async get_trace_count(tenant_id: str | None) -> int,?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,_get_pool | acquire | fetchrow,asyncpg | models | redact,pure,yes,10,Get total trace count.
logs,L6,pg_store,PostgresTraceStore.list_traces,"async list_traces(tenant_id: str | None, limit: int, offset: int) -> list[TraceSummary]",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,search_traces,asyncpg | models | redact,pure,yes,8,"List traces, optionally filtered by tenant (TraceStore interface)."
logs,L6,pg_store,PostgresTraceStore.mark_trace_aborted,"async mark_trace_aborted(run_id: str, reason: str) -> None",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,_get_pool | acquire | dumps | execute | now,asyncpg | models | redact,pure,yes,40,Mark a trace as ABORTED due to finalization failure.
logs,L6,pg_store,PostgresTraceStore.record_step,"async record_step(trace_id: str, run_id: str, step_index: int, skill_name: str, params: dict[str, Any], status: str, outcome_category: str, outcome_code: str | None, outcome_data: dict[str, Any] | None, cost_cents: float, duration_ms: float, retry_count: int) -> None",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,ValueError | _get_pool | _status_to_level | acquire | dumps | execute | fetchval | hasattr | now,asyncpg | models | redact,pure,yes,90,Record a step in the trace (for replay compatibility).
logs,L6,pg_store,PostgresTraceStore.search_traces,"async search_traces(tenant_id: str | None, agent_id: str | None, root_hash: str | None, plan_hash: str | None, seed: int | None, status: str | None, from_date: str | None, to_date: str | None, limit: int, offset: int) -> list[TraceSummary]",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,TraceSummary | _get_pool | acquire | append | fetch | fromisoformat | join | now,asyncpg | models | redact,pure,yes,100,Search traces with multiple filters.
logs,L6,pg_store,PostgresTraceStore.start_trace,"async start_trace(run_id: str, correlation_id: str, tenant_id: str, agent_id: str | None, plan: list[dict[str, Any]]) -> str",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,_get_pool | acquire | dumps | execute | now,asyncpg | models | redact,pure,yes,61,Start a new trace record (for replay compatibility).
logs,L6,pg_store,PostgresTraceStore.store_trace,"async store_trace(trace: dict[str, Any], tenant_id: str, stored_by: str | None, redact_pii: bool) -> str",?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,_get_pool | _status_to_level | acquire | dumps | execute | fromisoformat | get | now | redact_trace_data | uuid4,asyncpg | models | redact,pure,yes,124,Store a trace from SDK or simulation.
logs,L6,pg_store,_status_to_level,_status_to_level(status: str) -> str,?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,isinstance | lower | str,asyncpg | models | redact,pure,no,17,Derive log level from step status.
logs,L6,pg_store,get_postgres_trace_store,get_postgres_trace_store() -> PostgresTraceStore,?:traces | ?:runner | ?:__init__ | ?:logs_read_service | ?:replay | L5:logs_read_engine | L2:traces | ?:apply | ?:mypy_zones | ?:test_trace_fail_closed,PostgresTraceStore,asyncpg | models | redact,pure,no,6,Get singleton PostgreSQL trace store.
logs,L6,replay_driver,IdempotencyStore.delete,"async delete(key: str, tenant_id: str) -> bool",,,asyncio,pure,yes,2,
logs,L6,replay_driver,IdempotencyStore.get,"async get(key: str, tenant_id: str) -> Optional[dict]",,,asyncio,pure,yes,2,
logs,L6,replay_driver,IdempotencyStore.set,"async set(key: str, tenant_id: str, value: dict) -> None",,,asyncio,pure,yes,2,
logs,L6,replay_driver,IdempotencyViolationError.__init__,"__init__(idempotency_key: str, message: str)",,__init__ | super,asyncio,pure,no,7,
logs,L6,replay_driver,InMemoryIdempotencyStore.__init__,__init__(),,,asyncio,pure,no,2,
logs,L6,replay_driver,InMemoryIdempotencyStore._make_key,"_make_key(key: str, tenant_id: str) -> str",,,asyncio,pure,no,2,
logs,L6,replay_driver,InMemoryIdempotencyStore.clear,clear() -> None,,clear,asyncio,pure,no,3,Clear all stored keys.
logs,L6,replay_driver,InMemoryIdempotencyStore.delete,"async delete(key: str, tenant_id: str) -> bool",,_make_key,asyncio,pure,yes,6,
logs,L6,replay_driver,InMemoryIdempotencyStore.get,"async get(key: str, tenant_id: str) -> Optional[dict]",,_make_key | get,asyncio,pure,yes,3,
logs,L6,replay_driver,InMemoryIdempotencyStore.set,"async set(key: str, tenant_id: str, value: dict) -> None",,_make_key,asyncio,pure,yes,3,
logs,L6,replay_driver,RedisIdempotencyStore.__init__,"__init__(redis_url: str | None, ttl_seconds: int)",,getenv,asyncio,pure,no,6,
logs,L6,replay_driver,RedisIdempotencyStore._get_client,async _get_client(),,from_url,asyncio,pure,yes,6,
logs,L6,replay_driver,RedisIdempotencyStore._make_key,"_make_key(key: str, tenant_id: str) -> str",,,asyncio,pure,no,2,
logs,L6,replay_driver,RedisIdempotencyStore.delete,"async delete(key: str, tenant_id: str) -> bool",,_get_client | _make_key | bool | delete,asyncio,"db_write,external_api",yes,5,
logs,L6,replay_driver,RedisIdempotencyStore.get,"async get(key: str, tenant_id: str) -> Optional[dict]",,_get_client | _make_key | get | loads,asyncio,external_api,yes,7,
logs,L6,replay_driver,RedisIdempotencyStore.set,"async set(key: str, tenant_id: str, value: dict) -> None",,_get_client | _make_key | dumps | setex,asyncio,pure,yes,4,
logs,L6,replay_driver,ReplayEnforcer.__init__,__init__(idempotency_store: Optional['IdempotencyStore']),,InMemoryIdempotencyStore,asyncio,pure,no,2,
logs,L6,replay_driver,ReplayEnforcer.enforce_step,"async enforce_step(step: dict, execute_fn: Callable[[], Awaitable[Any]], tenant_id: str) -> ReplayResult",,ReplayBehavior | ReplayMismatchError | ReplayResult | execute_fn | get | hash_output | set,asyncio,pure,yes,90,Enforce replay behavior for a single step.
logs,L6,replay_driver,ReplayEnforcer.enforce_trace,"async enforce_trace(trace: dict, step_executor: Callable[[dict], Awaitable[Any]], tenant_id: str) -> list[ReplayResult]",,append | enforce_step | get | step_executor,asyncio,pure,yes,34,Enforce replay behavior for an entire trace.
logs,L6,replay_driver,ReplayMismatchError.__init__,"__init__(step_index: int, expected_hash: str, actual_hash: str, message: str)",,__init__ | super,asyncio,pure,no,11,
logs,L6,replay_driver,get_replay_enforcer,get_replay_enforcer(use_redis: bool) -> ReplayEnforcer,,InMemoryIdempotencyStore | RedisIdempotencyStore | ReplayEnforcer,asyncio,pure,no,10,Get singleton replay enforcer.
logs,L6,replay_driver,hash_output,hash_output(data: Any) -> str,,dumps | encode | hexdigest | sha256,asyncio,pure,no,6,Compute hash of output data for comparison.
logs,L6,traces_store,InMemoryTraceStore.__init__,__init__(),,,models | sqlite3,pure,no,3,
logs,L6,traces_store,InMemoryTraceStore.complete_trace,"async complete_trace(run_id: str, status: str, metadata: dict[str, Any] | None) -> None",,TraceRecord | get | now,models | sqlite3,pure,yes,20,
logs,L6,traces_store,InMemoryTraceStore.delete_trace,async delete_trace(run_id: str) -> bool,,pop,models | sqlite3,pure,yes,6,
logs,L6,traces_store,InMemoryTraceStore.get_trace,async get_trace(run_id: str) -> TraceRecord | None,,TraceRecord | get,models | sqlite3,pure,yes,17,
logs,L6,traces_store,InMemoryTraceStore.list_traces,"async list_traces(tenant_id: str | None, limit: int, offset: int) -> list[TraceSummary]",,TraceSummary | append | get | len | list | sort | sum | values,models | sqlite3,pure,yes,33,
logs,L6,traces_store,InMemoryTraceStore.record_step,"async record_step(run_id: str, step_index: int, skill_name: str, params: dict[str, Any], status: TraceStatus, outcome_category: str, outcome_code: str | None, outcome_data: dict[str, Any] | None, cost_cents: float, duration_ms: float, retry_count: int) -> None",,TraceStep | append | sort,models | sqlite3,pure,yes,33,
logs,L6,traces_store,InMemoryTraceStore.start_trace,"async start_trace(run_id: str, correlation_id: str, tenant_id: str, agent_id: str | None, plan: list[dict[str, Any]]) -> None",,TraceRecord | now,models | sqlite3,pure,yes,20,
logs,L6,traces_store,SQLiteTraceStore.__init__,__init__(db_path: str | Path),,Path | _init_db | mkdir,models | sqlite3,pure,no,4,
logs,L6,traces_store,SQLiteTraceStore._get_conn,_get_conn() -> sqlite3.Connection,,connect,models | sqlite3,pure,no,5,Get a database connection.
logs,L6,traces_store,SQLiteTraceStore._init_db,_init_db() -> None,,commit | connect | executescript,models | sqlite3,db_write,no,63,Initialize database schema.
logs,L6,traces_store,SQLiteTraceStore.cleanup_old_traces,async cleanup_old_traces(days: int) -> int,,_get_conn | commit | execute | to_thread,models | sqlite3,db_write,yes,16,Delete traces older than specified days.
logs,L6,traces_store,SQLiteTraceStore.complete_trace,"async complete_trace(run_id: str, status: str, metadata: dict[str, Any] | None) -> None",,_get_conn | commit | dumps | execute | isoformat | now | to_thread,models | sqlite3,db_write,yes,21,Mark a trace as completed.
logs,L6,traces_store,SQLiteTraceStore.delete_trace,async delete_trace(run_id: str) -> bool,,_get_conn | commit | execute | to_thread,models | sqlite3,db_write,yes,10,Delete a trace by run_id.
logs,L6,traces_store,SQLiteTraceStore.find_matching_traces,"async find_matching_traces(plan_hash: str, seed: int) -> list[TraceSummary]",,search_traces,models | sqlite3,pure,yes,11,Find traces with matching plan and seed (for replay verification).
logs,L6,traces_store,SQLiteTraceStore.get_trace,async get_trace(run_id: str) -> TraceRecord | None,,TraceRecord | TraceStatus | TraceStep | _get_conn | execute | fetchall | fetchone | fromisoformat | loads | to_thread,models | sqlite3,pure,yes,47,Get a complete trace by run_id.
logs,L6,traces_store,SQLiteTraceStore.get_trace_by_root_hash,async get_trace_by_root_hash(root_hash: str) -> TraceRecord | None,,_get_conn | execute | fetchone | get_trace | to_thread,models | sqlite3,pure,yes,12,Get a trace by its deterministic root hash.
logs,L6,traces_store,SQLiteTraceStore.get_trace_count,async get_trace_count(tenant_id: str | None) -> int,,_get_conn | execute | fetchone | to_thread,models | sqlite3,pure,yes,12,Get total trace count.
logs,L6,traces_store,SQLiteTraceStore.list_traces,"async list_traces(tenant_id: str | None, limit: int, offset: int) -> list[TraceSummary]",,TraceSummary | _get_conn | execute | fetchall | fromisoformat | to_thread,models | sqlite3,pure,yes,65,"List traces, optionally filtered by tenant."
logs,L6,traces_store,SQLiteTraceStore.record_step,"async record_step(run_id: str, step_index: int, skill_name: str, params: dict[str, Any], status: TraceStatus, outcome_category: str, outcome_code: str | None, outcome_data: dict[str, Any] | None, cost_cents: float, duration_ms: float, retry_count: int) -> None",,_get_conn | commit | dumps | execute | isoformat | now | to_thread,models | sqlite3,db_write,yes,43,Record a step in the trace.
logs,L6,traces_store,SQLiteTraceStore.search_traces,"async search_traces(tenant_id: str | None, agent_id: str | None, root_hash: str | None, plan_hash: str | None, seed: int | None, status: str | None, from_date: str | None, to_date: str | None, limit: int, offset: int) -> list[TraceSummary]",,TraceSummary | _get_conn | append | execute | fetchall | fromisoformat | join | to_thread,models | sqlite3,pure,yes,108,Search traces with multiple filter criteria.
logs,L6,traces_store,SQLiteTraceStore.start_trace,"async start_trace(run_id: str, correlation_id: str, tenant_id: str, agent_id: str | None, plan: list[dict[str, Any]]) -> None",,_get_conn | commit | dumps | execute | isoformat | now | to_thread,models | sqlite3,db_write,yes,29,Start a new trace record.
logs,L6,traces_store,SQLiteTraceStore.update_trace_determinism,"async update_trace_determinism(run_id: str, seed: int, frozen_timestamp: str | None, root_hash: str, plan_hash: str) -> None",,_get_conn | commit | execute | to_thread,models | sqlite3,db_write,yes,23,Update determinism fields after trace finalization.
logs,L6,traces_store,TraceStore.complete_trace,"async complete_trace(run_id: str, status: str, metadata: dict[str, Any] | None) -> None",,,models | sqlite3,pure,yes,8,Mark a trace as completed.
logs,L6,traces_store,TraceStore.delete_trace,async delete_trace(run_id: str) -> bool,,,models | sqlite3,pure,yes,3,Delete a trace by run_id.
logs,L6,traces_store,TraceStore.get_trace,async get_trace(run_id: str) -> TraceRecord | None,,,models | sqlite3,pure,yes,3,Get a complete trace by run_id.
logs,L6,traces_store,TraceStore.list_traces,"async list_traces(tenant_id: str | None, limit: int, offset: int) -> list[TraceSummary]",,,models | sqlite3,pure,yes,8,"List traces, optionally filtered by tenant."
logs,L6,traces_store,TraceStore.record_step,"async record_step(run_id: str, step_index: int, skill_name: str, params: dict[str, Any], status: TraceStatus, outcome_category: str, outcome_code: str | None, outcome_data: dict[str, Any] | None, cost_cents: float, duration_ms: float, retry_count: int) -> None",,,models | sqlite3,pure,yes,16,Record a step in the trace.
logs,L6,traces_store,TraceStore.start_trace,"async start_trace(run_id: str, correlation_id: str, tenant_id: str, agent_id: str | None, plan: list[dict[str, Any]]) -> None",,,models | sqlite3,pure,yes,10,Start a new trace record.
logs,L6,traces_store,generate_correlation_id,generate_correlation_id() -> str,,str | uuid4,models | sqlite3,pure,no,3,Generate a unique correlation ID for tracing.
logs,L6,traces_store,generate_run_id,generate_run_id() -> str,,uuid4,models | sqlite3,pure,no,3,Generate a unique run ID.
overview,L5,overview_facade,CostPeriod.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,isoformat,__future__ | asyncio | overview_facade_driver | time,pure,no,5,
overview,L5,overview_facade,CostsResult.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,to_dict,__future__ | asyncio | overview_facade_driver | time,pure,no,11,
overview,L5,overview_facade,DecisionItem.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,isoformat,__future__ | asyncio | overview_facade_driver | time,pure,no,10,
overview,L5,overview_facade,DecisionsCountResult.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,,__future__ | asyncio | overview_facade_driver | time,pure,no,6,
overview,L5,overview_facade,DecisionsResult.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,to_dict,__future__ | asyncio | overview_facade_driver | time,pure,no,7,
overview,L5,overview_facade,DomainCount.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,,__future__ | asyncio | overview_facade_driver | time,pure,no,7,
overview,L5,overview_facade,HighlightsResult.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,isoformat | to_dict,__future__ | asyncio | overview_facade_driver | time,pure,no,6,
overview,L5,overview_facade,LimitCostItem.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,,__future__ | asyncio | overview_facade_driver | time,pure,no,10,
overview,L5,overview_facade,OverviewFacade.__init__,__init__() -> None,?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,OverviewFacadeDriver,__future__ | asyncio | overview_facade_driver | time,pure,no,3,Initialize the facade with its driver.
overview,L5,overview_facade,OverviewFacade.get_costs,"async get_costs(session: AsyncSession, tenant_id: str, period_days: int) -> CostsResult",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,CostPeriod | CostsResult | LimitCostItem | append | fetch_breach_stats | fetch_budget_limits | fetch_run_cost | float | max | timedelta | utc_now,__future__ | asyncio | overview_facade_driver | time,pure,yes,56,Get cost intelligence (O2).
overview,L5,overview_facade,OverviewFacade.get_decisions,"async get_decisions(session: AsyncSession, tenant_id: str, source_domain: Optional[str], priority: Optional[str], limit: int, offset: int) -> DecisionsResult",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,DecisionItem | DecisionsResult | append | fetch_pending_incidents | fetch_pending_proposals | len | replace | sort,__future__ | asyncio | overview_facade_driver | time,pure,yes,91,Get pending decisions (O2).
overview,L5,overview_facade,OverviewFacade.get_decisions_count,"async get_decisions_count(session: AsyncSession, tenant_id: str) -> DecisionsCountResult",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,DecisionsCountResult | fetch_incident_decision_counts | fetch_proposal_count,__future__ | asyncio | overview_facade_driver | time,pure,yes,36,Get decisions count summary (O2).
overview,L5,overview_facade,OverviewFacade.get_highlights,"async get_highlights(session: AsyncSession, tenant_id: str) -> HighlightsResult",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,DomainCount | HighlightsResult | SystemPulse | fetch_breach_counts | fetch_incident_counts | fetch_last_activity | fetch_proposal_counts | fetch_run_counts | timedelta | utc_now,__future__ | asyncio | overview_facade_driver | time,pure,yes,67,Get system highlights (O1).
overview,L5,overview_facade,OverviewFacade.get_recovery_stats,"async get_recovery_stats(session: AsyncSession, tenant_id: str, period_days: int) -> RecoveryStatsResult",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,CostPeriod | RecoveryStatsResult | fetch_recovery_stats | round | timedelta | utc_now,__future__ | asyncio | overview_facade_driver | time,pure,yes,28,Get recovery statistics (O3).
overview,L5,overview_facade,RecoveryStatsResult.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,to_dict,__future__ | asyncio | overview_facade_driver | time,pure,no,9,
overview,L5,overview_facade,SystemPulse.to_dict,"to_dict() -> Dict[str, Any]",?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,,__future__ | asyncio | overview_facade_driver | time,pure,no,9,
overview,L5,overview_facade,get_overview_facade,get_overview_facade() -> OverviewFacade,?:overview | ?:overview_facade | L4:operation_registry | L4:overview_handler | L5:__init__,OverviewFacade,__future__ | asyncio | overview_facade_driver | time,pure,no,6,Get the singleton OverviewFacade instance.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_breach_counts,"async fetch_breach_counts(session: AsyncSession, tenant_id: str, since: datetime) -> BreachCountSnapshot",L6:__init__ | L5:overview_facade,BreachCountSnapshot | count | execute | label | one | select | warning | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,20,Fetch limit breach counts from DB (defensive query).
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_breach_stats,"async fetch_breach_stats(session: AsyncSession, tenant_id: str, since: datetime) -> BreachStatsSnapshot",L6:__init__ | L5:overview_facade,BreachStatsSnapshot | coalesce | count | execute | float | label | max | one | select | sum | warning | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,26,Fetch breach statistics (defensive query).
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_budget_limits,"async fetch_budget_limits(session: AsyncSession, tenant_id: str) -> List[LimitSnapshot]",L6:__init__ | L5:overview_facade,LimitSnapshot | all | execute | float | order_by | scalars | select | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,28,Fetch active budget limits.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_incident_counts,"async fetch_incident_counts(session: AsyncSession, tenant_id: str) -> IncidentCountSnapshot",L6:__init__ | L5:overview_facade,IncidentCountSnapshot | and_ | case | count | execute | in_ | label | one | select | sum | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,33,Fetch incident counts from DB.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_incident_decision_counts,"async fetch_incident_decision_counts(session: AsyncSession, tenant_id: str) -> IncidentDecisionCountSnapshot",L6:__init__ | L5:overview_facade,IncidentDecisionCountSnapshot | case | count | execute | in_ | label | one | select | sum | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,26,Fetch incident counts by severity for decisions count.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_last_activity,"async fetch_last_activity(session: AsyncSession, tenant_id: str) -> AuditCountSnapshot",L6:__init__ | L5:overview_facade,AuditCountSnapshot | execute | max | scalar | select | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,11,Fetch last activity timestamp from audit ledger.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_pending_incidents,"async fetch_pending_incidents(session: AsyncSession, tenant_id: str) -> List[IncidentSnapshot]",L6:__init__ | L5:overview_facade,IncidentSnapshot | all | desc | execute | order_by | scalars | select | str | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,27,Fetch pending incidents for decisions projection.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_pending_proposals,"async fetch_pending_proposals(session: AsyncSession, tenant_id: str) -> List[ProposalSnapshot]",L6:__init__ | L5:overview_facade,ProposalSnapshot | all | desc | execute | order_by | scalars | select | str | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,26,Fetch pending policy proposals for decisions projection.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_proposal_count,"async fetch_proposal_count(session: AsyncSession, tenant_id: str) -> int",L6:__init__ | L5:overview_facade,count | execute | label | scalar | select | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,12,Fetch count of pending policy proposals.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_proposal_counts,"async fetch_proposal_counts(session: AsyncSession, tenant_id: str) -> ProposalCountSnapshot",L6:__init__ | L5:overview_facade,ProposalCountSnapshot | case | count | execute | label | one | select | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,18,Fetch policy proposal counts from DB.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_recovery_stats,"async fetch_recovery_stats(session: AsyncSession, tenant_id: str, since: datetime) -> RecoverySnapshot",L6:__init__ | L5:overview_facade,RecoverySnapshot | case | count | execute | in_ | label | one | select | sum | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,37,Fetch incident recovery statistics.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_run_cost,"async fetch_run_cost(session: AsyncSession, tenant_id: str, since: datetime) -> RunCostSnapshot",L6:__init__ | L5:overview_facade,RunCostSnapshot | coalesce | execute | int | label | scalar | select | sum | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,17,Fetch total LLM run cost from worker runs.
overview,L6,overview_facade_driver,OverviewFacadeDriver.fetch_run_counts,"async fetch_run_counts(session: AsyncSession, tenant_id: str) -> RunCountSnapshot",L6:__init__ | L5:overview_facade,RunCountSnapshot | case | count | execute | label | one | select | sum | where,asyncio | audit_ledger | killswitch | policy | policy_control_plane | sqlalchemy | tenant | time,db_write,yes,20,Fetch worker run counts from DB.
policies,L5,ast,BlockAction.to_dict,"to_dict() -> dict[str, Any]",?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,2,
policies,L5,ast,Clause.__post_init__,__post_init__() -> None,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,ValueError,__future__,pure,no,4,
policies,L5,ast,Clause.to_dict,"to_dict() -> dict[str, Any]",?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,to_dict,__future__,pure,no,5,
policies,L5,ast,ExistsPredicate.to_dict,"to_dict() -> dict[str, Any]",?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,5,
policies,L5,ast,LogicalCondition.to_dict,"to_dict() -> dict[str, Any]",?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,to_dict,__future__,pure,no,7,
policies,L5,ast,PolicyAST.__post_init__,__post_init__() -> None,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,ValueError,__future__,pure,no,4,
policies,L5,ast,PolicyAST.compute_hash,compute_hash() -> str,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,encode | hexdigest | sha256 | to_json,__future__,pure,no,12,Compute deterministic SHA256 hash of the AST.
policies,L5,ast,PolicyAST.mode,mode() -> Mode,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,3,Convenience accessor for policy mode.
policies,L5,ast,PolicyAST.name,name() -> str,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,3,Convenience accessor for policy name.
policies,L5,ast,PolicyAST.scope,scope() -> Scope,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,3,Convenience accessor for policy scope.
policies,L5,ast,PolicyAST.to_dict,"to_dict() -> dict[str, Any]",?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,to_dict,__future__,pure,no,6,Convert AST to dictionary (serializable).
policies,L5,ast,PolicyAST.to_json,to_json(indent: int | None) -> str,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,dumps | to_dict,__future__,pure,no,3,Convert AST to JSON string.
policies,L5,ast,PolicyAST.version,version() -> int,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,3,Convenience accessor for policy version.
policies,L5,ast,PolicyMetadata.__post_init__,__post_init__() -> None,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,ValueError | strip,__future__,pure,no,7,
policies,L5,ast,PolicyMetadata.to_dict,"to_dict() -> dict[str, Any]",?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,7,
policies,L5,ast,Predicate.to_dict,"to_dict() -> dict[str, Any]",?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,7,
policies,L5,ast,RequireApprovalAction.to_dict,"to_dict() -> dict[str, Any]",?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,2,
policies,L5,ast,WarnAction.to_dict,"to_dict() -> dict[str, Any]",?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,,__future__,pure,no,2,
policies,L5,ast,is_block_action,is_block_action(action: Action) -> bool,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,isinstance,__future__,pure,no,3,Check if action is a BLOCK action.
policies,L5,ast,is_exists_predicate,is_exists_predicate(condition: Condition) -> bool,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,isinstance,__future__,pure,no,3,Check if condition is an exists predicate.
policies,L5,ast,is_logical_condition,is_logical_condition(condition: Condition) -> bool,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,isinstance,__future__,pure,no,3,Check if condition is a compound logical condition.
policies,L5,ast,is_predicate,is_predicate(condition: Condition) -> bool,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,isinstance,__future__,pure,no,3,Check if condition is a simple predicate.
policies,L5,ast,is_require_approval_action,is_require_approval_action(action: Action) -> bool,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,isinstance,__future__,pure,no,3,Check if action is a REQUIRE_APPROVAL action.
policies,L5,ast,is_warn_action,is_warn_action(action: Action) -> bool,?:parser | ?:ir_builder | ?:visitors | ?:__init__ | L5:ir_compiler | L5:visitors | L5:compiler_parser | L5:ir_builder | L5:validator | L5:dsl_parser,isinstance,__future__,pure,no,3,Check if action is a WARN action.
policies,L5,authority_checker,OverrideAuthorityChecker._is_override_active,_is_override_active(override_authority: Any) -> bool,?:__init__,getattr | hasattr | is_override_active | now,,pure,no,17,Check if override is currently active.
policies,L5,authority_checker,OverrideAuthorityChecker.check,check(override_authority: Any) -> OverrideCheckResult,?:__init__,OverrideCheckResult | _is_override_active | getattr | int | max | now | total_seconds,,pure,no,69,Check override authority status.
policies,L5,authority_checker,OverrideAuthorityChecker.check_from_dict,"check_from_dict(policy_id: str, currently_overridden: bool, override_allowed: bool, override_by: Optional[str], override_reason: Optional[str], override_started_at: Optional[datetime], override_expires_at: Optional[datetime]) -> OverrideCheckResult",?:__init__,OverrideCheckResult | int | max | now | total_seconds,,pure,no,72,Check override status from individual fields.
policies,L5,authority_checker,OverrideCheckResult.to_dict,"to_dict() -> dict[str, Any]",?:__init__,isoformat,,pure,no,16,Convert to dictionary for API responses.
policies,L5,authority_checker,should_skip_enforcement,should_skip_enforcement(override_authority: Any) -> bool,?:__init__,OverrideAuthorityChecker | check,,pure,no,13,Quick helper to check if enforcement should be skipped.
policies,L5,binding_moment_enforcer,_check_fields_changed,"_check_fields_changed(policy: Any, context: Dict[str, Any]) -> bool",?:prevention_engine | L5:prevention_engine | ?:test_binding_moment_enforcer,get | getattr,,pure,no,17,Check if monitored fields changed (for ON_CHANGE binding).
policies,L5,binding_moment_enforcer,_mark_evaluated,"_mark_evaluated(run_id: str, policy_id: str) -> None",?:prevention_engine | L5:prevention_engine | ?:test_binding_moment_enforcer,add | set,,db_write,no,5,Mark a policy as evaluated for a run.
policies,L5,binding_moment_enforcer,_was_evaluated,"_was_evaluated(run_id: str, policy_id: str) -> bool",?:prevention_engine | L5:prevention_engine | ?:test_binding_moment_enforcer,,,pure,no,3,Check if a policy was already evaluated for a run.
policies,L5,binding_moment_enforcer,clear_run_cache,clear_run_cache(run_id: str) -> None,?:prevention_engine | L5:prevention_engine | ?:test_binding_moment_enforcer,,,pure,no,4,Clear the evaluation cache for a run (call on run completion).
policies,L5,binding_moment_enforcer,get_binding_moment,get_binding_moment(policy: Any) -> BindingMoment,?:prevention_engine | L5:prevention_engine | ?:test_binding_moment_enforcer,BindingMoment | getattr | isinstance | lower | warning,,pure,no,39,Get the binding moment for a policy.
policies,L5,binding_moment_enforcer,should_evaluate_policy,"should_evaluate_policy(policy: Any, context: Dict[str, Any], evaluation_point: EvaluationPoint) -> BindingDecision",?:prevention_engine | L5:prevention_engine | ?:test_binding_moment_enforcer,BindingDecision | _check_fields_changed | _mark_evaluated | _was_evaluated | debug | get | get_binding_moment | getattr | str,,pure,no,110,Determine if a policy should be evaluated at this point.
policies,L5,claim_decision_engine,determine_claim_status,"determine_claim_status(evaluation_result: Dict[str, Any]) -> str",,get,,pure,no,20,Determine the execution status from an evaluation result.
policies,L5,claim_decision_engine,get_result_confidence,"get_result_confidence(evaluation_result: Dict[str, Any]) -> float",,get,,pure,no,15,Extract confidence from evaluation result with default fallback.
policies,L5,claim_decision_engine,is_candidate_claimable,is_candidate_claimable(confidence: Optional[float]) -> bool,,,,pure,no,21,Determine if a candidate is eligible for claiming based on confidence.
policies,L5,compiler_parser,ParseError.__init__,"__init__(message: str, token: Token)",,__init__ | super,grammar | nodes | tokenizer,pure,no,4,
policies,L5,compiler_parser,Parser.__init__,__init__(tokens: List[Token]),,,grammar | nodes | tokenizer,pure,no,3,
policies,L5,compiler_parser,Parser.advance,advance() -> Token,,,grammar | nodes | tokenizer,pure,no,5,Advance to next token and return current.
policies,L5,compiler_parser,Parser.current,current() -> Token,,len,grammar | nodes | tokenizer,pure,no,5,Get current token.
policies,L5,compiler_parser,Parser.expect,expect(token_type: TokenType) -> Token,,ParseError | advance,grammar | nodes | tokenizer,pure,no,5,Expect current token to be of given type.
policies,L5,compiler_parser,Parser.from_source,from_source(source: str) -> 'Parser',,Tokenizer | cls | tokenize,grammar | nodes | tokenizer,pure,no,5,Create parser from source code.
policies,L5,compiler_parser,Parser.match,match(*token_types) -> bool,,,grammar | nodes | tokenizer,pure,no,3,Check if current token matches any of the given types.
policies,L5,compiler_parser,Parser.parse,parse() -> ProgramNode,,ParseError | ProgramNode | append | match | parse_import | parse_policy_decl | parse_rule_decl,grammar | nodes | tokenizer,pure,no,20,Parse the token stream into an AST.
policies,L5,compiler_parser,Parser.parse_action_block,parse_action_block() -> ActionBlockNode,,ActionBlockNode | ParseError | advance | parse_route_target,grammar | nodes | tokenizer,pure,no,26,Parse an action block.
policies,L5,compiler_parser,Parser.parse_and_expr,parse_and_expr() -> ExprNode,,BinaryOpNode | advance | match | parse_not_expr,grammar | nodes | tokenizer,pure,no,16,Parse AND expression.
policies,L5,compiler_parser,Parser.parse_category,parse_category() -> PolicyCategory,,ParseError | advance | items | match,grammar | nodes | tokenizer,pure,no,16,Parse a policy category.
policies,L5,compiler_parser,Parser.parse_comparison,parse_comparison() -> ExprNode,,BinaryOpNode | advance | parse_value,grammar | nodes | tokenizer,pure,no,25,Parse comparison expression.
policies,L5,compiler_parser,Parser.parse_condition_block,parse_condition_block() -> ConditionBlockNode,,ConditionBlockNode | expect | parse_action_block | parse_expr,grammar | nodes | tokenizer,pure,no,13,Parse a when/then condition block.
policies,L5,compiler_parser,Parser.parse_expr,parse_expr() -> ExprNode,,parse_or_expr,grammar | nodes | tokenizer,pure,no,3,Parse an expression.
policies,L5,compiler_parser,Parser.parse_func_call,parse_func_call(callee: ExprNode) -> FuncCallNode,,FuncCallNode | advance | append | expect | match | parse_expr,grammar | nodes | tokenizer,pure,no,19,Parse a function call.
policies,L5,compiler_parser,Parser.parse_import,parse_import() -> ImportNode,,ImportNode | expect,grammar | nodes | tokenizer,pure,no,9,Parse an import statement.
policies,L5,compiler_parser,Parser.parse_not_expr,parse_not_expr() -> ExprNode,,UnaryOpNode | advance | match | parse_comparison | parse_not_expr,grammar | nodes | tokenizer,pure,no,12,Parse NOT expression.
policies,L5,compiler_parser,Parser.parse_or_expr,parse_or_expr() -> ExprNode,,BinaryOpNode | advance | match | parse_and_expr,grammar | nodes | tokenizer,pure,no,16,Parse OR expression.
policies,L5,compiler_parser,Parser.parse_policy_body,parse_policy_body() -> List[ASTNode],,ParseError | append | match | parse_action_block | parse_condition_block | parse_priority | parse_rule_decl | parse_rule_ref | peek,grammar | nodes | tokenizer,pure,no,21,Parse policy body contents.
policies,L5,compiler_parser,Parser.parse_policy_decl,parse_policy_decl() -> PolicyDeclNode,,PolicyDeclNode | expect | parse_category | parse_policy_body,grammar | nodes | tokenizer,pure,no,17,Parse a policy declaration.
policies,L5,compiler_parser,Parser.parse_priority,parse_priority() -> PriorityNode,,PriorityNode | expect | int,grammar | nodes | tokenizer,pure,no,9,Parse a priority declaration.
policies,L5,compiler_parser,Parser.parse_route_target,parse_route_target() -> RouteTargetNode,,RouteTargetNode | expect,grammar | nodes | tokenizer,pure,no,9,Parse a route target.
policies,L5,compiler_parser,Parser.parse_rule_body,parse_rule_body() -> List[ASTNode],,ParseError | append | match | parse_action_block | parse_condition_block | parse_priority,grammar | nodes | tokenizer,pure,no,15,Parse rule body contents.
policies,L5,compiler_parser,Parser.parse_rule_decl,parse_rule_decl() -> RuleDeclNode,,RuleDeclNode | expect | parse_category | parse_rule_body,grammar | nodes | tokenizer,pure,no,17,Parse a rule declaration.
policies,L5,compiler_parser,Parser.parse_rule_ref,parse_rule_ref() -> RuleRefNode,,RuleRefNode | expect,grammar | nodes | tokenizer,pure,no,9,Parse a rule reference.
policies,L5,compiler_parser,Parser.parse_value,parse_value() -> ExprNode,,AttrAccessNode | IdentNode | LiteralNode | ParseError | advance | expect | float | int | match | parse_expr | parse_func_call,grammar | nodes | tokenizer,pure,no,55,Parse a value expression.
policies,L5,compiler_parser,Parser.peek,peek(offset: int) -> Token,,len,grammar | nodes | tokenizer,pure,no,6,Peek ahead by offset tokens.
policies,L5,content_accuracy,ContentAccuracyResult.to_dict,"to_dict() -> Dict[str, Any]",?:__init__ | ?:prevention_hook | L5:prevention_hook,len | str,,pure,no,21,
policies,L5,content_accuracy,ContentAccuracyValidator.__init__,"__init__(strict_mode: bool, required_fields: Optional[List[str]], domain_terms: Optional[Dict[str, List[str]]])",?:__init__ | ?:prevention_hook | L5:prevention_hook,compile,,pure,no,14,
policies,L5,content_accuracy,ContentAccuracyValidator._claims_affirmative,_claims_affirmative(claim: str) -> bool,?:__init__ | ?:prevention_hook | L5:prevention_hook,search,,pure,no,17,Check if the claim makes an affirmative statement.
policies,L5,content_accuracy,ContentAccuracyValidator._detect_assertion_type,_detect_assertion_type(text: str) -> AssertionType,?:__init__ | ?:prevention_hook | L5:prevention_hook,lower | search,,pure,no,20,Detect the type of assertion in the text.
policies,L5,content_accuracy,ContentAccuracyValidator._extract_claim,"_extract_claim(text: str, terms: List[str]) -> str",?:__init__ | ?:prevention_hook | L5:prevention_hook,escape | search | split | strip,,pure,no,10,Extract the sentence containing the claim about these terms.
policies,L5,content_accuracy,ContentAccuracyValidator._get_nested_value,"_get_nested_value(data: Dict[str, Any], key: str) -> Any",?:__init__ | ?:prevention_hook | L5:prevention_hook,get | isinstance | split,,pure,no,15,Get a value from nested dict using dot notation.
policies,L5,content_accuracy,ContentAccuracyValidator.validate,"validate(output: str, context: Dict[str, Any], user_query: Optional[str]) -> ContentAccuracyResult",?:__init__ | ?:prevention_hook | L5:prevention_hook,AssertionCheck | ContentAccuracyResult | _claims_affirmative | _detect_assertion_type | _extract_claim | _get_nested_value | any | append | escape | items | join | len | search,,pure,no,93,Validate that output content is accurate given the context.
policies,L5,content_accuracy,validate_content_accuracy,"validate_content_accuracy(output: str, context: Dict[str, Any], user_query: Optional[str], strict_mode: bool) -> ContentAccuracyResult",?:__init__ | ?:prevention_hook | L5:prevention_hook,ContentAccuracyValidator | validate,,pure,no,22,Convenience function to validate content accuracy.
policies,L5,cus_enforcement_engine,CusEnforcementEngine.evaluate,async evaluate(**kwargs) -> EnforcementDecision,?:cus_enforcement_service | L4:policies_handler,NotImplementedError,,pure,yes,3,Evaluate enforcement policy  stub.
policies,L5,cus_enforcement_engine,CusEnforcementEngine.evaluate_batch,async evaluate_batch(**kwargs) -> list[EnforcementDecision],?:cus_enforcement_service | L4:policies_handler,NotImplementedError,,pure,yes,3,Evaluate enforcement for batch  stub.
policies,L5,cus_enforcement_engine,CusEnforcementEngine.get_enforcement_status,"async get_enforcement_status(**kwargs) -> dict[str, Any]",?:cus_enforcement_service | L4:policies_handler,NotImplementedError,,pure,yes,3,Get enforcement status  stub.
policies,L5,cus_enforcement_engine,EnforcementDecision.__post_init__,__post_init__() -> None,?:cus_enforcement_service | L4:policies_handler,,,pure,no,3,
policies,L5,cus_enforcement_engine,get_cus_enforcement_engine,get_cus_enforcement_engine() -> CusEnforcementEngine,?:cus_enforcement_service | L4:policies_handler,CusEnforcementEngine,,pure,no,3,Get the CusEnforcementEngine instance.
policies,L5,cus_enforcement_engine,get_cus_enforcement_service,get_cus_enforcement_service() -> CusEnforcementService,?:cus_enforcement_service | L4:policies_handler,get_cus_enforcement_engine,,pure,no,3,Get the CusEnforcementService instance (backward alias).
policies,L5,customer_policy_read_engine,CustomerPolicyReadService.__init__,__init__(session: 'Session'),L4:policies_bridge | L3:customer_policies_adapter,get_policy_read_driver,policy_read_driver | sqlmodel,pure,no,8,Initialize with database session (passed to driver).
policies,L5,customer_policy_read_engine,CustomerPolicyReadService._calculate_period_bounds,"_calculate_period_bounds(now: datetime, period: str) -> tuple[datetime, datetime]",L4:policies_bridge | L3:customer_policies_adapter,replace | timedelta | weekday,policy_read_driver | sqlmodel,pure,no,32,Calculate period start and reset time.
policies,L5,customer_policy_read_engine,CustomerPolicyReadService._get_budget_constraint,_get_budget_constraint(tenant_id: str) -> Optional[BudgetConstraint],L4:policies_bridge | L3:customer_policies_adapter,BudgetConstraint | _calculate_period_bounds | get_tenant_budget_settings | get_usage_sum_since | isoformat | max | now | round,policy_read_driver | sqlmodel,pure,no,35,Get budget constraint for tenant.
policies,L5,customer_policy_read_engine,CustomerPolicyReadService._get_guardrails,_get_guardrails() -> List[GuardrailSummary],L4:policies_bridge | L3:customer_policies_adapter,GuardrailSummary | list_all_guardrails,policy_read_driver | sqlmodel,pure,no,21,Get all guardrails.
policies,L5,customer_policy_read_engine,CustomerPolicyReadService._get_rate_limits,_get_rate_limits(tenant_id: str) -> List[RateLimit],L4:policies_bridge | L3:customer_policies_adapter,RateLimit,policy_read_driver | sqlmodel,pure,no,16,Get rate limits for tenant.
policies,L5,customer_policy_read_engine,CustomerPolicyReadService.get_guardrail_detail,"get_guardrail_detail(tenant_id: str, guardrail_id: str) -> Optional[GuardrailSummary]",L4:policies_bridge | L3:customer_policies_adapter,GuardrailSummary | ValueError | get_guardrail_by_id,policy_read_driver | sqlmodel,pure,no,35,Get guardrail detail.
policies,L5,customer_policy_read_engine,CustomerPolicyReadService.get_policy_constraints,get_policy_constraints(tenant_id: str) -> PolicyConstraints,L4:policies_bridge | L3:customer_policies_adapter,PolicyConstraints | ValueError | _get_budget_constraint | _get_guardrails | _get_rate_limits | isoformat | now,policy_read_driver | sqlmodel,pure,no,34,Get policy constraints for a tenant.
policies,L5,customer_policy_read_engine,get_customer_policy_read_service,get_customer_policy_read_service(session: 'Session') -> CustomerPolicyReadService,L4:policies_bridge | L3:customer_policies_adapter,CustomerPolicyReadService,policy_read_driver | sqlmodel,pure,no,15,Factory function for CustomerPolicyReadService.
policies,L5,decorator,_extract_subject,"_extract_subject(args: tuple, kwargs: dict, extractor: Optional[Callable[..., str]]) -> str",?:__init__,extractor | hasattr | str,__future__ | kernel,pure,no,25,Extract subject from function arguments.
policies,L5,decorator,_extract_tenant_id,"_extract_tenant_id(args: tuple, kwargs: dict, extractor: Optional[Callable[..., str]]) -> str",?:__init__,extractor | hasattr | str | values,__future__ | kernel,pure,no,27,Extract tenant_id from function arguments.
policies,L5,decorator,governed,"governed(capability: str, execution_vector: str, extract_tenant: Optional[Callable[..., str]], extract_subject: Optional[Callable[..., str]], reason: Optional[str]) -> Callable[[F], F]",?:__init__,InvocationContext | _extract_subject | _extract_tenant_id | func | invoke | invoke_async | iscoroutinefunction | wraps,__future__ | kernel,pure,no,88,Decorator that routes execution through the ExecutionKernel.
policies,L5,degraded_mode,DegradedModeStatus.get_inactive,get_inactive() -> 'DegradedModeStatus',?:test_degraded_mode,cls,,pure,no,3,Get inactive status.
policies,L5,degraded_mode,enter_degraded_mode,"enter_degraded_mode(reason: str, entered_by: str, existing_runs_action: str) -> DegradedModeTransition",?:test_degraded_mode,DegradedModeStatus | DegradedModeTransition | isoformat | now | warning,,pure,no,47,Enter degraded mode.
policies,L5,degraded_mode,exit_degraded_mode,exit_degraded_mode(exited_by: str) -> DegradedModeTransition,?:test_degraded_mode,DegradedModeTransition | get_inactive | info | isoformat | now,,pure,no,37,Exit degraded mode.
policies,L5,degraded_mode,get_degraded_mode_status,get_degraded_mode_status() -> DegradedModeStatus,?:test_degraded_mode,get_inactive,,pure,no,13,Get current degraded mode status.
policies,L5,degraded_mode,get_existing_run_action,get_existing_run_action() -> str,?:test_degraded_mode,,,pure,no,11,Get action for existing/in-flight runs in degraded mode.
policies,L5,degraded_mode,is_degraded_mode_active,is_degraded_mode_active() -> bool,?:test_degraded_mode,,,pure,no,9,Check if degraded mode is currently active.
policies,L5,degraded_mode,should_allow_new_run,should_allow_new_run(run_id: str) -> bool,?:test_degraded_mode,is_degraded_mode_active | warning,,pure,no,20,Check if a new run should be allowed.
policies,L5,deterministic_engine,DeterministicEngine.__init__,__init__(),?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,IntentEmitter | _register_builtins,grammar | intent | ir_nodes,pure,no,3,
policies,L5,deterministic_engine,DeterministicEngine._action_to_intent_type,_action_to_intent_type(action: ActionType) -> IntentType,?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,get,grammar | intent | ir_nodes,pure,no,9,Convert action to intent type.
policies,L5,deterministic_engine,DeterministicEngine._call_function,"async _call_function(name: str, args: List[Any], context: ExecutionContext, module: IRModule) -> Any",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,_execute_function | get_function,grammar | intent | ir_nodes,pure,yes,22,Call a function (builtin or user-defined).
policies,L5,deterministic_engine,DeterministicEngine._eval_binary_op,"_eval_binary_op(op: str, left: Any, right: Any) -> Any",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,bool,grammar | intent | ir_nodes,pure,no,7,Evaluate binary operation.
policies,L5,deterministic_engine,DeterministicEngine._eval_compare,"_eval_compare(op: str, left: Any, right: Any) -> bool",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,bool | get,grammar | intent | ir_nodes,pure,no,14,Evaluate comparison.
policies,L5,deterministic_engine,DeterministicEngine._eval_unary_op,"_eval_unary_op(op: str, operand: Any) -> Any",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,bool,grammar | intent | ir_nodes,pure,no,5,Evaluate unary operation.
policies,L5,deterministic_engine,DeterministicEngine._execute_function,"async _execute_function(module: IRModule, func: IRFunction, context: ExecutionContext) -> Optional[ActionType]",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,RuntimeError | _execute_instruction | add_trace | get | isinstance | pop_call | push_call,grammar | intent | ir_nodes,pure,yes,56,Execute a single function.
policies,L5,deterministic_engine,DeterministicEngine._execute_instruction,"async _execute_instruction(instr: IRInstruction, registers: Dict[int, Any], context: ExecutionContext, module: IRModule) -> Any",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,IntentPayload | _action_to_intent_type | _call_function | _eval_binary_op | _eval_compare | _eval_unary_op | add_trace | create_intent | get | get_variable | isinstance | set_variable | upper,grammar | intent | ir_nodes,pure,yes,108,Execute a single instruction.
policies,L5,deterministic_engine,DeterministicEngine._register_builtins,"_register_builtins() -> Dict[str, Callable[..., Any]]",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,endswith | get | getattr | isinstance | len | lower | startswith | upper,grammar | intent | ir_nodes,pure,no,14,Register built-in functions.
policies,L5,deterministic_engine,DeterministicEngine.execute,"async execute(module: IRModule, context: ExecutionContext, entry_function: Optional[str]) -> ExecutionResult",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,ExecutionResult | _execute_function | clear | emit_all | get_function | get_functions_by_category | list | str | values,grammar | intent | ir_nodes,pure,yes,69,Execute a compiled policy module.
policies,L5,deterministic_engine,ExecutionContext.__post_init__,__post_init__(),?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,_generate_id,grammar | intent | ir_nodes,pure,no,3,
policies,L5,deterministic_engine,ExecutionContext._generate_id,_generate_id() -> str,?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,encode | hexdigest | sha256,grammar | intent | ir_nodes,pure,no,4,Generate deterministic execution ID from context.
policies,L5,deterministic_engine,ExecutionContext.add_trace,"add_trace(event: str, data: Dict[str, Any]) -> None",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,append,grammar | intent | ir_nodes,pure,no,9,Add event to execution trace.
policies,L5,deterministic_engine,ExecutionContext.get_variable,get_variable(name: str) -> Any,?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,get,grammar | intent | ir_nodes,pure,no,12,Get a variable value.
policies,L5,deterministic_engine,ExecutionContext.pop_call,pop_call() -> Optional[str],?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,pop,grammar | intent | ir_nodes,pure,no,5,Pop function from call stack.
policies,L5,deterministic_engine,ExecutionContext.push_call,push_call(function_name: str) -> None,?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,append,grammar | intent | ir_nodes,pure,no,3,Push function onto call stack.
policies,L5,deterministic_engine,ExecutionContext.set_variable,"set_variable(name: str, value: Any) -> None",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,,grammar | intent | ir_nodes,pure,no,3,Set a variable value.
policies,L5,deterministic_engine,ExecutionResult.to_dict,"to_dict() -> Dict[str, Any]",?:__init__ | ?:dag_executor | ?:worker | L4:dag_executor | ?:apply | ?:mypy_zones | ?:test_m20_runtime,len | to_dict,grammar | intent | ir_nodes,pure,no,13,Convert to dictionary for API response.
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.fetch_debounce_count,"fetch_debounce_count(tenant_id: str, lesson_type: str, metric_type: str, hours: int, threshold_band: Optional[str]) -> int",,,__future__,pure,no,8,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.fetch_expired_deferred,"fetch_expired_deferred(deferred_status: str, limit: int) -> List[Any]",,,__future__,pure,no,3,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.fetch_lesson_by_id,"fetch_lesson_by_id(lesson_id: str, tenant_id: str) -> Optional[Dict[str, Any]]",,,__future__,pure,no,3,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.fetch_lesson_stats,fetch_lesson_stats(tenant_id: str) -> List[Any],,,__future__,pure,no,1,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.fetch_lessons_list,"fetch_lessons_list(tenant_id: str, lesson_type: Optional[str], status: Optional[str], severity: Optional[str], include_synthetic: bool, limit: int, offset: int) -> List[Dict[str, Any]]",,,__future__,pure,no,10,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.insert_lesson,"insert_lesson(lesson_id: str, tenant_id: str, lesson_type: str, severity: Optional[str], source_event_id: str, source_event_type: str, source_run_id: Optional[str], title: str, description: str, proposed_action: Optional[str], detected_pattern: Optional[Dict[str, Any]], now: datetime, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> bool",,,__future__,pure,no,17,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.insert_policy_proposal_from_lesson,"insert_policy_proposal_from_lesson(proposal_id: str, tenant_id: str, title: str, description: str, proposed_action: str, source_lesson_id: str, created_at: datetime, created_by: str) -> bool",,,__future__,pure,no,11,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.update_lesson_converted,"update_lesson_converted(lesson_id: str, tenant_id: str, converted_status: str, proposal_id: str, converted_at: datetime) -> bool",,,__future__,pure,no,8,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.update_lesson_deferred,"update_lesson_deferred(lesson_id: str, tenant_id: str, deferred_status: str, defer_until: datetime) -> bool",,,__future__,pure,no,7,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.update_lesson_dismissed,"update_lesson_dismissed(lesson_id: str, tenant_id: str, dismissed_status: str, dismissed_at: datetime, dismissed_by: str, reason: str) -> bool",,,__future__,pure,no,9,
policies,L5,domain_bridge_capabilities,LessonsQueryCapability.update_lesson_reactivated,"update_lesson_reactivated(lesson_id: str, tenant_id: str, pending_status: str, from_status: str) -> bool",,,__future__,pure,no,7,
policies,L5,domain_bridge_capabilities,LimitsQueryCapability.fetch_budget_limits,"async fetch_budget_limits(tenant_id: str, scope: Optional[str], status: str, limit: int, offset: int) -> List[Dict[str, Any]]",,,__future__,pure,yes,8,
policies,L5,domain_bridge_capabilities,LimitsQueryCapability.fetch_limit_by_id,"async fetch_limit_by_id(tenant_id: str, limit_id: str) -> Optional[Dict[str, Any]]",,,__future__,pure,yes,3,
policies,L5,domain_bridge_capabilities,LimitsQueryCapability.fetch_limits,"async fetch_limits(tenant_id: str, category: str, status: str, scope: Optional[str], enforcement: Optional[str], limit_type: Optional[str], created_after: Optional[datetime], created_before: Optional[datetime], limit: int, offset: int) -> tuple[List[Dict[str, Any]], int]",,,__future__,pure,yes,13,
policies,L5,domain_bridge_capabilities,PolicyLimitsCapability.add_integrity,add_integrity(integrity: Any) -> None,,,__future__,pure,no,1,
policies,L5,domain_bridge_capabilities,PolicyLimitsCapability.add_limit,add_limit(limit: Any) -> None,,,__future__,pure,no,1,
policies,L5,domain_bridge_capabilities,PolicyLimitsCapability.fetch_limit_by_id,"async fetch_limit_by_id(tenant_id: str, limit_id: str) -> Any",,,__future__,pure,yes,1,
policies,L5,domain_bridge_capabilities,PolicyLimitsCapability.flush,async flush() -> None,,,__future__,pure,yes,1,
policies,L5,dsl_parser,Lexer.__init__,__init__(source: str) -> None,,compile,__future__ | ast,pure,no,6,
policies,L5,dsl_parser,Lexer._advance,_advance(text: str) -> None,,,__future__ | ast,pure,no,9,"Advance position, tracking line/column."
policies,L5,dsl_parser,Lexer._convert_value,"_convert_value(token_type: str, text: str) -> Any",,float | int,__future__ | ast,pure,no,12,Convert token text to appropriate Python value.
policies,L5,dsl_parser,Lexer.tokenize,tokenize() -> list[Token],,ParseError | ParseLocation | Token | _advance | _convert_value | append | group | len | match,__future__ | ast,pure,no,35,Convert source text to list of tokens.
policies,L5,dsl_parser,ParseError.__init__,"__init__(message: str, location: ParseLocation | None) -> None",,__init__ | super,__future__ | ast,pure,no,7,
policies,L5,dsl_parser,ParseLocation.__str__,__str__() -> str,,,__future__ | ast,pure,no,2,
policies,L5,dsl_parser,Parser.__init__,__init__(tokens: list[Token]) -> None,,,__future__ | ast,pure,no,3,
policies,L5,dsl_parser,Parser._parse_actions,_parse_actions() -> list[Action],,_try_parse_action | append | error,__future__ | ast,pure,no,14,Parse one or more actions.
policies,L5,dsl_parser,Parser._parse_and_expr,_parse_and_expr() -> Condition,,LogicalCondition | _parse_atom | accept,__future__ | ast,pure,no,13,Parse AND expression: atom (AND atom)*
policies,L5,dsl_parser,Parser._parse_atom,_parse_atom() -> Condition,,ExistsPredicate | _parse_or_expr | _parse_predicate | accept | expect,__future__ | ast,pure,no,17,Parse atomic condition: predicate | exists | ( or_expr )
policies,L5,dsl_parser,Parser._parse_clause,_parse_clause() -> Clause,,Clause | _parse_actions | _parse_condition | expect | tuple,__future__ | ast,pure,no,11,Parse a single when-then clause.
policies,L5,dsl_parser,Parser._parse_clauses,_parse_clauses() -> list[Clause],,_parse_clause | append | error,__future__ | ast,pure,no,11,Parse one or more when-then clauses.
policies,L5,dsl_parser,Parser._parse_condition,_parse_condition() -> Condition,,_parse_or_expr,__future__ | ast,pure,no,3,Parse a condition (or_expr).
policies,L5,dsl_parser,Parser._parse_header,_parse_header() -> PolicyMetadata,,PolicyMetadata | accept | error | expect,__future__ | ast,pure,no,30,Parse policy header.
policies,L5,dsl_parser,Parser._parse_or_expr,_parse_or_expr() -> Condition,,LogicalCondition | _parse_and_expr | accept,__future__ | ast,pure,no,13,Parse OR expression: and_expr (OR and_expr)*
policies,L5,dsl_parser,Parser._parse_predicate,_parse_predicate() -> Predicate,,Predicate | _parse_value | accept | error | expect,__future__ | ast,pure,no,18,Parse simple predicate: metric comparator value
policies,L5,dsl_parser,Parser._parse_value,_parse_value() -> int | float | str | bool,,accept | error,__future__ | ast,pure,no,6,Parse a literal value.
policies,L5,dsl_parser,Parser._try_parse_action,_try_parse_action() -> Action | None,,BlockAction | RequireApprovalAction | WarnAction | accept | expect,__future__ | ast,pure,no,13,"Try to parse an action, return None if not an action."
policies,L5,dsl_parser,Parser.accept,accept(*token_types) -> Token | None,,,__future__ | ast,pure,no,7,Consume token if it matches any of the types.
policies,L5,dsl_parser,Parser.current,current() -> Token,,len,__future__ | ast,pure,no,5,Current token.
policies,L5,dsl_parser,Parser.error,error(message: str) -> ParseError,,ParseError | ParseLocation,__future__ | ast,pure,no,6,Create a parse error at current position.
policies,L5,dsl_parser,Parser.expect,expect(token_type: str) -> Token,,error,__future__ | ast,pure,no,7,Consume and return token of expected type.
policies,L5,dsl_parser,Parser.parse,parse() -> PolicyAST,,PolicyAST | _parse_clauses | _parse_header | expect | tuple,__future__ | ast,pure,no,6,Parse complete policy.
policies,L5,dsl_parser,parse,parse(source: str) -> PolicyAST,,Lexer | Parser | parse | tokenize,__future__ | ast,pure,no,30,Parse Policy DSL text into AST.
policies,L5,dsl_parser,parse_condition,parse_condition(source: str) -> Condition,,Lexer | Parser | _parse_condition | error | tokenize,__future__ | ast,pure,no,25,Parse a standalone condition expression.
policies,L5,eligibility_engine,CapabilityLookup.exists,exists(capability_name: str) -> bool,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,3,Check if capability exists in registry.
policies,L5,eligibility_engine,CapabilityLookup.is_frozen,is_frozen(capability_name: str) -> bool,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,3,Check if capability is frozen.
policies,L5,eligibility_engine,ContractLookup.has_similar_pending,"has_similar_pending(capabilities: tuple[str, ...], window_hours: int) -> tuple[bool, Optional[UUID]]",?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,13,Check for similar pending contracts.
policies,L5,eligibility_engine,DefaultCapabilityLookup.__init__,"__init__(registry: Optional[frozenset[str]], frozen: Optional[frozenset[str]])",?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,frozenset,orchestrator,pure,no,7,
policies,L5,eligibility_engine,DefaultCapabilityLookup.exists,exists(capability_name: str) -> bool,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,2,
policies,L5,eligibility_engine,DefaultCapabilityLookup.is_frozen,is_frozen(capability_name: str) -> bool,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,2,
policies,L5,eligibility_engine,DefaultContractLookup.__init__,"__init__(pending_contracts: Optional[dict[frozenset[str], UUID]])",?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,5,
policies,L5,eligibility_engine,DefaultContractLookup.has_similar_pending,"has_similar_pending(capabilities: tuple[str, ...], window_hours: int) -> tuple[bool, Optional[UUID]]",?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,frozenset | items,orchestrator,pure,no,10,
policies,L5,eligibility_engine,DefaultGovernanceSignalLookup.__init__,"__init__(blocking_scopes: Optional[dict[str, str]])",?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,2,
policies,L5,eligibility_engine,DefaultGovernanceSignalLookup.has_blocking_signal,"has_blocking_signal(scope: str) -> tuple[bool, Optional[str]]",?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,4,
policies,L5,eligibility_engine,DefaultPreApprovalLookup.__init__,__init__(approved_ids: Optional[set[UUID]]),?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,set,orchestrator,pure,no,2,
policies,L5,eligibility_engine,DefaultPreApprovalLookup.has_system_pre_approval,has_system_pre_approval(proposal_id: UUID) -> bool,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,2,
policies,L5,eligibility_engine,DefaultSystemHealthLookup.__init__,__init__(status: SystemHealthStatus),?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,2,
policies,L5,eligibility_engine,DefaultSystemHealthLookup.get_status,get_status() -> SystemHealthStatus,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,2,
policies,L5,eligibility_engine,EligibilityEngine.__init__,"__init__(config: Optional[EligibilityConfig], capability_lookup: Optional[CapabilityLookup], governance_lookup: Optional[GovernanceSignalLookup], health_lookup: Optional[SystemHealthLookup], contract_lookup: Optional[ContractLookup], pre_approval_lookup: Optional[PreApprovalLookup])",?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,DefaultCapabilityLookup | DefaultContractLookup | DefaultGovernanceSignalLookup | DefaultPreApprovalLookup | DefaultSystemHealthLookup,orchestrator,pure,no,26,Initialize eligibility engine with lookups.
policies,L5,eligibility_engine,EligibilityEngine._create_verdict,"_create_verdict(decision: EligibilityDecision, reason: str, results: list[RuleResult], first_failing: Optional[str], blocking_signals: list[str], missing_prerequisites: list[str]) -> EligibilityVerdict",?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,EligibilityVerdict | len | now | tuple,orchestrator,pure,no,21,Create eligibility verdict from evaluation results.
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e001_confidence_threshold,_evaluate_e001_confidence_threshold(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult | float,orchestrator,pure,no,20,E-001: Validator Confidence Threshold
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e002_known_capability,_evaluate_e002_known_capability(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult | append | exists | join | len | list,orchestrator,pure,no,26,E-002: Known Capability Reference
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e003_no_blocking_signal,_evaluate_e003_no_blocking_signal(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult | append | has_blocking_signal | join | len,orchestrator,pure,no,31,E-003: No Blocking Governance Signal
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e004_actionable_type,_evaluate_e004_actionable_type(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult,orchestrator,pure,no,36,E-004: Actionable Issue Type
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e005_source_allowlist,_evaluate_e005_source_allowlist(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult | list,orchestrator,pure,no,19,E-005: Source Allowlist
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e006_not_duplicate,_evaluate_e006_not_duplicate(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult | has_similar_pending | str,orchestrator,pure,no,24,E-006: Not Duplicate
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e100_below_minimum_confidence,_evaluate_e100_below_minimum_confidence(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult | float,orchestrator,pure,no,22,E-100: Below Minimum Confidence
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e101_critical_without_escalation,_evaluate_e101_critical_without_escalation(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult,orchestrator,pure,no,25,E-101: Critical Without Escalation
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e102_frozen_capability,_evaluate_e102_frozen_capability(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult | append | is_frozen | join | len | list,orchestrator,pure,no,25,E-102: Frozen Capability Target
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e103_system_scope_without_preapproval,_evaluate_e103_system_scope_without_preapproval(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult | has_system_pre_approval,orchestrator,pure,no,23,E-103: System Scope Without Founder Pre-Approval
policies,L5,eligibility_engine,EligibilityEngine._evaluate_e104_health_degraded,_evaluate_e104_health_degraded(input: EligibilityInput) -> RuleResult,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,RuleResult | get_status,orchestrator,pure,no,21,E-104: Health Degraded
policies,L5,eligibility_engine,EligibilityEngine.evaluate,evaluate(input: EligibilityInput) -> EligibilityVerdict,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,_create_verdict | append | rule_fn,orchestrator,pure,no,89,Evaluate eligibility for a proposal.
policies,L5,eligibility_engine,GovernanceSignalLookup.has_blocking_signal,"has_blocking_signal(scope: str) -> tuple[bool, Optional[str]]",?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,9,Check if scope has a blocking governance signal.
policies,L5,eligibility_engine,PreApprovalLookup.has_system_pre_approval,has_system_pre_approval(proposal_id: UUID) -> bool,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,3,Check if proposal has system-wide pre-approval.
policies,L5,eligibility_engine,SystemHealthLookup.get_status,get_status() -> SystemHealthStatus,?:__init__ | ?:contract_service | L4:__init__ | ?:test_founder_review_invariants | ?:test_contract_invariants | ?:test_eligibility_invariants,,orchestrator,pure,no,3,Get current system health status.
policies,L5,engine,PolicyEngine.__init__,"__init__(database_url: Optional[str], governor)",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,get | get_policy_engine_driver,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,33,
policies,L5,engine,PolicyEngine._add_windowed_value,"_add_windowed_value(key: str, value: float) -> None",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,append | now,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,6,Add value to windowed tracking.
policies,L5,engine,PolicyEngine._check_business_rules,"async _check_business_rules(request: PolicyEvaluationRequest) -> Tuple[List[PolicyViolation], List[PolicyModification]]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_evaluate_business_rule | append | get,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,27,Check request against business rules.
policies,L5,engine,PolicyEngine._check_compliance,async _check_compliance(request: PolicyEvaluationRequest) -> List[PolicyViolation],?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_evaluate_compliance_rule | append,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,20,Check request against compliance policies.
policies,L5,engine,PolicyEngine._check_cooldown,_check_cooldown(request: PolicyEvaluationRequest) -> Optional[PolicyViolation],?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyViolation | isoformat | items | list | now | total_seconds,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,27,Check if agent is in cooldown.
policies,L5,engine,PolicyEngine._check_ethical_constraints,async _check_ethical_constraints(request: PolicyEvaluationRequest) -> List[PolicyViolation],?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_evaluate_ethical_constraint | append,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,13,Check request against ethical constraints.
policies,L5,engine,PolicyEngine._check_risk_ceilings,"async _check_risk_ceilings(request: PolicyEvaluationRequest) -> Tuple[List[PolicyViolation], List[PolicyModification]]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_evaluate_risk_ceiling | append | now,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,24,Check request against risk ceilings.
policies,L5,engine,PolicyEngine._check_safety_rules,async _check_safety_rules(request: PolicyEvaluationRequest) -> List[PolicyViolation],?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_evaluate_safety_rule | append | get | now | sorted,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,29,Check request against safety rules.
policies,L5,engine,PolicyEngine._classify_recoverability,_classify_recoverability(violation) -> 'RecoverabilityType',?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,13,Classify violation recoverability (GAP 5).
policies,L5,engine,PolicyEngine._classify_severity,_classify_severity(violation) -> 'ViolationSeverity',?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,20,Classify violation severity (GAP 5).
policies,L5,engine,PolicyEngine._evaluate_business_rule,"_evaluate_business_rule(rule: BusinessRule, request: PolicyEvaluationRequest) -> Tuple[Optional[PolicyViolation], Optional[PolicyModification]]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyViolation | get,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,63,Evaluate a business rule.
policies,L5,engine,PolicyEngine._evaluate_compliance_rule,"_evaluate_compliance_rule(policy: Policy, rule: PolicyRule, request: PolicyEvaluationRequest) -> Optional[PolicyViolation]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyViolation | get | list | match | set,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,58,Evaluate a compliance rule.
policies,L5,engine,PolicyEngine._evaluate_ethical_constraint,"_evaluate_ethical_constraint(constraint: EthicalConstraint, request: PolicyEvaluationRequest) -> Optional[PolicyViolation]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyViolation | _extract_text_content | get | lower,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,45,Evaluate a single ethical constraint.
policies,L5,engine,PolicyEngine._evaluate_risk_ceiling,"_evaluate_risk_ceiling(ceiling: RiskCeiling, request: PolicyEvaluationRequest) -> Tuple[Optional[PolicyViolation], Optional[PolicyModification]]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyModification | PolicyViolation | _add_windowed_value | _get_windowed_value | get | warning,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,69,Evaluate a single risk ceiling.
policies,L5,engine,PolicyEngine._evaluate_safety_rule,"_evaluate_safety_rule(rule: SafetyRule, request: PolicyEvaluationRequest) -> Optional[PolicyViolation]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyViolation | _extract_text_content | get | lower | now | search | timedelta,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,86,Evaluate a single safety rule.
policies,L5,engine,PolicyEngine._extract_text_content,_extract_text_content(request: PolicyEvaluationRequest) -> str,?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,append | dumps | join | str,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,17,Extract text content from request for analysis.
policies,L5,engine,PolicyEngine._get_windowed_value,"_get_windowed_value(key: str, window_seconds: int) -> float",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,now | sum | timedelta,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,13,Get accumulated value within time window.
policies,L5,engine,PolicyEngine._is_cache_stale,_is_cache_stale() -> bool,?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,now | total_seconds,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,7,Check if policy cache is stale.
policies,L5,engine,PolicyEngine._load_default_policies,_load_default_policies() -> None,?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,EthicalConstraint | RiskCeiling | SafetyRule | now,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,51,Load default policies when database is unavailable.
policies,L5,engine,PolicyEngine._load_policies,async _load_policies() -> PolicyLoadResult,?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,BusinessRule | BusinessRuleType | EthicalConstraint | EthicalConstraintType | PolicyLoadResult | RiskCeiling | SafetyRule | SafetyRuleType | _get_engine | _load_default_policies | append | connect | error | fetch_business_rules | fetch_ethical_constraints,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,126,Load policies from database.
policies,L5,engine,PolicyEngine._persist_evaluation,"async _persist_evaluation(request: PolicyEvaluationRequest, result: PolicyEvaluationResult) -> None",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | commit | connect | debug | dispose | dumps | insert_evaluation | insert_violation | model_dump,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,54,Persist evaluation to audit log.
policies,L5,engine,PolicyEngine._route_to_governor,async _route_to_governor(violations: List[PolicyViolation]) -> None,?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,error | force_freeze | warning,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,32,Route violations to M18 Governor.
policies,L5,engine,PolicyEngine.acknowledge_violation,"async acknowledge_violation(db, violation_id: str, notes: Optional[str]) -> bool",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | commit | connect | debug | update_violation_acknowledged,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,22,Mark a violation as acknowledged.
policies,L5,engine,PolicyEngine.activate_policy_version,"async activate_policy_version(db, version_id: str, activated_by: str, dry_run: bool) -> Dict[str, Any]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyEvaluationRequest | _get_engine | activate_version | add | append | commit | connect | deactivate_all_versions | dispose | error | evaluate | extend | fetch_active_policies_for_integrity | fetch_conflicts | fetch_dependency_edges,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,228,Activate a policy version with comprehensive pre-activation integrity checks.
policies,L5,engine,PolicyEngine.add_dependency_with_dag_check,"async add_dependency_with_dag_check(db, source_policy: str, target_policy: str, dependency_type: str, resolution_strategy: str, priority: int, description: Optional[str]) -> Dict[str, Any]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | add | append | can_reach | commit | connect | dispose | error | extend | fetch_dependency_edges | get | insert_dependency | pop | set | str,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,90,Add a policy dependency with DAG validation.
policies,L5,engine,PolicyEngine.clear_cooldowns,"async clear_cooldowns(db, agent_id: str, rule_name: Optional[str]) -> int",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,keys | list,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,9,Clear cooldowns for an agent.
policies,L5,engine,PolicyEngine.create_policy_version,"async create_policy_version(db, description: str, created_by: str)",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyVersion | _get_engine | commit | connect | deactivate_all_versions | debug | dispose | dumps | encode | get_current_version | hexdigest | insert_policy_version | int | model_dump | sha256,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,65,Create a new policy version snapshot.
policies,L5,engine,PolicyEngine.create_temporal_policy,"async create_temporal_policy(db, data: Dict)",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,TemporalPolicy | _get_engine | commit | connect | debug | dispose | get | insert_temporal_policy,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,30,Create a temporal policy.
policies,L5,engine,PolicyEngine.evaluate,"async evaluate(request: PolicyEvaluationRequest, db, dry_run: bool) -> PolicyEvaluationResult",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyEvaluationResult | _check_business_rules | _check_compliance | _check_cooldown | _check_ethical_constraints | _check_risk_ceilings | _check_safety_rules | _is_cache_stale | _load_policies | _persist_evaluation | _route_to_governor | append | emit_policy_decision | extend | info,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,162,Evaluate a request against all applicable policies.
policies,L5,engine,PolicyEngine.evaluate_with_context,"async evaluate_with_context(db, action_type, policy_context, proposed_action: Optional[str], target_resource: Optional[str], estimated_cost: Optional[float], data_categories: Optional[List[str]], context: Optional[Dict])",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,EnhancedPolicyEvaluationResult | EnhancedPolicyViolation | PolicyEvaluationRequest | _classify_recoverability | _classify_severity | _load_policies | append | evaluate | get | get_temporal_policies | get_temporal_utilization | join | len | model_copy | now,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,156,Enhanced evaluation with full policy context.
policies,L5,engine,PolicyEngine.get_active_cooldowns,"async get_active_cooldowns(db, agent_id: Optional[str]) -> List[Dict[str, Any]]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,append | items | len | list | now | split | timedelta | total_seconds,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,28,Get all active cooldowns.
policies,L5,engine,PolicyEngine.get_current_version,async get_current_version(db) -> Optional[Dict],?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | connect | debug | fetch_current_active_version | get | isoformat | str,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,26,Get the currently active policy version.
policies,L5,engine,PolicyEngine.get_dependency_graph,async get_dependency_graph(db),?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,DependencyGraph | PolicyConflict | PolicyDependency | _get_engine | append | connect | debug | dispose | fetch_conflicts | fetch_dependencies | get | str,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,64,Get the policy dependency graph.
policies,L5,engine,PolicyEngine.get_ethical_constraints,"async get_ethical_constraints(db, include_inactive: bool) -> List[EthicalConstraint]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_load_policies,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,12,Get all ethical constraints.
policies,L5,engine,PolicyEngine.get_metrics,"async get_metrics(db, hours: int) -> Dict[str, Any]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,max,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,12,Get policy engine metrics.
policies,L5,engine,PolicyEngine.get_policy_conflicts,"async get_policy_conflicts(db, include_resolved: bool) -> List",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyConflict | _get_engine | append | connect | debug | dispose | fetch_conflicts | get | str,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,29,Get policy conflicts.
policies,L5,engine,PolicyEngine.get_policy_versions,"async get_policy_versions(db, limit: int, include_inactive: bool) -> List[Dict]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | connect | debug | fetch_policy_versions | get | isoformat | str,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,35,Get policy version history.
policies,L5,engine,PolicyEngine.get_risk_ceiling,"async get_risk_ceiling(db, ceiling_id: str) -> Optional[RiskCeiling]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_load_policies,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,7,Get a specific risk ceiling.
policies,L5,engine,PolicyEngine.get_risk_ceilings,"async get_risk_ceilings(db, tenant_id: Optional[str], include_inactive: bool) -> List[RiskCeiling]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_load_policies,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,15,Get all risk ceilings.
policies,L5,engine,PolicyEngine.get_safety_rules,"async get_safety_rules(db, tenant_id: Optional[str], include_inactive: bool) -> List[SafetyRule]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_load_policies,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,15,Get all safety rules.
policies,L5,engine,PolicyEngine.get_state,async get_state(db) -> PolicyState,?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyState | len | max,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,17,Get current policy layer state.
policies,L5,engine,PolicyEngine.get_temporal_policies,"async get_temporal_policies(db, metric: Optional[str], include_inactive: bool) -> List",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,TemporalPolicy | _get_engine | append | connect | debug | dispose | fetch_temporal_policies | get | str,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,37,Get temporal (sliding window) policies.
policies,L5,engine,PolicyEngine.get_temporal_storage_stats,"async get_temporal_storage_stats(db) -> Dict[str, Any]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | connect | dispose | error | fetch_temporal_storage_stats | isoformat | str,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,29,Get storage statistics for temporal metrics.
policies,L5,engine,PolicyEngine.get_temporal_utilization,"async get_temporal_utilization(db, policy_id: str, agent_id: Optional[str]) -> Dict",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | connect | debug | dispose | fetch_temporal_metric_sum | fetch_temporal_policy_for_utilization,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,39,Get current utilization for a temporal policy.
policies,L5,engine,PolicyEngine.get_topological_evaluation_order,get_topological_evaluation_order(dependencies: List) -> List[str],?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,add | append | error | get | hasattr | len | pop | set,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,no,47,Get topological order for policy evaluation based on dependencies.
policies,L5,engine,PolicyEngine.get_version_provenance,"async get_version_provenance(db, version_id: str) -> List[Dict]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | connect | debug | fetch_provenance | get | isoformat,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,26,Get provenance (change history) for a version.
policies,L5,engine,PolicyEngine.get_violation,"async get_violation(db, violation_id: str) -> Optional[PolicyViolation]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyViolation | ViolationType | _get_engine | connect | debug | dispose | fetch_violation_by_id | get | isinstance | loads | str,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,37,Get a specific violation by ID.
policies,L5,engine,PolicyEngine.get_violations,"async get_violations(db, violation_type: Optional[ViolationType], agent_id: Optional[str], tenant_id: Optional[str], severity_min: Optional[float], since: Optional[datetime], limit: int) -> List[PolicyViolation]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyViolation | ViolationType | _get_engine | append | connect | debug | dispose | fetch_violations | get | isinstance | loads | str,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,58,Get violations from database with filtering.
policies,L5,engine,PolicyEngine.pre_check,"async pre_check(request_id: str, agent_id: str, goal: str, tenant_id: str) -> Dict[str, Any]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyEvaluationRequest | _is_cache_stale | _load_policies | error | evaluate | warning,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,82,Pre-check policy constraints before run creation.
policies,L5,engine,PolicyEngine.prune_temporal_metrics,"async prune_temporal_metrics(db, retention_hours: int, compact_older_than_hours: int, max_events_per_policy: int) -> Dict[str, Any]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | cap_temporal_events | commit | compact_temporal_events | connect | delete_old_temporal_events | dispose | error | fetch_temporal_stats | get | str,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,61,Prune and compact temporal metric events to prevent storage explosion.
policies,L5,engine,PolicyEngine.reload_policies,async reload_policies(db) -> PolicyLoadResult,?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_load_policies,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,yes,4,Force reload policies from database.
policies,L5,engine,PolicyEngine.reset_risk_ceiling,"async reset_risk_ceiling(db, ceiling_id: str) -> bool",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | commit | connect | debug | reset_risk_ceiling,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,21,Reset a risk ceiling's current value.
policies,L5,engine,PolicyEngine.resolve_conflict,"async resolve_conflict(db, conflict_id: str, resolution: str, resolved_by: str) -> bool",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | commit | connect | debug | resolve_conflict,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,23,Resolve a policy conflict.
policies,L5,engine,PolicyEngine.rollback_to_version,"async rollback_to_version(db, target_version: str, reason: str, rolled_back_by: str)",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | activate_version | commit | connect | error | fetch_version_for_rollback | insert_provenance | mark_version_rolled_back | reload_policies | str | uuid4,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,44,Rollback to a previous policy version.
policies,L5,engine,PolicyEngine.set_governor,set_governor(governor) -> None,?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,3,Set the M18 Governor for violation routing.
policies,L5,engine,PolicyEngine.update_risk_ceiling,"async update_risk_ceiling(db, ceiling_id: str, updates: Dict[str, Any]) -> Optional[RiskCeiling]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | commit | connect | debug | dispose | get_risk_ceiling | reload_policies | update_risk_ceiling,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,26,Update a risk ceiling.
policies,L5,engine,PolicyEngine.update_safety_rule,"async update_safety_rule(db, rule_id: str, updates: Dict[str, Any]) -> Optional[SafetyRule]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | commit | connect | debug | dispose | dumps | items | reload_policies | update_safety_rule,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,37,Update a safety rule.
policies,L5,engine,PolicyEngine.validate_dependency_dag,"async validate_dependency_dag(db) -> Dict[str, Any]",?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,_get_engine | add | append | connect | dfs | dispose | error | fetch_dependency_edges_with_type | get | index | len | reverse | set | str | sum,__future__ | decisions | exc | models | policy_engine_driver | routing,db_write,yes,84,Validate that policy dependencies form a valid DAG (Directed Acyclic Graph).
policies,L5,engine,get_policy_engine,get_policy_engine() -> PolicyEngine,?:identity_chain | ?:policy | ?:workers | ?:__init__ | ?:dag_executor | ?:checkpoint | ?:policies | ?:golden | ?:service | ?:invoke_audit_driver,PolicyEngine | get_governor | info | set_governor | warning,__future__ | decisions | exc | models | policy_engine_driver | routing,pure,no,19,Get singleton policy engine with M18 Governor integration.
policies,L5,failure_mode_handler,get_failure_mode,get_failure_mode() -> FailureMode,?:prevention_engine | L5:prevention_engine | ?:test_failure_mode_handler,FailureMode | get_governance_config | hasattr | lower | str | warning,profile_policy_mode,pure,no,32,Get configured failure mode.
policies,L5,failure_mode_handler,handle_evaluation_error,"handle_evaluation_error(error: Exception, context: Dict[str, Any]) -> FailureDecision",?:prevention_engine | L5:prevention_engine | ?:test_failure_mode_handler,handle_policy_failure,profile_policy_mode,pure,no,16,Handle policy evaluation error.
policies,L5,failure_mode_handler,handle_missing_policy,"handle_missing_policy(context: Dict[str, Any]) -> FailureDecision",?:prevention_engine | L5:prevention_engine | ?:test_failure_mode_handler,handle_policy_failure,profile_policy_mode,pure,no,17,Handle case where no policy exists for the action.
policies,L5,failure_mode_handler,handle_policy_failure,"handle_policy_failure(error: Optional[Exception], context: Dict[str, Any], failure_type: FailureType) -> FailureDecision",?:prevention_engine | L5:prevention_engine | ?:test_failure_mode_handler,FailureDecision | error | get | get_failure_mode | info | isoformat | now | str | warning,profile_policy_mode,pure,no,97,Handle a policy evaluation failure.
policies,L5,failure_mode_handler,handle_timeout,"handle_timeout(context: Dict[str, Any], timeout_seconds: float) -> FailureDecision",?:prevention_engine | L5:prevention_engine | ?:test_failure_mode_handler,Exception | handle_policy_failure,profile_policy_mode,pure,no,16,Handle policy evaluation timeout.
policies,L5,folds,ConstantFolder.__init__,__init__(),?:__init__ | ?:test_m20_optimizer,,ir_nodes,pure,no,3,
policies,L5,folds,ConstantFolder._fold_binary_op,_fold_binary_op(instr: IRBinaryOp) -> FoldResult,?:__init__ | ?:test_m20_optimizer,FoldResult | get,ir_nodes,pure,no,19,Fold binary operation if both operands are constant.
policies,L5,folds,ConstantFolder._fold_compare,_fold_compare(instr: IRCompare) -> FoldResult,?:__init__ | ?:test_m20_optimizer,FoldResult | get,ir_nodes,pure,no,25,Fold comparison if both operands are constant.
policies,L5,folds,ConstantFolder._fold_unary_op,_fold_unary_op(instr: IRUnaryOp) -> FoldResult,?:__init__ | ?:test_m20_optimizer,FoldResult | get,ir_nodes,pure,no,16,Fold unary operation if operand is constant.
policies,L5,folds,ConstantFolder.fold_block,fold_block(block: IRBlock) -> None,?:__init__ | ?:test_m20_optimizer,IRLoadConst | append | try_fold,ir_nodes,pure,no,21,Fold constants in a basic block.
policies,L5,folds,ConstantFolder.fold_function,fold_function(func: IRFunction) -> None,?:__init__ | ?:test_m20_optimizer,fold_block | values,ir_nodes,pure,no,6,Fold constants in a function.
policies,L5,folds,ConstantFolder.fold_module,fold_module(module: IRModule) -> IRModule,?:__init__ | ?:test_m20_optimizer,fold_function | values,ir_nodes,pure,no,10,Fold constants in all functions.
policies,L5,folds,ConstantFolder.try_fold,try_fold(instr: IRInstruction) -> FoldResult,?:__init__ | ?:test_m20_optimizer,FoldResult | _fold_binary_op | _fold_compare | _fold_unary_op | isinstance,ir_nodes,pure,no,21,Try to fold an instruction.
policies,L5,folds,DeadCodeEliminator.__init__,__init__(),?:__init__ | ?:test_m20_optimizer,set,ir_nodes,pure,no,3,
policies,L5,folds,DeadCodeEliminator._eliminate_function,_eliminate_function(func: IRFunction) -> None,?:__init__ | ?:test_m20_optimizer,_find_reachable_blocks | _find_used_instructions | isinstance | keys | len | set | values,ir_nodes,pure,no,26,Eliminate dead code in a function.
policies,L5,folds,DeadCodeEliminator._find_reachable_blocks,_find_reachable_blocks(func: IRFunction) -> Set[str],?:__init__ | ?:test_m20_optimizer,add | append | get | isinstance | pop | set,ir_nodes,db_write,no,24,Find all reachable blocks from entry.
policies,L5,folds,DeadCodeEliminator._find_used_instructions,_find_used_instructions(func: IRFunction) -> Set[int],?:__init__ | ?:test_m20_optimizer,add | isinstance | set | values,ir_nodes,db_write,no,38,Find all instructions whose results are used.
policies,L5,folds,DeadCodeEliminator._mark_governance_critical,_mark_governance_critical(func: IRFunction) -> None,?:__init__ | ?:test_m20_optimizer,add | isinstance | set | values,ir_nodes,db_write,no,12,Mark instructions that are governance-critical (cannot be eliminated).
policies,L5,folds,DeadCodeEliminator.eliminate,eliminate(module: IRModule) -> IRModule,?:__init__ | ?:test_m20_optimizer,_eliminate_function | _mark_governance_critical | values,ir_nodes,pure,no,11,Eliminate dead code in module.
policies,L5,folds,PolicySimplifier.__init__,__init__(),?:__init__ | ?:test_m20_optimizer,,ir_nodes,pure,no,2,
policies,L5,folds,PolicySimplifier._find_mergeable_policies,_find_mergeable_policies(module: IRModule) -> List[List[str]],?:__init__ | ?:test_m20_optimizer,append | items | list | values,ir_nodes,pure,no,26,Find groups of policies that can be safely merged.
policies,L5,folds,PolicySimplifier._merge_policies,"_merge_policies(module: IRModule, policy_names: List[str]) -> None",?:__init__ | ?:test_m20_optimizer,len | sort,ir_nodes,pure,no,24,Merge a group of compatible policies.
policies,L5,folds,PolicySimplifier.simplify,simplify(module: IRModule) -> IRModule,?:__init__ | ?:test_m20_optimizer,_find_mergeable_policies | _merge_policies | len,ir_nodes,pure,no,16,Simplify policies in module.
policies,L5,governance_facade,BootStatusResult.to_dict,"to_dict() -> Dict[str, Any]",L4:policies_handler,isoformat,policy_driver | runtime_switch,pure,no,8,Convert to dictionary.
policies,L5,governance_facade,ConflictResolutionResult.to_dict,"to_dict() -> Dict[str, Any]",L4:policies_handler,isoformat,policy_driver | runtime_switch,pure,no,11,Convert to dictionary.
policies,L5,governance_facade,GovernanceFacade.__init__,__init__(),L4:policies_handler,now,policy_driver | runtime_switch,pure,no,4,Initialize facade with lazy-loaded services.
policies,L5,governance_facade,GovernanceFacade.disable_kill_switch,disable_kill_switch(actor: str) -> KillSwitchResult,L4:policies_handler,KillSwitchResult | enable_governance_runtime | error | info | is_governance_active | now | str,policy_driver | runtime_switch,pure,no,51,Disable kill switch - re-enable governance enforcement.
policies,L5,governance_facade,GovernanceFacade.enable_kill_switch,"enable_kill_switch(reason: str, actor: str) -> KillSwitchResult",L4:policies_handler,KillSwitchResult | disable_governance_runtime | error | is_degraded_mode | is_governance_active | now | str | warning,policy_driver | runtime_switch,pure,no,67,Enable kill switch - disable all governance enforcement.
policies,L5,governance_facade,GovernanceFacade.get_boot_status,get_boot_status() -> BootStatusResult,L4:policies_handler,BootStatusResult | all | error | get | get_policy_facade | int | is_governance_active | now | str | total_seconds | values,policy_driver | runtime_switch,pure,no,70,Get SPINE component health status.
policies,L5,governance_facade,GovernanceFacade.get_governance_state,get_governance_state() -> GovernanceStateResult,L4:policies_handler,GovernanceStateResult | error | fromisoformat | get | get_runtime_state | is_degraded_mode | is_governance_active | str,policy_driver | runtime_switch,pure,no,56,Get current governance state.
policies,L5,governance_facade,GovernanceFacade.list_conflicts,"list_conflicts(tenant_id: Optional[str], status: Optional[str]) -> List[Dict[str, Any]]",L4:policies_handler,error | str,policy_driver | runtime_switch,pure,no,25,List policy conflicts.
policies,L5,governance_facade,GovernanceFacade.resolve_conflict,"resolve_conflict(conflict_id: str, resolution: str, actor: str, notes: Optional[str]) -> ConflictResolutionResult",L4:policies_handler,ConflictResolutionResult | error | info | now | str,policy_driver | runtime_switch,pure,no,55,Manually resolve a policy conflict.
policies,L5,governance_facade,GovernanceFacade.set_mode,"set_mode(mode: GovernanceMode, reason: str, actor: str) -> KillSwitchResult",L4:policies_handler,KillSwitchResult | disable_governance_runtime | enable_governance_runtime | enter_degraded_mode | error | exit_degraded_mode | info | is_degraded_mode | is_governance_active | now | str,policy_driver | runtime_switch,pure,no,77,"Set governance mode (NORMAL, DEGRADED, KILL)."
policies,L5,governance_facade,GovernanceStateResult.to_dict,"to_dict() -> Dict[str, Any]",L4:policies_handler,isoformat,policy_driver | runtime_switch,pure,no,10,Convert to dictionary.
policies,L5,governance_facade,KillSwitchResult.to_dict,"to_dict() -> Dict[str, Any]",L4:policies_handler,isoformat,policy_driver | runtime_switch,pure,no,11,Convert to dictionary.
policies,L5,governance_facade,get_governance_facade,get_governance_facade() -> GovernanceFacade,L4:policies_handler,GovernanceFacade,policy_driver | runtime_switch,pure,no,14,Get the governance facade instance.
policies,L5,grammar,PLangGrammar.get_action_precedence,get_action_precedence(action: str) -> int,?:__init__ | ?:parser | ?:conflict_resolver | ?:dag_sorter | ?:ir_builder | ?:ir_nodes | ?:symbol_table | ?:visitors | ?:nodes | ?:dag_executor,get,,pure,no,3,Get precedence for an action type.
policies,L5,grammar,PLangGrammar.get_category_priority,get_category_priority(category: str) -> int,?:__init__ | ?:parser | ?:conflict_resolver | ?:dag_sorter | ?:ir_builder | ?:ir_nodes | ?:symbol_table | ?:visitors | ?:nodes | ?:dag_executor,get,,pure,no,3,Get priority for a policy category.
policies,L5,grammar,PLangGrammar.is_action,is_action(word: str) -> bool,?:__init__ | ?:parser | ?:conflict_resolver | ?:dag_sorter | ?:ir_builder | ?:ir_nodes | ?:symbol_table | ?:visitors | ?:nodes | ?:dag_executor,,,pure,no,3,Check if word is a valid action.
policies,L5,grammar,PLangGrammar.is_category,is_category(word: str) -> bool,?:__init__ | ?:parser | ?:conflict_resolver | ?:dag_sorter | ?:ir_builder | ?:ir_nodes | ?:symbol_table | ?:visitors | ?:nodes | ?:dag_executor,,,pure,no,3,Check if word is a valid M19 category.
policies,L5,grammar,PLangGrammar.is_keyword,is_keyword(word: str) -> bool,?:__init__ | ?:parser | ?:conflict_resolver | ?:dag_sorter | ?:ir_builder | ?:ir_nodes | ?:symbol_table | ?:visitors | ?:nodes | ?:dag_executor,,,pure,no,3,Check if word is a PLang keyword.
policies,L5,grammar,PLangGrammar.is_operator,is_operator(char: str) -> bool,?:__init__ | ?:parser | ?:conflict_resolver | ?:dag_sorter | ?:ir_builder | ?:ir_nodes | ?:symbol_table | ?:visitors | ?:nodes | ?:dag_executor,,,pure,no,3,Check if character is part of an operator.
policies,L5,intent,Intent.__post_init__,__post_init__(),?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,_generate_id,,pure,no,3,
policies,L5,intent,Intent._generate_id,_generate_id() -> str,?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,dumps | encode | hexdigest | sha256 | to_dict,,pure,no,12,Generate deterministic intent ID.
policies,L5,intent,Intent.from_dict,"from_dict(data: Dict[str, Any]) -> 'Intent'",?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,cls | from_dict | get,,pure,no,16,Create from dictionary.
policies,L5,intent,Intent.to_dict,"to_dict() -> Dict[str, Any]",?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,to_dict,,pure,no,18,Convert to dictionary for M18 consumption.
policies,L5,intent,IntentEmitter.__init__,__init__(),?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,,,pure,no,4,
policies,L5,intent,IntentEmitter.clear,clear() -> None,?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,clear,,pure,no,4,Clear all intents.
policies,L5,intent,IntentEmitter.create_intent,"create_intent(intent_type: IntentType, payload: Optional[IntentPayload], priority: int, source_policy: Optional[str], source_rule: Optional[str], category: Optional[str], requires_confirmation: bool) -> Intent",?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,Intent | IntentPayload | append,,pure,no,38,Create a new intent.
policies,L5,intent,IntentEmitter.emit,async emit(intent: Intent) -> bool,?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,append | get | handler | remove | validate_intent,,pure,yes,35,Emit a validated intent to M18.
policies,L5,intent,IntentEmitter.emit_all,async emit_all() -> List[Intent],?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,append | emit | list,,pure,yes,12,Emit all pending intents.
policies,L5,intent,IntentEmitter.get_emitted,get_emitted() -> List[Intent],?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,list,,pure,no,3,Get all emitted intents.
policies,L5,intent,IntentEmitter.get_pending,get_pending() -> List[Intent],?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,list,,pure,no,3,Get all pending intents.
policies,L5,intent,IntentEmitter.register_handler,"register_handler(intent_type: IntentType, handler: Callable[..., Any]) -> None",?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,append,,pure,no,9,Register a handler for an intent type.
policies,L5,intent,IntentEmitter.validate_intent,async validate_intent(intent: Intent) -> bool,?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,append | len,,pure,yes,40,Validate intent against M19 policy engine.
policies,L5,intent,IntentPayload.from_dict,"from_dict(data: Dict[str, Any]) -> 'IntentPayload'",?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,cls | get,,pure,no,14,Create from dictionary.
policies,L5,intent,IntentPayload.to_dict,"to_dict() -> Dict[str, Any]",?:recovery | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L5:deterministic_engine | L2:recovery | ?:test_m20_runtime,,,pure,no,14,Convert to dictionary for serialization.
policies,L5,interpreter,ActionResult.to_dict,"to_dict() -> dict[str, Any]",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,,__future__ | ir_compiler,pure,no,5,
policies,L5,interpreter,ClauseResult.to_dict,"to_dict() -> dict[str, Any]",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,to_dict,__future__ | ir_compiler,pure,no,5,
policies,L5,interpreter,EvaluationError.__init__,"__init__(message: str, instruction: Instruction | None) -> None",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,__init__ | super,__future__ | ir_compiler,pure,no,7,
policies,L5,interpreter,EvaluationResult.has_block,has_block() -> bool,?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,any,__future__ | ir_compiler,pure,no,3,Check if any action is BLOCK.
policies,L5,interpreter,EvaluationResult.has_require_approval,has_require_approval() -> bool,?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,any,__future__ | ir_compiler,pure,no,3,Check if any action is REQUIRE_APPROVAL.
policies,L5,interpreter,EvaluationResult.to_dict,"to_dict() -> dict[str, Any]",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,to_dict,__future__ | ir_compiler,pure,no,6,
policies,L5,interpreter,EvaluationResult.warnings,warnings() -> list[str],?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,,__future__ | ir_compiler,pure,no,3,Get all warning messages.
policies,L5,interpreter,Interpreter.__init__,__init__() -> None,?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,,__future__ | ir_compiler,pure,no,3,Initialize interpreter (stateless).
policies,L5,interpreter,Interpreter._collect_actions,"_collect_actions(instructions: tuple[Instruction, ...]) -> list[ActionResult]",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,ActionResult | EvaluationError | append,__future__ | ir_compiler,pure,no,28,Collect action results from action IR.
policies,L5,interpreter,Interpreter._compare,"_compare(left: Any, comparator: str, right: Any, inst: Instruction) -> bool",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,EvaluationError | TypeMismatchError | _types_compatible | type,__future__ | ir_compiler,pure,no,33,Perform comparison.
policies,L5,interpreter,Interpreter._evaluate_clause,"_evaluate_clause(clause: CompiledClause, facts: dict[str, Any]) -> ClauseResult",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,ClauseResult | _collect_actions | _evaluate_condition | tuple,__future__ | ir_compiler,pure,no,16,Evaluate a single clause.
policies,L5,interpreter,Interpreter._evaluate_condition,"_evaluate_condition(instructions: tuple[Instruction, ...], facts: dict[str, Any]) -> bool",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,EvaluationError | _execute_instruction | isinstance | len | type,__future__ | ir_compiler,pure,no,29,Evaluate condition instructions using a stack.
policies,L5,interpreter,Interpreter._execute_instruction,"_execute_instruction(inst: Instruction, facts: dict[str, Any], stack: list[Any]) -> None",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,EvaluationError | MissingMetricError | TypeMismatchError | _compare | append | isinstance | len | pop | type,__future__ | ir_compiler,pure,no,77,Execute a single instruction.
policies,L5,interpreter,Interpreter._types_compatible,"_types_compatible(left: Any, right: Any) -> bool",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,isinstance | type,__future__ | ir_compiler,pure,no,19,Check if types are compatible for comparison.
policies,L5,interpreter,Interpreter.evaluate,"evaluate(ir: PolicyIR, facts: dict[str, Any]) -> EvaluationResult",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,EvaluationResult | _evaluate_clause | any | append | extend | tuple,__future__ | ir_compiler,pure,no,37,Evaluate policy IR against facts.
policies,L5,interpreter,_LenientInterpreter._compare,"_compare(left: Any, comparator: str, right: Any, inst: Instruction) -> bool",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,_compare | super,__future__ | ir_compiler,pure,no,11,Compare with sentinel handling.
policies,L5,interpreter,_LenientInterpreter._execute_instruction,"_execute_instruction(inst: Instruction, facts: dict[str, Any], stack: list[Any]) -> None",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,_execute_instruction | append | super,__future__ | ir_compiler,pure,no,15,Execute instruction with lenient missing metric handling.
policies,L5,interpreter,evaluate,"evaluate(ir: PolicyIR, facts: dict[str, Any]) -> EvaluationResult",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,Interpreter | evaluate,__future__ | ir_compiler,pure,no,41,Evaluate policy IR against facts.
policies,L5,interpreter,evaluate_policy,"evaluate_policy(ir: PolicyIR, facts: dict[str, Any], strict: bool) -> EvaluationResult",?:__init__ | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,_LenientInterpreter | evaluate,__future__ | ir_compiler,pure,no,27,Evaluate policy with optional strict mode.
policies,L5,ir_builder,IRBuilder.__init__,__init__(),?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,SymbolTable,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,7,
policies,L5,ir_builder,IRBuilder._emit,_emit(instr: IRInstruction) -> int,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,_next_id | add_instruction,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,6,Emit an instruction to current block.
policies,L5,ir_builder,IRBuilder._new_block,_new_block(name: str) -> IRBlock,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRBlock | add_block,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,6,Create a new basic block.
policies,L5,ir_builder,IRBuilder._next_block_name,_next_block_name(prefix: str) -> str,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,4,Get next block name.
policies,L5,ir_builder,IRBuilder._next_id,_next_id() -> int,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,4,Get next instruction ID.
policies,L5,ir_builder,IRBuilder.build,"build(ast: ProgramNode, module_name: str) -> IRModule",?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRModule | accept | append | isinstance,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,24,Build IR module from AST.
policies,L5,ir_builder,IRBuilder.visit_action_block,visit_action_block(node: ActionBlockNode) -> Any,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRAction | IREmitIntent | IRLoadConst | _emit | append,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,28,
policies,L5,ir_builder,IRBuilder.visit_attr_access,visit_attr_access(node: AttrAccessNode) -> int,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRCall | IRLoadConst | _emit | accept,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,9,
policies,L5,ir_builder,IRBuilder.visit_binary_op,visit_binary_op(node: BinaryOpNode) -> int,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRBinaryOp | IRCompare | _emit | accept,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,23,
policies,L5,ir_builder,IRBuilder.visit_condition_block,visit_condition_block(node: ConditionBlockNode) -> Any,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRJump | IRJumpIf | _emit | _new_block | _next_block_name | accept,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,31,
policies,L5,ir_builder,IRBuilder.visit_func_call,visit_func_call(node: FuncCallNode) -> int,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRCall | _emit | accept | isinstance,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,17,
policies,L5,ir_builder,IRBuilder.visit_ident,visit_ident(node: IdentNode) -> int,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRLoadVar | _emit | add_reference,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,4,
policies,L5,ir_builder,IRBuilder.visit_import,visit_import(node: ImportNode) -> Any,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,3,
policies,L5,ir_builder,IRBuilder.visit_literal,visit_literal(node: LiteralNode) -> int,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRLoadConst | _emit,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,2,
policies,L5,ir_builder,IRBuilder.visit_policy_decl,visit_policy_decl(node: PolicyDeclNode) -> Any,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRAction | IRFunction | Symbol | _emit | _new_block | accept | add_function | define | enter_scope | exit_scope | from_ast,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,48,
policies,L5,ir_builder,IRBuilder.visit_priority,visit_priority(node: PriorityNode) -> Any,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,4,
policies,L5,ir_builder,IRBuilder.visit_program,visit_program(node: ProgramNode) -> Any,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,accept,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,3,
policies,L5,ir_builder,IRBuilder.visit_route_target,visit_route_target(node: RouteTargetNode) -> Any,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,3,
policies,L5,ir_builder,IRBuilder.visit_rule_decl,visit_rule_decl(node: RuleDeclNode) -> Any,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRFunction | IRReturn | Symbol | _emit | _new_block | accept | add_function | define | enter_scope | exit_scope | from_ast,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,53,
policies,L5,ir_builder,IRBuilder.visit_rule_ref,visit_rule_ref(node: RuleRefNode) -> Any,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRCall | IRLoadVar | _emit | lookup_rule,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,8,
policies,L5,ir_builder,IRBuilder.visit_unary_op,visit_unary_op(node: UnaryOpNode) -> int,?:__init__ | ?:test_m20_optimizer | ?:test_m20_runtime | ?:test_m20_ir,IRUnaryOp | _emit | accept,grammar | ir_nodes | nodes | symbol_table | visitors,pure,no,9,
policies,L5,ir_compiler,CompiledClause.to_dict,"to_dict() -> dict[str, Any]",L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,to_dict,__future__ | ast,pure,no,6,Serialize to dict.
policies,L5,ir_compiler,IRCompiler.__init__,__init__(optimize: bool) -> None,L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,,__future__ | ast,pure,no,8,Initialize compiler.
policies,L5,ir_compiler,IRCompiler._compile_actions,"_compile_actions(actions: tuple[Action, ...]) -> list[Instruction]",L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,Instruction | append | is_block_action | is_require_approval_action | is_warn_action,__future__ | ast,pure,no,29,Compile actions to IR.
policies,L5,ir_compiler,IRCompiler._compile_clause,_compile_clause(clause: Clause) -> CompiledClause,L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,CompiledClause | _compile_actions | _compile_condition | tuple,__future__ | ast,pure,no,12,Compile a single when-then clause.
policies,L5,ir_compiler,IRCompiler._compile_condition,_compile_condition(condition: Condition) -> list[Instruction],L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,_emit_condition,__future__ | ast,pure,no,14,Compile a condition to IR.
policies,L5,ir_compiler,IRCompiler._emit_condition,"_emit_condition(condition: Condition, out: list[Instruction]) -> None",L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,_emit_exists | _emit_logical | _emit_predicate | is_exists_predicate | is_logical_condition | is_predicate,__future__ | ast,pure,no,8,Recursively emit condition instructions.
policies,L5,ir_compiler,IRCompiler._emit_exists,"_emit_exists(pred: ExistsPredicate, out: list[Instruction]) -> None",L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,Instruction | append,__future__ | ast,pure,no,12,Emit exists check.
policies,L5,ir_compiler,IRCompiler._emit_logical,"_emit_logical(cond: LogicalCondition, out: list[Instruction]) -> None",L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,Instruction | _emit_condition | append,__future__ | ast,pure,no,20,Emit logical condition.
policies,L5,ir_compiler,IRCompiler._emit_predicate,"_emit_predicate(pred: Predicate, out: list[Instruction]) -> None",L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,Instruction | append,__future__ | ast,pure,no,29,Emit predicate comparison.
policies,L5,ir_compiler,IRCompiler.compile,compile(ast: PolicyAST) -> PolicyIR,L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,PolicyIR | _compile_clause | append | tuple,__future__ | ast,pure,no,25,Compile AST to IR.
policies,L5,ir_compiler,Instruction.to_dict,"to_dict() -> dict[str, Any]",L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,list,__future__ | ast,pure,no,6,Serialize to dict for hashing/storage.
policies,L5,ir_compiler,OptimizingIRCompiler.__init__,__init__() -> None,L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,__init__ | super,__future__ | ast,pure,no,3,
policies,L5,ir_compiler,OptimizingIRCompiler.compile,compile(ast: PolicyAST) -> PolicyIR,L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,compile | super,__future__ | ast,pure,no,5,Compile with optimizations.
policies,L5,ir_compiler,PolicyIR.compute_hash,compute_hash() -> str,L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,encode | hexdigest | sha256 | to_json,__future__ | ast,pure,no,9,Compute deterministic SHA256 hash of IR.
policies,L5,ir_compiler,PolicyIR.instruction_count,instruction_count() -> int,L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,len,__future__ | ast,pure,no,7,Total number of instructions in IR.
policies,L5,ir_compiler,PolicyIR.to_dict,"to_dict() -> dict[str, Any]",L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,to_dict,__future__ | ast,pure,no,10,Serialize to dict for storage/hashing.
policies,L5,ir_compiler,PolicyIR.to_json,to_json(indent: int | None) -> str,L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,dumps | to_dict,__future__ | ast,pure,no,3,Serialize to JSON.
policies,L5,ir_compiler,compile_policy,"compile_policy(ast: PolicyAST, optimize: bool) -> PolicyIR",L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,IRCompiler | OptimizingIRCompiler | compile,__future__ | ast,pure,no,33,Compile PolicyAST to PolicyIR.
policies,L5,ir_compiler,ir_hash,ir_hash(ast: PolicyAST) -> str,L5:interpreter | ?:__init__ | ?:interpreter | ?:test_ir_compiler | ?:test_roundtrip | ?:test_interpreter | ?:test_replay,compile_policy | compute_hash,__future__ | ast,pure,no,8,Convenience function to get IR hash from AST.
policies,L5,ir_nodes,IRAction.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,append | join,grammar,pure,no,7,
policies,L5,ir_nodes,IRBinaryOp.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,ir_nodes,IRBlock.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,append | join,grammar,pure,no,5,
policies,L5,ir_nodes,IRBlock.add_instruction,add_instruction(instr: IRInstruction) -> None,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,append,grammar,pure,no,2,
policies,L5,ir_nodes,IRBlock.is_terminated,is_terminated() -> bool,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,isinstance,grammar,pure,no,6,Check if block ends with terminator instruction.
policies,L5,ir_nodes,IRCall.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,join,grammar,pure,no,3,
policies,L5,ir_nodes,IRCheckPolicy.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,3,
policies,L5,ir_nodes,IRCompare.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,ir_nodes,IREmitIntent.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,append | join,grammar,pure,no,8,
policies,L5,ir_nodes,IRFunction.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,append | join | str | values,grammar,pure,no,9,
policies,L5,ir_nodes,IRFunction.add_block,add_block(block: IRBlock) -> None,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,ir_nodes,IRFunction.get_block,get_block(name: str) -> Optional[IRBlock],?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,get,grammar,pure,no,2,
policies,L5,ir_nodes,IRGovernance.from_ast,from_ast(governance: Any) -> 'IRGovernance',?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,cls,grammar,pure,no,10,Create from AST governance metadata.
policies,L5,ir_nodes,IRGovernance.to_dict,"to_dict() -> Dict[str, Any]",?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,10,Convert to dictionary for serialization.
policies,L5,ir_nodes,IRJump.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,ir_nodes,IRJumpIf.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,ir_nodes,IRLoadConst.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,ir_nodes,IRLoadVar.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,ir_nodes,IRModule.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,append | join | str | values,grammar,pure,no,9,
policies,L5,ir_nodes,IRModule.add_function,add_function(func: IRFunction) -> None,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,append,grammar,pure,no,8,
policies,L5,ir_nodes,IRModule.get_function,get_function(name: str) -> Optional[IRFunction],?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,get,grammar,pure,no,2,
policies,L5,ir_nodes,IRModule.get_functions_by_category,get_functions_by_category(category: PolicyCategory) -> List[IRFunction],?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,get | sorted,grammar,pure,no,5,"Get all functions in a category, sorted by priority."
policies,L5,ir_nodes,IRNode.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,ir_nodes,IRReturn.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,4,
policies,L5,ir_nodes,IRStoreVar.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,ir_nodes,IRUnaryOp.__str__,__str__() -> str,?:conflict_resolver | ?:folds | ?:dag_sorter | ?:ir_builder | ?:__init__ | ?:dag_executor | ?:deterministic_engine | L4:dag_executor | L4:dag_sorter | L6:optimizer_conflict_resolver,,grammar,pure,no,2,
policies,L5,kernel,ExecutionKernel._emit_envelope,"_emit_envelope(capability_id: str, execution_vector: str, context: InvocationContext, reason: Optional[str]) -> str",?:__init__ | ?:decorator | ?:recovery_claim_worker | ?:main | L5:decorator | ?:aos,create_admin_envelope | emit_envelope | error | info | startswith | str | uuid4,__future__ | execution_envelope,pure,no,82,Emit execution envelope for attribution.
policies,L5,kernel,ExecutionKernel._record_invocation_complete,"_record_invocation_complete(capability_id: str, context: InvocationContext, success: bool, duration_ms: float, error: Optional[str]) -> None",?:__init__ | ?:decorator | ?:recovery_claim_worker | ?:main | L5:decorator | ?:aos,info,__future__ | execution_envelope,pure,no,19,Record invocation completion for metrics and audit.
policies,L5,kernel,ExecutionKernel._record_invocation_start,"_record_invocation_start(capability_id: str, execution_vector: str, context: InvocationContext, enforcement_mode: EnforcementMode) -> None",?:__init__ | ?:decorator | ?:recovery_claim_worker | ?:main | L5:decorator | ?:aos,info,__future__ | execution_envelope,pure,no,19,Record invocation start for metrics and audit.
policies,L5,kernel,ExecutionKernel.get_known_capabilities,get_known_capabilities() -> set[str],?:__init__ | ?:decorator | ?:recovery_claim_worker | ?:main | L5:decorator | ?:aos,copy,__future__ | execution_envelope,pure,no,3,Get the set of known capability IDs.
policies,L5,kernel,ExecutionKernel.invoke,"invoke(capability_id: str, execution_vector: str, context: InvocationContext, work: Callable[[], T], reason: Optional[str]) -> ExecutionResult",?:__init__ | ?:decorator | ?:recovery_claim_worker | ?:main | L5:decorator | ?:aos,ExecutionResult | _emit_envelope | _record_invocation_complete | _record_invocation_start | get_enforcement_mode | isoformat | now | str | total_seconds | warning | work,__future__ | execution_envelope,pure,no,124,Execute work through the governance kernel.
policies,L5,kernel,ExecutionKernel.invoke_async,"async invoke_async(capability_id: str, execution_vector: str, context: InvocationContext, work: Callable[[], T], reason: Optional[str]) -> ExecutionResult",?:__init__ | ?:decorator | ?:recovery_claim_worker | ?:main | L5:decorator | ?:aos,ExecutionResult | _emit_envelope | _record_invocation_complete | _record_invocation_start | get_enforcement_mode | hasattr | isoformat | now | str | total_seconds | warning | work,__future__ | execution_envelope,pure,yes,108,Async version of invoke for async execution paths.
policies,L5,kernel,ExecutionKernel.is_known_capability,is_known_capability(capability_id: str) -> bool,?:__init__ | ?:decorator | ?:recovery_claim_worker | ?:main | L5:decorator | ?:aos,,__future__ | execution_envelope,pure,no,3,Check if a capability ID is known to the kernel.
policies,L5,kernel,get_enforcement_mode,get_enforcement_mode(capability_id: str) -> EnforcementMode,?:__init__ | ?:decorator | ?:recovery_claim_worker | ?:main | L5:decorator | ?:aos,get,__future__ | execution_envelope,pure,no,11,Get enforcement mode for a capability.
policies,L5,kernel,set_enforcement_mode,"set_enforcement_mode(capability_id: str, mode: EnforcementMode) -> None",?:__init__ | ?:decorator | ?:recovery_claim_worker | ?:main | L5:decorator | ?:aos,info,__future__ | execution_envelope,pure,no,19,Set enforcement mode for a capability.
policies,L5,kill_switch,KillSwitchStatus.get_current,get_current() -> 'KillSwitchStatus',?:test_kill_switch,cls,,pure,no,7,Get current kill switch status.
policies,L5,kill_switch,activate_kill_switch,"activate_kill_switch(reason: str, activated_by: str, auto_expire_minutes: int) -> KillSwitchActivation",?:test_kill_switch,KillSwitchActivation | KillSwitchStatus | isoformat | now | timedelta | warning,,pure,no,55,Activate the runtime kill switch.
policies,L5,kill_switch,deactivate_kill_switch,deactivate_kill_switch(deactivated_by: str) -> KillSwitchDeactivation,?:test_kill_switch,KillSwitchDeactivation | KillSwitchStatus | info | isoformat | now,,pure,no,37,Deactivate the runtime kill switch.
policies,L5,kill_switch,is_kill_switch_active,is_kill_switch_active() -> bool,?:test_kill_switch,KillSwitchStatus | fromisoformat | info | now,,pure,no,26,Check if kill switch is currently active.
policies,L5,kill_switch,should_bypass_governance,should_bypass_governance() -> bool,?:test_kill_switch,is_kill_switch_active,,pure,no,10,Check if governance should be bypassed.
policies,L5,learning_proof_engine,AdaptiveConfidenceSystem.get_confidence_report,get_confidence_report() -> dict,,abs | len | values,__future__,pure,no,18,Report on confidence calibration health.
policies,L5,learning_proof_engine,AdaptiveConfidenceSystem.get_or_create_calibration,get_or_create_calibration(pattern_id: str) -> PatternCalibration,,PatternCalibration,__future__,pure,no,4,
policies,L5,learning_proof_engine,AdaptiveConfidenceSystem.get_threshold_for_pattern,"get_threshold_for_pattern(pattern_id: str) -> tuple[float, float]",,,__future__,pure,no,8,Get calibrated strong/weak thresholds for a pattern.
policies,L5,learning_proof_engine,AdaptiveConfidenceSystem.record_outcome,"record_outcome(pattern_id: str, confidence: float, was_correct: bool) -> None",,get_or_create_calibration | record_outcome,__future__,pure,no,10,Record prediction outcome for calibration.
policies,L5,learning_proof_engine,CheckpointConfig.get_priority,"get_priority(checkpoint_type: str, confidence: float) -> CheckpointPriority",,get,__future__,pure,no,19,"Get priority for a checkpoint, considering confidence."
policies,L5,learning_proof_engine,CheckpointConfig.is_blocking,is_blocking(checkpoint_type: str) -> bool,,,__future__,pure,no,3,Check if checkpoint type blocks loop progress.
policies,L5,learning_proof_engine,CheckpointConfig.should_auto_dismiss,"should_auto_dismiss(checkpoint_type: str, age_hours: float) -> bool",,get_priority,__future__,pure,no,8,Check if checkpoint should be auto-dismissed.
policies,L5,learning_proof_engine,GlobalRegretTracker.get_or_create_tracker,get_or_create_tracker(policy_id: str) -> PolicyRegretTracker,,PolicyRegretTracker,__future__,pure,no,4,
policies,L5,learning_proof_engine,GlobalRegretTracker.has_proven_rollback,has_proven_rollback() -> bool,,,__future__,pure,no,3,Gate 2 check: Has at least one policy been auto-demoted?
policies,L5,learning_proof_engine,GlobalRegretTracker.record_regret,"record_regret(policy_id: str, event: RegretEvent) -> bool",,add_regret | get_or_create_tracker,__future__,pure,no,8,Record regret. Returns True if demotion triggered.
policies,L5,learning_proof_engine,GlobalRegretTracker.system_regret_rate,system_regret_rate() -> float,,len | sum | values,__future__,pure,no,6,Overall regret rate across all policies.
policies,L5,learning_proof_engine,M25GraduationStatus._get_next_action,_get_next_action() -> str,,,__future__,pure,no,13,What to do next to graduate.
policies,L5,learning_proof_engine,M25GraduationStatus.gate1_passed,gate1_passed() -> bool,,,__future__,pure,no,3,Gate 1: At least one prevention recorded.
policies,L5,learning_proof_engine,M25GraduationStatus.gate2_passed,gate2_passed() -> bool,,,__future__,pure,no,3,Gate 2: At least one regret-driven rollback.
policies,L5,learning_proof_engine,M25GraduationStatus.gate3_passed,gate3_passed() -> bool,,len,__future__,pure,no,3,Gate 3: At least one incident shows timeline with prevention.
policies,L5,learning_proof_engine,M25GraduationStatus.is_graduated,is_graduated() -> bool,,,__future__,pure,no,3,All gates passed - M25 is proven.
policies,L5,learning_proof_engine,M25GraduationStatus.status_label,status_label() -> str,,sum,__future__,pure,no,7,Human-readable status.
policies,L5,learning_proof_engine,M25GraduationStatus.to_dashboard,to_dashboard() -> dict,,_get_next_action | len,__future__,pure,no,35,Dashboard display of graduation status.
policies,L5,learning_proof_engine,PatternCalibration._recalibrate,_recalibrate() -> None,,len | sorted | sum,__future__,pure,no,24,Recalibrate thresholds based on outcomes.
policies,L5,learning_proof_engine,PatternCalibration.accuracy,accuracy() -> float,,,__future__,pure,no,5,Overall accuracy for this pattern.
policies,L5,learning_proof_engine,PatternCalibration.get_calibrated_band,get_calibrated_band(confidence: float) -> str,,,__future__,pure,no,8,Get band using calibrated thresholds.
policies,L5,learning_proof_engine,PatternCalibration.is_calibrated,is_calibrated() -> bool,,len,__future__,pure,no,3,Has enough data for reliable calibration?
policies,L5,learning_proof_engine,PatternCalibration.record_outcome,"record_outcome(predicted_confidence: float, was_correct: bool) -> None",,_recalibrate | append | len,__future__,pure,no,17,Record a prediction outcome for calibration.
policies,L5,learning_proof_engine,PolicyRegretTracker._trigger_demotion,_trigger_demotion(reason: str) -> None,,now,__future__,pure,no,4,Auto-demote policy due to excessive regret.
policies,L5,learning_proof_engine,PolicyRegretTracker.add_regret,add_regret(event: RegretEvent) -> bool,,_trigger_demotion | append | len,__future__,pure,no,19,Add regret event. Returns True if auto-demotion triggered.
policies,L5,learning_proof_engine,PolicyRegretTracker.decay_regret,decay_regret() -> None,,max,__future__,pure,no,3,Apply daily decay to regret score.
policies,L5,learning_proof_engine,PolicyRegretTracker.is_demoted,is_demoted() -> bool,,,__future__,pure,no,2,
policies,L5,learning_proof_engine,PolicyRegretTracker.to_rollback_timeline,to_rollback_timeline() -> dict | None,,isoformat | len,__future__,pure,no,16,Format for console timeline if demoted.
policies,L5,learning_proof_engine,PreventionRecord.create_prevention,"create_prevention(policy_id: str, pattern_id: str, original_incident_id: str, blocked_incident_id: str, tenant_id: str, signature_match: float, policy_age: timedelta, calls_evaluated: int) -> 'PreventionRecord'",,cls | now | uuid4,__future__,pure,no,25,Create a prevention record - this is evidence of learning.
policies,L5,learning_proof_engine,PreventionRecord.to_console_timeline,to_console_timeline() -> dict,,isoformat | str,__future__,pure,no,16,Format for console timeline visualization.
policies,L5,learning_proof_engine,PreventionTimeline._generate_narrative,"_generate_narrative(has_prevention: bool, has_rollback: bool) -> str",,,__future__,pure,no,14,Generate human-readable narrative.
policies,L5,learning_proof_engine,PreventionTimeline.add_incident_created,"add_incident_created(timestamp: datetime, details: dict) -> None",,append | get | isoformat,__future__,pure,no,12,Original incident that created the pattern/policy.
policies,L5,learning_proof_engine,PreventionTimeline.add_policy_born,"add_policy_born(timestamp: datetime, policy_id: str, policy_name: str) -> None",,append | isoformat,__future__,pure,no,12,Policy was created from this incident.
policies,L5,learning_proof_engine,PreventionTimeline.add_prevention,"add_prevention(timestamp: datetime, record: PreventionRecord) -> None",,append | isoformat,__future__,pure,no,17,The prevention event - this is the proof.
policies,L5,learning_proof_engine,PreventionTimeline.add_regret,"add_regret(timestamp: datetime, event: RegretEvent) -> None",,append | isoformat,__future__,pure,no,15,Regret event if policy caused harm.
policies,L5,learning_proof_engine,PreventionTimeline.add_rollback,"add_rollback(timestamp: datetime, tracker: PolicyRegretTracker) -> None",,append | isoformat,__future__,pure,no,16,Auto-rollback event.
policies,L5,learning_proof_engine,PreventionTimeline.to_console,to_console() -> dict,,_generate_narrative | any | len | sorted,__future__,pure,no,21,Format for console display.
policies,L5,learning_proof_engine,PreventionTracker.get_top_preventing_patterns,"get_top_preventing_patterns(n: int) -> list[tuple[str, int]]",,items | len | sorted,__future__,pure,no,4,Patterns that have been most effectively prevented.
policies,L5,learning_proof_engine,PreventionTracker.has_proven_prevention,has_proven_prevention() -> bool,,,__future__,pure,no,3,Gate 1 check: Has at least one policy prevented one incident?
policies,L5,learning_proof_engine,PreventionTracker.prevention_rate,prevention_rate() -> float,,,__future__,pure,no,6,Overall prevention success rate.
policies,L5,learning_proof_engine,PreventionTracker.record_failure,"record_failure(policy_id: str, pattern_id: str) -> None",,,__future__,pure,no,3,Record a prevention failure (policy didn't stop recurrence).
policies,L5,learning_proof_engine,PreventionTracker.record_prevention,record_prevention(record: PreventionRecord) -> None,,add | append,__future__,db_write,no,5,Record a successful prevention.
policies,L5,learning_proof_engine,PrioritizedCheckpoint.check_auto_dismiss,check_auto_dismiss() -> bool,,now,__future__,pure,no,10,Check and apply auto-dismiss if expired.
policies,L5,learning_proof_engine,PrioritizedCheckpoint.create,"create(checkpoint_type: str, incident_id: str, tenant_id: str, description: str, confidence: float, config: CheckpointConfig) -> 'PrioritizedCheckpoint'",,cls | get_priority | is_blocking | now | timedelta | uuid4,__future__,pure,no,30,Create checkpoint with priority from config.
policies,L5,lessons_engine,LessonsLearnedEngine.__init__,"__init__(db_url: Optional[str], driver: Any)",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,get,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,12,Initialize the lessons learned engine.
policies,L5,lessons_engine,LessonsLearnedEngine._create_lesson,"_create_lesson(tenant_id: str, lesson_type: str, severity: Optional[str], source_event_id: UUID, source_event_type: str, source_run_id: Optional[UUID], title: str, description: str, proposed_action: Optional[str], detected_pattern: Optional[Dict[str, Any]], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[UUID]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_get_driver | error | info | insert_lesson | str | utc_now | uuid4,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,51,Create a lesson record in the database.
policies,L5,lessons_engine,LessonsLearnedEngine._generate_failure_description,"_generate_failure_description(error_code: Optional[str], error_message: Optional[str], severity: str) -> str",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,lower,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,14,Generate description for a failure lesson.
policies,L5,lessons_engine,LessonsLearnedEngine._generate_failure_proposed_action,"_generate_failure_proposed_action(error_code: Optional[str], severity: str) -> str",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,17,Generate proposed action for a failure lesson.
policies,L5,lessons_engine,LessonsLearnedEngine._get_driver,_get_driver() -> Any,?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,RuntimeError | SessionLocal | create_engine | get_lessons_driver | sessionmaker,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,28,Get or create the lessons driver.
policies,L5,lessons_engine,LessonsLearnedEngine._is_debounced,"_is_debounced(tenant_id: str, metric_type: str, lesson_type: str, threshold_band: Optional[str]) -> bool",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_get_driver | fetch_debounce_count | warning,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,44,Check if a lesson of this type has been created recently (debounce).
policies,L5,lessons_engine,LessonsLearnedEngine.convert_lesson_to_draft,"convert_lesson_to_draft(lesson_id: UUID, tenant_id: str, converted_by: str) -> Optional[UUID]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_get_driver | error | get_lesson | info | insert_policy_proposal_from_lesson | is_valid_transition | str | update_lesson_converted | utc_now | uuid4 | warning,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,70,Convert a lesson to a draft policy proposal.
policies,L5,lessons_engine,LessonsLearnedEngine.defer_lesson,"defer_lesson(lesson_id: UUID, tenant_id: str, defer_until: datetime) -> bool",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_get_driver | error | get_lesson | info | is_valid_transition | str | update_lesson_deferred | warning,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,57,Defer a lesson until a future date.
policies,L5,lessons_engine,LessonsLearnedEngine.detect_lesson_from_critical_success,"detect_lesson_from_critical_success(run_id: UUID, tenant_id: str, success_type: str, metrics: dict[str, Any], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[UUID]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_create_lesson,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,55,Detect and create a lesson from a critical success event.
policies,L5,lessons_engine,LessonsLearnedEngine.detect_lesson_from_failure,"detect_lesson_from_failure(run_id: UUID, tenant_id: str, error_code: Optional[str], error_message: Optional[str], severity: str, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[UUID]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_create_lesson | _generate_failure_description | _generate_failure_proposed_action,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,55,Detect and create a lesson from a failure event.
policies,L5,lessons_engine,LessonsLearnedEngine.detect_lesson_from_near_threshold,"detect_lesson_from_near_threshold(run_id: UUID, tenant_id: str, threshold_type: str, current_value: float, threshold_value: float, utilization_percent: float, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[UUID]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_create_lesson | _is_debounced | debug | get_threshold_band,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,84,Detect and create a lesson from a near-threshold event.
policies,L5,lessons_engine,LessonsLearnedEngine.dismiss_lesson,"dismiss_lesson(lesson_id: UUID, tenant_id: str, dismissed_by: str, reason: str) -> bool",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_get_driver | error | get_lesson | info | is_valid_transition | str | update_lesson_dismissed | utc_now | warning,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,63,Dismiss a lesson (mark as not actionable).
policies,L5,lessons_engine,LessonsLearnedEngine.emit_critical_success,"emit_critical_success(tenant_id: str, success_type: str, metrics: dict[str, Any], source_event_id: UUID, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[UUID]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,detect_lesson_from_critical_success | error | inc | labels,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,42,Worker-safe method to emit a critical success lesson.
policies,L5,lessons_engine,LessonsLearnedEngine.emit_near_threshold,"emit_near_threshold(tenant_id: str, metric: str, utilization: float, threshold_value: float, current_value: float, source_event_id: UUID, window: str, is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> Optional[UUID]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,detect_lesson_from_near_threshold | error | inc | labels,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,50,Worker-safe method to emit a near-threshold lesson.
policies,L5,lessons_engine,LessonsLearnedEngine.get_expired_deferred_lessons,"get_expired_deferred_lessons(limit: int) -> List[tuple[UUID, str]]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,UUID | _get_driver | error | fetch_expired_deferred | str,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,26,Get deferred lessons whose deferred_until has passed.
policies,L5,lessons_engine,LessonsLearnedEngine.get_lesson,"get_lesson(lesson_id: UUID, tenant_id: str) -> Optional[Dict[str, Any]]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_get_driver | fetch_lesson_by_id | str,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,18,Get a specific lesson by ID.
policies,L5,lessons_engine,LessonsLearnedEngine.get_lesson_stats,"get_lesson_stats(tenant_id: str) -> Dict[str, Any]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_get_driver | fetch_lesson_stats,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,36,Get lesson statistics for a tenant.
policies,L5,lessons_engine,LessonsLearnedEngine.list_lessons,"list_lessons(tenant_id: str, lesson_type: Optional[str], status: Optional[str], severity: Optional[str], include_synthetic: bool, limit: int, offset: int) -> List[Dict[str, Any]]",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_get_driver | fetch_lessons_list,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,37,List lessons for a tenant with optional filters.
policies,L5,lessons_engine,LessonsLearnedEngine.reactivate_deferred_lesson,"reactivate_deferred_lesson(lesson_id: UUID, tenant_id: str) -> bool",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,_get_driver | error | get_lesson | info | is_valid_transition | str | update_lesson_reactivated | warning,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,58,Reactivate a deferred lesson back to pending status.
policies,L5,lessons_engine,LessonsLearnedEngine.reactivate_expired_deferred_lessons,reactivate_expired_deferred_lessons() -> int,?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,get_expired_deferred_lessons | info | len | reactivate_deferred_lesson,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,27,Reactivate all deferred lessons whose deferred_until has passed.
policies,L5,lessons_engine,get_lessons_learned_engine,get_lessons_learned_engine(driver: Any) -> LessonsLearnedEngine,?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,LessonsLearnedEngine,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,13,Get a LessonsLearnedEngine instance.
policies,L5,lessons_engine,get_threshold_band,get_threshold_band(utilization: float) -> str,?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,7,Get the threshold band for a utilization percentage.
policies,L5,lessons_engine,is_valid_transition,"is_valid_transition(from_status: str, to_status: str) -> bool",?:policy_layer | ?:policy | ?:policies_facade | ?:lessons_engine | ?:run_governance_facade | ?:incident_engine | ?:main | L4:policies_handler | L4:lessons_coordinator,get | set,lessons_driver | orm | prometheus_client | sqlalchemy | sqlmodel | time,pure,no,4,Check if a state transition is valid.
policies,L5,limits,Limits.is_unlimited,is_unlimited() -> bool,?:rate_limits | ?:policy_rules_crud | ?:policies | ?:aos_cus_integrations | ?:billing_gate | ?:billing_dependencies | ?:simulate | ?:__init__ | ?:override | ?:policy_limits_crud,all,,pure,no,13,Check if all limits are unlimited (None).
policies,L5,limits,derive_limits,derive_limits(limits_profile: str) -> Limits,?:rate_limits | ?:policy_rules_crud | ?:policies | ?:aos_cus_integrations | ?:billing_gate | ?:billing_dependencies | ?:simulate | ?:__init__ | ?:override | ?:policy_limits_crud,get,,pure,no,13,Derive limits from a limits profile key.
policies,L5,limits_facade,LimitCheckResult.to_dict,"to_dict() -> Dict[str, Any]",L4:policies_handler,,,pure,no,11,Convert to dictionary.
policies,L5,limits_facade,LimitConfig.to_dict,"to_dict() -> Dict[str, Any]",L4:policies_handler,max | round,,pure,no,19,Convert to dictionary.
policies,L5,limits_facade,LimitsFacade.__init__,__init__(),L4:policies_handler,,,pure,no,3,Initialize facade.
policies,L5,limits_facade,LimitsFacade._get_or_create_limit,"_get_or_create_limit(tenant_id: str, limit_type: str, period: str, max_value: int) -> LimitConfig",L4:policies_handler,LimitConfig | isoformat | now | str | timedelta | uuid4,,pure,no,35,Get or create a limit configuration.
policies,L5,limits_facade,LimitsFacade.check_limit,"async check_limit(tenant_id: str, limit_type: str, increment: int) -> LimitCheckResult",L4:policies_handler,LimitCheckResult | _get_or_create_limit | fromisoformat | isoformat | max | now | replace | timedelta,,pure,yes,54,Check if a limit allows the operation.
policies,L5,limits_facade,LimitsFacade.get_limit,"async get_limit(limit_id: str, tenant_id: str) -> Optional[LimitConfig]",L4:policies_handler,values,,pure,yes,19,Get a specific limit.
policies,L5,limits_facade,LimitsFacade.get_usage,async get_usage(tenant_id: str) -> UsageSummary,L4:policies_handler,UsageSummary | _get_or_create_limit | append | isoformat | now | to_dict | values,,pure,yes,39,Get current usage summary.
policies,L5,limits_facade,LimitsFacade.list_limits,"async list_limits(tenant_id: str, limit_type: Optional[str], limit: int, offset: int) -> List[LimitConfig]",L4:policies_handler,_get_or_create_limit | append | sort | values,,pure,yes,33,List limits for a tenant.
policies,L5,limits_facade,LimitsFacade.reset_limit,"async reset_limit(tenant_id: str, limit_type: str) -> Optional[LimitConfig]",L4:policies_handler,get | info | isoformat | now,,pure,yes,30,Reset a limit's current value.
policies,L5,limits_facade,LimitsFacade.update_limit,"async update_limit(limit_id: str, tenant_id: str, max_value: Optional[int], enabled: Optional[bool], metadata: Optional[Dict[str, Any]]) -> Optional[LimitConfig]",L4:policies_handler,isoformat | now | update | values,,pure,yes,41,Update a limit.
policies,L5,limits_facade,UsageSummary.to_dict,"to_dict() -> Dict[str, Any]",L4:policies_handler,,,pure,no,9,Convert to dictionary.
policies,L5,limits_facade,get_limits_facade,get_limits_facade() -> LimitsFacade,L4:policies_handler,LimitsFacade,,pure,no,14,Get the limits facade instance.
policies,L5,limits_simulation_engine,LimitsSimulationEngine.__init__,__init__(session: Any) -> None,L4:policies_handler,,asyncio,pure,no,2,
policies,L5,limits_simulation_engine,LimitsSimulationEngine.simulate,"async simulate(**kwargs) -> dict[str, Any]",L4:policies_handler,NotImplementedError,asyncio,pure,yes,3,Simulate limit check  stub.
policies,L5,limits_simulation_engine,get_limits_simulation_engine,get_limits_simulation_engine(session: 'AsyncSession') -> LimitsSimulationEngine,L4:policies_handler,LimitsSimulationEngine,asyncio,pure,no,3,Get the LimitsSimulationEngine instance.
policies,L5,limits_simulation_engine,get_limits_simulation_service,get_limits_simulation_service(session: 'AsyncSession') -> LimitsSimulationService,L4:policies_handler,get_limits_simulation_engine,asyncio,pure,no,3,Get the LimitsSimulationService instance (backward alias).
policies,L5,llm_policy_engine,LLMRateLimiter.__init__,__init__(requests_per_minute: int),L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,Lock | deque,,pure,no,5,
policies,L5,llm_policy_engine,LLMRateLimiter.check_and_record,check_and_record() -> bool,L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,append | len | popleft | time,,pure,no,21,Check if request is allowed and record it.
policies,L5,llm_policy_engine,LLMRateLimiter.get_instance,get_instance(provider: str) -> 'LLMRateLimiter',L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,cls,,pure,no,6,Get or create rate limiter for a provider.
policies,L5,llm_policy_engine,LLMRateLimiter.requests_remaining,requests_remaining() -> int,L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,len | max | popleft | time,,pure,no,11,Get number of requests remaining in current window.
policies,L5,llm_policy_engine,check_safety_limits,"check_safety_limits(model: str, max_tokens: int, estimated_input_tokens: int, provider: str, max_tokens_limit: Optional[int], max_cost_cents_limit: Optional[float]) -> SafetyCheckResult",L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,SafetyCheckResult | check_and_record | estimate_cost_cents | get_instance | requests_remaining,,pure,no,86,Check safety limits before making LLM API call (L4 domain function).
policies,L5,llm_policy_engine,estimate_cost_cents,"estimate_cost_cents(model: str, input_tokens: int, output_tokens: int) -> float",L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,get,,pure,no,16,Estimate cost in cents (L4 domain function).
policies,L5,llm_policy_engine,estimate_tokens,estimate_tokens(text: str) -> int,L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,len,,pure,no,7,Estimate token count for text (L4 domain function).
policies,L5,llm_policy_engine,get_effective_model,"get_effective_model(requested_model: Optional[str], preferred_model: str, fallback_model: str, allowed_models: List[str]) -> str",L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,,,pure,no,30,Get effective model based on request and tenant config (L4 policy decision).
policies,L5,llm_policy_engine,get_model_for_task,"get_model_for_task(task_type: str, requested_model: Optional[str], tenant_allowed_models: Optional[List[str]], allow_expensive: bool) -> str",L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,get | is_expensive_model | is_model_allowed | warning,,pure,no,41,Get appropriate model for a task type (L4 policy decision).
policies,L5,llm_policy_engine,is_expensive_model,is_expensive_model(model: str) -> bool,L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,,,pure,no,3,Check if a model is classified as expensive (L4 domain function).
policies,L5,llm_policy_engine,is_model_allowed,"is_model_allowed(model: str, tenant_allowed_models: Optional[List[str]]) -> bool",L3:openai_adapter | L3:tenant_config | ?:tenant_config | ?:openai_adapter,,,pure,no,21,Check if a model is allowed (L4 domain function).
policies,L5,nodes,ASTNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Accept a visitor for traversal.
policies,L5,nodes,ASTNode.location,location() -> str,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Get source location string.
policies,L5,nodes,ASTVisitor.visit_action_block,visit_action_block(node: 'ActionBlockNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit action block.
policies,L5,nodes,ASTVisitor.visit_attr_access,visit_attr_access(node: 'AttrAccessNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit attribute access.
policies,L5,nodes,ASTVisitor.visit_binary_op,visit_binary_op(node: 'BinaryOpNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit binary operation.
policies,L5,nodes,ASTVisitor.visit_condition_block,visit_condition_block(node: 'ConditionBlockNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit condition block.
policies,L5,nodes,ASTVisitor.visit_func_call,visit_func_call(node: 'FuncCallNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit function call.
policies,L5,nodes,ASTVisitor.visit_ident,visit_ident(node: 'IdentNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit identifier.
policies,L5,nodes,ASTVisitor.visit_import,visit_import(node: 'ImportNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit import statement.
policies,L5,nodes,ASTVisitor.visit_literal,visit_literal(node: 'LiteralNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit literal value.
policies,L5,nodes,ASTVisitor.visit_policy_decl,visit_policy_decl(node: 'PolicyDeclNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit policy declaration.
policies,L5,nodes,ASTVisitor.visit_priority,visit_priority(node: 'PriorityNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit priority node.
policies,L5,nodes,ASTVisitor.visit_program,visit_program(node: 'ProgramNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit program node.
policies,L5,nodes,ASTVisitor.visit_route_target,visit_route_target(node: 'RouteTargetNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit route target.
policies,L5,nodes,ASTVisitor.visit_rule_decl,visit_rule_decl(node: 'RuleDeclNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit rule declaration.
policies,L5,nodes,ASTVisitor.visit_rule_ref,visit_rule_ref(node: 'RuleRefNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit rule reference.
policies,L5,nodes,ASTVisitor.visit_unary_op,visit_unary_op(node: 'UnaryOpNode') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,,grammar,pure,no,3,Visit unary operation.
policies,L5,nodes,ActionBlockNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_action_block,grammar,pure,no,2,
policies,L5,nodes,AttrAccessNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_attr_access,grammar,pure,no,2,
policies,L5,nodes,BinaryOpNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_binary_op,grammar,pure,no,2,
policies,L5,nodes,ConditionBlockNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_condition_block,grammar,pure,no,2,
policies,L5,nodes,FuncCallNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_func_call,grammar,pure,no,2,
policies,L5,nodes,GovernanceMetadata.merge_with,merge_with(other: 'GovernanceMetadata') -> 'GovernanceMetadata',?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,GovernanceMetadata | strip,grammar,pure,no,14,"Merge governance metadata, taking higher priority."
policies,L5,nodes,IdentNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_ident,grammar,pure,no,2,
policies,L5,nodes,ImportNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_import,grammar,pure,no,2,
policies,L5,nodes,LiteralNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_literal,grammar,pure,no,2,
policies,L5,nodes,PolicyDeclNode.__post_init__,__post_init__(),?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,GovernanceMetadata | get_category_priority,grammar,pure,no,9,
policies,L5,nodes,PolicyDeclNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_policy_decl,grammar,pure,no,2,
policies,L5,nodes,PriorityNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_priority,grammar,pure,no,2,
policies,L5,nodes,ProgramNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_program,grammar,pure,no,2,
policies,L5,nodes,RouteTargetNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_route_target,grammar,pure,no,2,
policies,L5,nodes,RuleDeclNode.__post_init__,__post_init__(),?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,GovernanceMetadata | get_category_priority,grammar,pure,no,8,
policies,L5,nodes,RuleDeclNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_rule_decl,grammar,pure,no,2,
policies,L5,nodes,RuleRefNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_rule_ref,grammar,pure,no,2,
policies,L5,nodes,UnaryOpNode.accept,accept(visitor: 'ASTVisitor') -> Any,?:parser | ?:dag_sorter | ?:ir_builder | ?:visitors | ?:__init__ | ?:knowledge_plane | ?:policy_graph_engine | L4:dag_sorter | L4:knowledge_plane | L5:visitors,visit_unary_op,grammar,pure,no,2,
policies,L5,phase_status_invariants,InvariantCheckResponse.to_dict,"to_dict() -> dict[str, Any]",?:__init__,list,,pure,no,11,Convert to dictionary for API responses.
policies,L5,phase_status_invariants,PhaseStatusInvariantChecker.__init__,__init__(enforcement_enabled: bool),?:__init__,,,pure,no,8,Initialize the invariant checker.
policies,L5,phase_status_invariants,PhaseStatusInvariantChecker.check,"check(phase: str, status: str) -> InvariantCheckResponse",?:__init__,InvariantCheckResponse | frozenset | get_allowed_statuses | sorted | upper,,pure,no,62,Check if a phase-status combination is valid.
policies,L5,phase_status_invariants,PhaseStatusInvariantChecker.enforcement_enabled,enforcement_enabled() -> bool,?:__init__,,,pure,no,3,Check if enforcement is enabled.
policies,L5,phase_status_invariants,PhaseStatusInvariantChecker.ensure_valid,"ensure_valid(phase: str, status: str) -> None",?:__init__,PhaseStatusInvariantEnforcementError | check | sorted,,pure,no,28,Ensure a phase-status combination is valid or raise error.
policies,L5,phase_status_invariants,PhaseStatusInvariantChecker.from_governance_config,from_governance_config(config: Any) -> 'PhaseStatusInvariantChecker',?:__init__,cls | getattr,,pure,no,12,Create checker from GovernanceConfig.
policies,L5,phase_status_invariants,PhaseStatusInvariantChecker.get_allowed_statuses,get_allowed_statuses(phase: str) -> FrozenSet[str],?:__init__,frozenset | get | upper,,pure,no,11,Get allowed statuses for a phase.
policies,L5,phase_status_invariants,PhaseStatusInvariantChecker.is_valid_combination,"is_valid_combination(phase: str, status: str) -> bool",?:__init__,get_allowed_statuses,,pure,no,15,Check if a phase-status combination is valid.
policies,L5,phase_status_invariants,PhaseStatusInvariantChecker.should_allow_transition,"should_allow_transition(phase: str, status: str) -> tuple[bool, str]",?:__init__,check,,pure,no,26,Check if a transition should be allowed.
policies,L5,phase_status_invariants,PhaseStatusInvariantEnforcementError.__init__,"__init__(message: str, phase: str, status: str, allowed_statuses: FrozenSet[str], enforcement_enabled: bool)",?:__init__,__init__ | super,,pure,no,13,
policies,L5,phase_status_invariants,PhaseStatusInvariantEnforcementError.to_dict,"to_dict() -> dict[str, Any]",?:__init__,list | str,,pure,no,10,Convert to dictionary for logging/API responses.
policies,L5,phase_status_invariants,check_phase_status_invariant,"check_phase_status_invariant(phase: str, status: str, enforcement_enabled: bool) -> InvariantCheckResponse",?:__init__,PhaseStatusInvariantChecker | check,,pure,no,18,Quick helper to check a phase-status invariant.
policies,L5,phase_status_invariants,ensure_phase_status_invariant,"ensure_phase_status_invariant(phase: str, status: str, enforcement_enabled: bool) -> None",?:__init__,PhaseStatusInvariantChecker | ensure_valid,,pure,no,18,Quick helper to ensure phase-status invariant or raise error.
policies,L5,plan,Plan.__post_init__,__post_init__() -> None,?:evidence_sink | ?:execution_envelope | ?:tenant_auth | ?:guard | ?:billing_gate | ?:billing_dependencies | ?:__init__ | ?:accounts_facade | ?:tenant_service | ?:executor,ValueError,,pure,no,8,Validate plan on creation.
policies,L5,plan,PlanTier.from_string,from_string(value: str) -> 'PlanTier',?:evidence_sink | ?:execution_envelope | ?:tenant_auth | ?:guard | ?:billing_gate | ?:billing_dependencies | ?:__init__ | ?:accounts_facade | ?:tenant_service | ?:executor,ValueError | lower | strip,,pure,no,8,Parse tier from string (case-insensitive).
policies,L5,plan_generation_engine,PlanGenerationEngine.__init__,__init__(),?:main,get_planner | get_retriever,budget_tracker | memory | plan_inspector | planners | skills,pure,no,4,Initialize the plan generation engine.
policies,L5,plan_generation_engine,PlanGenerationEngine.generate,generate(context: PlanGenerationContext) -> PlanGenerationResult,?:main,PlanGenerationResult | RuntimeError | debug | dumps | error | get | get_context_for_planning | get_skill_manifest | info | join | len | plan | validate_plan | warning,budget_tracker | memory | plan_inspector | planners | skills,pure,no,96,Generate a plan for a run.
policies,L5,plan_generation_engine,generate_plan_for_run,"generate_plan_for_run(agent_id: str, goal: str, run_id: str) -> PlanGenerationResult",?:main,PlanGenerationContext | PlanGenerationEngine | generate | get_budget_tracker | get_status,budget_tracker | memory | plan_inspector | planners | skills,pure,no,35,Convenience function to generate a plan for a run.
policies,L5,policies_facade,PoliciesFacade.get_dependency_graph,async get_dependency_graph(**kwargs) -> DependencyGraphResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.get_lesson_stats,async get_lesson_stats(**kwargs) -> LessonStatsResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.get_limit_detail,async get_limit_detail(**kwargs) -> Optional[LimitDetailResult],?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.get_policy_metrics,async get_policy_metrics(**kwargs) -> PolicyMetricsResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.get_policy_rule_detail,async get_policy_rule_detail(**kwargs) -> Optional[PolicyRuleDetailResult],?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.get_policy_state,async get_policy_state(**kwargs) -> PolicyStateResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.list_budgets,async list_budgets(**kwargs) -> BudgetsListResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.list_conflicts,async list_conflicts(**kwargs) -> ConflictsListResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.list_lessons,async list_lessons(**kwargs) -> LessonsListResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.list_limits,async list_limits(**kwargs) -> LimitsListResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.list_policy_rules,async list_policy_rules(**kwargs) -> PolicyRulesListResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.list_requests,async list_requests(**kwargs) -> PolicyRequestsListResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,PoliciesFacade.list_violations,async list_violations(**kwargs) -> ViolationsListResult,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,NotImplementedError,,pure,yes,2,
policies,L5,policies_facade,get_policies_facade,get_policies_facade() -> PoliciesFacade,?:policies | L4:policies_handler | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PoliciesFacade,,pure,no,6,Get the PoliciesFacade singleton instance.
policies,L5,policies_limits_query_engine,LimitsQueryEngine.__init__,__init__(driver: Any),,,asyncio | limits_read_driver,pure,no,6,Args:
policies,L5,policies_limits_query_engine,LimitsQueryEngine.get_limit_detail,"async get_limit_detail(tenant_id: str, limit_id: str) -> Optional[LimitDetailResult]",,LimitDetailResult | fetch_limit_by_id,asyncio | limits_read_driver,pure,yes,30,Get limit detail. Tenant isolation enforced.
policies,L5,policies_limits_query_engine,LimitsQueryEngine.list_budgets,async list_budgets(tenant_id: str) -> BudgetsListResult,,BudgetDefinitionResult | BudgetsListResult | fetch_budget_limits | len,asyncio | limits_read_driver,pure,yes,43,List budget definitions for the tenant.
policies,L5,policies_limits_query_engine,LimitsQueryEngine.list_limits,async list_limits(tenant_id: str) -> LimitsListResult,,LimitSummaryResult | LimitsListResult | fetch_limits | isoformat | len,asyncio | limits_read_driver,pure,yes,74,List limits for the tenant.
policies,L5,policies_limits_query_engine,get_limits_query_engine,get_limits_query_engine(session: 'AsyncSession') -> LimitsQueryEngine,,LimitsQueryEngine | RuntimeError | get | getLogger | get_limits_read_driver | warning,asyncio | limits_read_driver,pure,no,29,Get a LimitsQueryEngine instance.
policies,L5,policies_proposals_query_engine,ProposalsQueryEngine.__init__,__init__(driver: ProposalsReadDriver),,,asyncio | proposals_read_driver,pure,no,2,
policies,L5,policies_proposals_query_engine,ProposalsQueryEngine.count_drafts,async count_drafts(tenant_id: str) -> int,,count_draft_proposals,asyncio | proposals_read_driver,pure,yes,6,Count draft proposals for badge display.
policies,L5,policies_proposals_query_engine,ProposalsQueryEngine.get_policy_request_detail,"async get_policy_request_detail(tenant_id: str, proposal_id: str) -> Optional[PolicyRequestDetailResult]",,PolicyRequestDetailResult | fetch_proposal_by_id,asyncio | proposals_read_driver,pure,yes,27,Get policy request detail. Tenant isolation enforced.
policies,L5,policies_proposals_query_engine,ProposalsQueryEngine.list_policy_requests,async list_policy_requests(tenant_id: str) -> PolicyRequestsListResult,,PolicyRequestResult | PolicyRequestsListResult | fetch_proposals | len,asyncio | proposals_read_driver,external_api,yes,54,List pending policy requests.
policies,L5,policies_proposals_query_engine,get_proposals_query_engine,get_proposals_query_engine(session: 'AsyncSession') -> ProposalsQueryEngine,,ProposalsQueryEngine | get_proposals_read_driver,asyncio | proposals_read_driver,pure,no,5,Get a ProposalsQueryEngine instance.
policies,L5,policies_rules_query_engine,PolicyRulesQueryEngine.__init__,__init__(driver: PolicyRulesReadDriver),,,asyncio | policy_rules_read_driver,pure,no,2,
policies,L5,policies_rules_query_engine,PolicyRulesQueryEngine.count_rules,"async count_rules(tenant_id: str, status: str) -> int",,count_policy_rules,asyncio | policy_rules_read_driver,pure,yes,7,Count policy rules for tenant.
policies,L5,policies_rules_query_engine,PolicyRulesQueryEngine.get_policy_rule_detail,"async get_policy_rule_detail(tenant_id: str, rule_id: str) -> Optional[PolicyRuleDetailResult]",,PolicyRuleDetailResult | fetch_policy_rule_by_id,asyncio | policy_rules_read_driver,pure,yes,29,Get policy rule detail. Tenant isolation enforced.
policies,L5,policies_rules_query_engine,PolicyRulesQueryEngine.list_policy_rules,async list_policy_rules(tenant_id: str) -> PolicyRulesListResult,,PolicyRuleSummaryResult | PolicyRulesListResult | fetch_policy_rules | isoformat | len,asyncio | policy_rules_read_driver,pure,yes,69,List policy rules for the tenant.
policies,L5,policies_rules_query_engine,get_policy_rules_query_engine,get_policy_rules_query_engine(session: 'AsyncSession') -> PolicyRulesQueryEngine,,PolicyRulesQueryEngine | get_policy_rules_read_driver,asyncio | policy_rules_read_driver,pure,no,7,Get a PolicyRulesQueryEngine instance.
policies,L5,policy_command,_record_approval_action,_record_approval_action(result: str) -> None,L3:policy_adapter | ?:__init__,debug | record_approval_action,cost_sim | metrics | policies,pure,no,12,Record approval action metric.
policies,L5,policy_command,_record_approval_escalation,_record_approval_escalation() -> None,L3:policy_adapter | ?:__init__,debug | record_approval_escalation,cost_sim | metrics | policies,pure,no,12,Record approval escalation metric.
policies,L5,policy_command,_record_approval_request_created,_record_approval_request_created(policy_type: str) -> None,L3:policy_adapter | ?:__init__,debug | record_approval_request_created,cost_sim | metrics | policies,pure,no,12,Record approval request creation metric.
policies,L5,policy_command,_record_budget_rejection,"_record_budget_rejection(resource_type: str, skill_id: str) -> None",L3:policy_adapter | ?:__init__,debug | record_budget_rejection,cost_sim | metrics | policies,pure,no,12,Record budget rejection metric.
policies,L5,policy_command,_record_capability_violation,"_record_capability_violation(violation_type: str, skill_id: str, tenant_id: Optional[str]) -> None",L3:policy_adapter | ?:__init__,debug | record_capability_violation,cost_sim | metrics | policies,pure,no,12,Record capability violation metric.
policies,L5,policy_command,_record_policy_decision,"_record_policy_decision(decision: str, policy_type: str) -> None",L3:policy_adapter | ?:__init__,debug | record_policy_decision,cost_sim | metrics | policies,pure,no,12,Record policy decision metric.
policies,L5,policy_command,_record_webhook_fallback,_record_webhook_fallback() -> None,L3:policy_adapter | ?:__init__,debug | record_webhook_fallback,cost_sim | metrics | policies,pure,no,12,Record webhook fallback metric.
policies,L5,policy_command,check_policy_violations,"async check_policy_violations(skill_id: str, tenant_id: str, agent_id: Optional[str], payload: Dict[str, Any], simulated_cost: Optional[int]) -> List[PolicyViolation]",L3:policy_adapter | ?:__init__,MinimalContext | MinimalStep | PolicyEnforcer | PolicyViolation | _record_budget_rejection | _record_capability_violation | append | check_can_execute | debug | str | type,cost_sim | metrics | policies,pure,yes,92,Check for policy violations.
policies,L5,policy_command,evaluate_policy,"async evaluate_policy(skill_id: str, tenant_id: str, agent_id: Optional[str], payload: Dict[str, Any], auto_approve_max_cost_cents: int, approval_level: int) -> PolicyEvaluationResult",L3:policy_adapter | ?:__init__,PolicyEvaluationResult | _record_policy_decision | all | append | check_policy_violations | simulate_cost,cost_sim | metrics | policies,pure,yes,78,Evaluate policy for a skill execution.
policies,L5,policy_command,record_approval_created,record_approval_created(policy_type: str) -> None,L3:policy_adapter | ?:__init__,_record_approval_request_created,cost_sim | metrics | policies,pure,no,8,Record that an approval request was created.
policies,L5,policy_command,record_approval_outcome,record_approval_outcome(result: str) -> None,L3:policy_adapter | ?:__init__,_record_approval_action,cost_sim | metrics | policies,pure,no,8,Record approval outcome (approved/rejected/expired).
policies,L5,policy_command,record_escalation,record_escalation() -> None,L3:policy_adapter | ?:__init__,_record_approval_escalation,cost_sim | metrics | policies,pure,no,8,Record that an escalation occurred.
policies,L5,policy_command,record_webhook_used,record_webhook_used() -> None,L3:policy_adapter | ?:__init__,_record_webhook_fallback,cost_sim | metrics | policies,pure,no,8,Record that webhook fallback was used.
policies,L5,policy_command,simulate_cost,"async simulate_cost(skill_id: str, tenant_id: str, payload: Dict[str, Any]) -> Optional[int]",L3:policy_adapter | ?:__init__,CostSimulator | debug | int | simulate | warning,cost_sim | metrics | policies,pure,yes,34,Simulate cost for a skill execution.
policies,L5,policy_conflict_resolver,create_conflict_log,"create_conflict_log(run_id: str, resolved: ResolvedAction, strategy: ConflictResolutionStrategy) -> PolicyConflictLog",,PolicyConflictLog | isoformat | now,,pure,no,24,Create audit log entry for conflict resolution.
policies,L5,policy_conflict_resolver,get_action_severity,get_action_severity(action: str) -> int,,get | upper,,pure,no,11,Get the severity level for an action.
policies,L5,policy_conflict_resolver,is_more_restrictive,"is_more_restrictive(action_a: str, action_b: str) -> bool",,get_action_severity,,pure,no,12,Check if action_a is more restrictive than action_b.
policies,L5,policy_conflict_resolver,resolve_policy_conflict,"resolve_policy_conflict(actions: List[PolicyAction], strategy: ConflictResolutionStrategy) -> ResolvedAction",,ResolvedAction | get | info | len | set | sorted | upper,,pure,no,82,Resolve conflict when multiple policies trigger.
policies,L5,policy_driver,PolicyDriver.__init__,__init__(db_url: Optional[str]),?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,,engine,pure,no,10,Initialize driver with optional database URL.
policies,L5,policy_driver,PolicyDriver._engine,_engine(),?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,PolicyEngine,engine,pure,no,6,Lazy-load policy engine.
policies,L5,policy_driver,PolicyDriver.acknowledge_violation,"async acknowledge_violation(db, violation_id: str, notes: Optional[str]) -> bool",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,acknowledge_violation,engine,pure,yes,3,Acknowledge a violation (mark as reviewed).
policies,L5,policy_driver,PolicyDriver.activate_policy_version,"async activate_policy_version(db, version_id: str, activated_by: str, dry_run: bool)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,activate_policy_version,engine,pure,yes,7,Activate a policy version with pre-activation integrity checks.
policies,L5,policy_driver,PolicyDriver.add_dependency_with_dag_check,"async add_dependency_with_dag_check(db, source_policy: str, target_policy: str, dependency_type: str, resolution_strategy: str, priority: int, description: Optional[str])",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,add_dependency_with_dag_check,engine,pure,yes,20,Add a policy dependency with DAG validation.
policies,L5,policy_driver,PolicyDriver.clear_cooldowns,"async clear_cooldowns(db, agent_id: str, rule_name: Optional[str]) -> int",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,clear_cooldowns,engine,pure,yes,3,Clear cooldowns for an agent.
policies,L5,policy_driver,PolicyDriver.create_policy_version,"async create_policy_version(db, description: str, created_by: str)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,create_policy_version,engine,pure,yes,3,Create a new policy version snapshot.
policies,L5,policy_driver,PolicyDriver.create_temporal_policy,"async create_temporal_policy(db, data: Dict)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,create_temporal_policy,engine,pure,yes,3,Create a new temporal policy.
policies,L5,policy_driver,PolicyDriver.evaluate,"async evaluate(request, db, dry_run: bool)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,debug | evaluate | str,engine,pure,yes,17,Evaluate a request against all applicable policies.
policies,L5,policy_driver,PolicyDriver.evaluate_with_context,"async evaluate_with_context(db, action_type, policy_context, proposed_action: Optional[str], target_resource: Optional[str], estimated_cost: Optional[float], data_categories: Optional[List[str]], context: Optional[Dict[str, Any]])",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,evaluate_with_context,engine,pure,yes,22,Context-aware policy evaluation.
policies,L5,policy_driver,PolicyDriver.get_active_cooldowns,"async get_active_cooldowns(db, agent_id: Optional[str])",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_active_cooldowns,engine,pure,yes,3,List all active cooldowns.
policies,L5,policy_driver,PolicyDriver.get_current_version,async get_current_version(db),?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_current_version,engine,pure,yes,3,Get the currently active policy version.
policies,L5,policy_driver,PolicyDriver.get_dependency_graph,async get_dependency_graph(db),?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_dependency_graph,engine,pure,yes,3,Get the policy dependency graph.
policies,L5,policy_driver,PolicyDriver.get_ethical_constraints,"async get_ethical_constraints(db, include_inactive: bool)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_ethical_constraints,engine,pure,yes,3,List all ethical constraints.
policies,L5,policy_driver,PolicyDriver.get_metrics,"async get_metrics(db, hours: int)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_metrics,engine,pure,yes,3,Get policy engine metrics for the specified time window.
policies,L5,policy_driver,PolicyDriver.get_policy_conflicts,"async get_policy_conflicts(db, include_resolved: bool)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_policy_conflicts,engine,pure,yes,3,List policy conflicts.
policies,L5,policy_driver,PolicyDriver.get_policy_versions,"async get_policy_versions(db, limit: int, include_inactive: bool)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_policy_versions,engine,pure,yes,3,List all policy versions.
policies,L5,policy_driver,PolicyDriver.get_risk_ceiling,"async get_risk_ceiling(db, ceiling_id: str)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_risk_ceiling,engine,pure,yes,3,Get a specific risk ceiling.
policies,L5,policy_driver,PolicyDriver.get_risk_ceilings,"async get_risk_ceilings(db, tenant_id: Optional[str], include_inactive: bool)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_risk_ceilings,engine,pure,yes,8,List all risk ceilings.
policies,L5,policy_driver,PolicyDriver.get_safety_rules,"async get_safety_rules(db, tenant_id: Optional[str], include_inactive: bool)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_safety_rules,engine,pure,yes,8,List all safety rules.
policies,L5,policy_driver,PolicyDriver.get_state,async get_state(db),?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_state,engine,pure,yes,3,Get the current state of the policy layer.
policies,L5,policy_driver,PolicyDriver.get_temporal_policies,"async get_temporal_policies(db, metric: Optional[str], include_inactive: bool)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_temporal_policies,engine,pure,yes,5,List temporal (sliding window) policies.
policies,L5,policy_driver,PolicyDriver.get_temporal_storage_stats,async get_temporal_storage_stats(db),?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_temporal_storage_stats,engine,pure,yes,3,Get storage statistics for temporal metrics.
policies,L5,policy_driver,PolicyDriver.get_temporal_utilization,"async get_temporal_utilization(db, policy_id: str, agent_id: Optional[str])",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_temporal_utilization,engine,pure,yes,5,Get current utilization for a temporal policy.
policies,L5,policy_driver,PolicyDriver.get_topological_evaluation_order,get_topological_evaluation_order(dependencies: List),?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_topological_evaluation_order,engine,pure,no,3,Get the topological evaluation order for policies.
policies,L5,policy_driver,PolicyDriver.get_version_provenance,"async get_version_provenance(db, version_id: str)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_version_provenance,engine,pure,yes,3,Get the provenance (change history) for a policy version.
policies,L5,policy_driver,PolicyDriver.get_violation,"async get_violation(db, violation_id: str)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_violation,engine,pure,yes,3,Get a specific violation by ID.
policies,L5,policy_driver,PolicyDriver.get_violations,"async get_violations(db, violation_type, agent_id: Optional[str], tenant_id: Optional[str], severity_min: Optional[float], since: Optional[datetime], limit: int)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,get_violations,engine,pure,yes,20,Get policy violations with filtering.
policies,L5,policy_driver,PolicyDriver.pre_check,"async pre_check(request_id: str, agent_id: str, goal: str, tenant_id: str) -> Dict[str, Any]",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,debug | pre_check,engine,pure,yes,14,Pre-check policy constraints before run creation.
policies,L5,policy_driver,PolicyDriver.prune_temporal_metrics,"async prune_temporal_metrics(db, retention_hours: int, compact_older_than_hours: int, max_events_per_policy: int)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,prune_temporal_metrics,engine,pure,yes,14,Prune and compact temporal metric events.
policies,L5,policy_driver,PolicyDriver.reload_policies,async reload_policies(db),?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,info | reload_policies,engine,pure,yes,4,Hot-reload policies from database.
policies,L5,policy_driver,PolicyDriver.reset_risk_ceiling,"async reset_risk_ceiling(db, ceiling_id: str) -> bool",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,reset_risk_ceiling,engine,pure,yes,3,Reset a risk ceiling's current value to 0.
policies,L5,policy_driver,PolicyDriver.resolve_conflict,"async resolve_conflict(db, conflict_id: str, resolution: str, resolved_by: str) -> bool",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,resolve_conflict,engine,pure,yes,5,Resolve a policy conflict.
policies,L5,policy_driver,PolicyDriver.rollback_to_version,"async rollback_to_version(db, target_version: str, reason: str, rolled_back_by: str)",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,rollback_to_version,engine,pure,yes,5,Rollback to a previous policy version.
policies,L5,policy_driver,PolicyDriver.update_risk_ceiling,"async update_risk_ceiling(db, ceiling_id: str, updates: Dict[str, Any])",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,update_risk_ceiling,engine,pure,yes,3,Update a risk ceiling configuration.
policies,L5,policy_driver,PolicyDriver.update_safety_rule,"async update_safety_rule(db, rule_id: str, updates: Dict[str, Any])",?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,update_safety_rule,engine,pure,yes,3,Update a safety rule configuration.
policies,L5,policy_driver,PolicyDriver.validate_dependency_dag,async validate_dependency_dag(db),?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,validate_dependency_dag,engine,pure,yes,3,Validate that policy dependencies form a valid DAG.
policies,L5,policy_driver,get_policy_driver,get_policy_driver(db_url: Optional[str]) -> PolicyDriver,?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,PolicyDriver,engine,pure,no,19,Get the PolicyDriver singleton.
policies,L5,policy_driver,reset_policy_driver,reset_policy_driver() -> None,?:__init__ | ?:policy_driver | L4:policies_handler | L5:governance_facade,,engine,pure,no,4,Reset the driver singleton (for testing).
policies,L5,policy_graph_engine,ConflictDetectionResult.to_dict,"to_dict() -> dict[str, Any]",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,isoformat | len | to_dict,__future__ | policy_graph_driver,pure,no,7,
policies,L5,policy_graph_engine,DependencyGraphResult.to_dict,"to_dict() -> dict[str, Any]",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,isoformat | to_dict,__future__ | policy_graph_driver,pure,no,6,
policies,L5,policy_graph_engine,PolicyConflict.to_dict,"to_dict() -> dict[str, Any]",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,isoformat,__future__ | policy_graph_driver,pure,no,12,
policies,L5,policy_graph_engine,PolicyConflictEngine.__init__,__init__(tenant_id: str),?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,,__future__ | policy_graph_driver,pure,no,2,
policies,L5,policy_graph_engine,PolicyConflictEngine._detect_priority_overrides,async _detect_priority_overrides(policies: list[dict]) -> list[PolicyConflict],?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PolicyConflict | append | enumerate | get | items | len,__future__ | policy_graph_driver,pure,yes,46,Detect PRIORITY_OVERRIDE conflicts.
policies,L5,policy_graph_engine,PolicyConflictEngine._detect_scope_overlaps,async _detect_scope_overlaps(policies: list[dict]) -> list[PolicyConflict],?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PolicyConflict | _has_contradicting_conditions | append | enumerate | get | items | len,__future__ | policy_graph_driver,pure,yes,62,Detect SCOPE_OVERLAP conflicts.
policies,L5,policy_graph_engine,PolicyConflictEngine._detect_temporal_conflicts,async _detect_temporal_conflicts(policies: list[dict]) -> list[PolicyConflict],?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PolicyConflict | _time_windows_overlap | append | enumerate | get,__future__ | policy_graph_driver,pure,yes,34,Detect TEMPORAL_CONFLICT conflicts.
policies,L5,policy_graph_engine,PolicyConflictEngine._detect_threshold_contradictions,async _detect_threshold_contradictions(driver: PolicyGraphDriver) -> list[PolicyConflict],?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PolicyConflict | append | fetch_active_limits | items | len | max | min | split,__future__ | policy_graph_driver,pure,yes,47,Detect THRESHOLD_CONTRADICTION conflicts.
policies,L5,policy_graph_engine,PolicyConflictEngine._has_contradicting_conditions,"_has_contradicting_conditions(cond1: dict, cond2: dict) -> bool",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,isinstance,__future__ | policy_graph_driver,pure,no,11,Check if two condition sets have explicit contradictions.
policies,L5,policy_graph_engine,PolicyConflictEngine._involves_policy,"_involves_policy(policy: dict, policy_id: str) -> bool",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,get,__future__ | policy_graph_driver,pure,no,3,Check if a conflict involves a specific policy.
policies,L5,policy_graph_engine,PolicyConflictEngine._time_windows_overlap,"_time_windows_overlap(tw1: dict, tw2: dict) -> bool",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,get,__future__ | policy_graph_driver,pure,no,9,Check if two time windows overlap.
policies,L5,policy_graph_engine,PolicyConflictEngine.detect_conflicts,"async detect_conflicts(driver: PolicyGraphDriver, policy_id: Optional[str], severity_filter: Optional[ConflictSeverity], include_resolved: bool) -> ConflictDetectionResult",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,ConflictDetectionResult | _detect_priority_overrides | _detect_scope_overlaps | _detect_temporal_conflicts | _detect_threshold_contradictions | _involves_policy | extend | fetch_active_policies | fetch_resolved_conflicts | len,__future__ | policy_graph_driver,pure,yes,55,Detect conflicts between policies.
policies,L5,policy_graph_engine,PolicyDependency.to_dict,"to_dict() -> dict[str, Any]",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,,__future__ | policy_graph_driver,pure,no,10,
policies,L5,policy_graph_engine,PolicyDependencyEngine.__init__,__init__(tenant_id: str),?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,,__future__ | policy_graph_driver,pure,no,2,
policies,L5,policy_graph_engine,PolicyDependencyEngine._detect_explicit_dependencies,_detect_explicit_dependencies(policies: list[dict]) -> list[PolicyDependency],?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PolicyDependency | append | get,__future__ | policy_graph_driver,pure,no,44,Detect EXPLICIT dependencies.
policies,L5,policy_graph_engine,PolicyDependencyEngine._detect_implicit_limit_dependencies,"_detect_implicit_limit_dependencies(policies: list[dict], limits: list[dict]) -> list[PolicyDependency]",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PolicyDependency | append | startswith,__future__ | policy_graph_driver,pure,no,47,Detect IMPLICIT_LIMIT dependencies.
policies,L5,policy_graph_engine,PolicyDependencyEngine._detect_implicit_scope_dependencies,_detect_implicit_scope_dependencies(policies: list[dict]) -> list[PolicyDependency],?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PolicyDependency | append | items | len,__future__ | policy_graph_driver,pure,no,55,Detect IMPLICIT_SCOPE dependencies.
policies,L5,policy_graph_engine,PolicyDependencyEngine.check_can_activate,"async check_can_activate(driver: PolicyGraphDriver, policy_id: str) -> tuple[bool, list[str]]",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,append | compute_dependency_graph | len | next,__future__ | policy_graph_driver,pure,yes,28,Check if a policy can be activated.
policies,L5,policy_graph_engine,PolicyDependencyEngine.check_can_delete,"async check_can_delete(driver: PolicyGraphDriver, policy_id: str) -> tuple[bool, list[str]]",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,compute_dependency_graph | get | len | next,__future__ | policy_graph_driver,pure,yes,24,Check if a policy can be deleted.
policies,L5,policy_graph_engine,PolicyDependencyEngine.compute_dependency_graph,"async compute_dependency_graph(driver: PolicyGraphDriver, policy_id: Optional[str]) -> DependencyGraphResult",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,DependencyGraphResult | PolicyNode | _detect_explicit_dependencies | _detect_implicit_limit_dependencies | _detect_implicit_scope_dependencies | add | append | extend | fetch_all_limits | fetch_all_policies,__future__ | policy_graph_driver,db_write,yes,71,Compute the policy dependency graph.
policies,L5,policy_graph_engine,PolicyNode.to_dict,"to_dict() -> dict[str, Any]",?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,,__future__ | policy_graph_driver,pure,no,11,
policies,L5,policy_graph_engine,get_conflict_engine,get_conflict_engine(tenant_id: str) -> PolicyConflictEngine,?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PolicyConflictEngine,__future__ | policy_graph_driver,pure,no,3,Get a PolicyConflictEngine instance for a tenant.
policies,L5,policy_graph_engine,get_dependency_engine,get_dependency_engine(tenant_id: str) -> PolicyDependencyEngine,?:policies_facade | ?:policy_proposal | L5:policy_proposal_engine | ?:policy_conflict_result | ?:policy_node_result | ?:policy_dependency_edge | ?:dependency_graph_result,PolicyDependencyEngine,__future__ | policy_graph_driver,pure,no,3,Get a PolicyDependencyEngine instance for a tenant.
policies,L5,policy_limits_engine,PolicyLimitsService.__init__,"__init__(session: 'AsyncSession', audit: Any, driver: Any)",L4:policies_handler,RuntimeError | get | getLogger | get_policy_limits_driver | warning,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_limits | policy_limits_driver | time,pure,no,33,Args:
policies,L5,policy_limits_engine,PolicyLimitsService._get_limit,"async _get_limit(tenant_id: str, limit_id: str) -> Limit",L4:policies_handler,LimitNotFoundError | fetch_limit_by_id,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_limits | policy_limits_driver | time,pure,yes,8,Get limit by ID with tenant check.
policies,L5,policy_limits_engine,PolicyLimitsService._to_response,_to_response(limit: Limit) -> PolicyLimitResponse,L4:policies_handler,PolicyLimitResponse,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_limits | policy_limits_driver | time,pure,no,19,Convert model to response.
policies,L5,policy_limits_engine,PolicyLimitsService._validate_category_fields,_validate_category_fields(request: CreatePolicyLimitRequest) -> None,L4:policies_handler,LimitValidationError,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_limits | policy_limits_driver | time,pure,no,11,Validate category-specific required fields.
policies,L5,policy_limits_engine,PolicyLimitsService.create,"async create(tenant_id: str, request: CreatePolicyLimitRequest, created_by: Optional[str]) -> PolicyLimitResponse",L4:policies_handler,Decimal | Limit | LimitIntegrity | _to_response | _validate_category_fields | add_integrity | add_limit | begin | generate_uuid | limit_created | str | utc_now,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_limits | policy_limits_driver | time,pure,yes,87,Create a new policy limit.
policies,L5,policy_limits_engine,PolicyLimitsService.delete,"async delete(tenant_id: str, limit_id: str, deleted_by: Optional[str]) -> None",L4:policies_handler,_get_limit | flush | utc_now,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_limits | policy_limits_driver | time,db_write,yes,23,Delete (disable) a policy limit.
policies,L5,policy_limits_engine,PolicyLimitsService.get,"async get(tenant_id: str, limit_id: str) -> PolicyLimitResponse",L4:policies_handler,_get_limit | _to_response,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_limits | policy_limits_driver | time,pure,yes,20,Get a policy limit by ID.
policies,L5,policy_limits_engine,PolicyLimitsService.update,"async update(tenant_id: str, limit_id: str, request: UpdatePolicyLimitRequest, updated_by: Optional[str]) -> PolicyLimitResponse",L4:policies_handler,_get_limit | _to_response | begin | limit_updated | str | utc_now,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_limits | policy_limits_driver | time,pure,yes,84,Update an existing policy limit.
policies,L5,policy_mapper,MCPPolicyDecision.allow,"allow(tool_name: str, server_id: str, policy_id: Optional[str]) -> 'MCPPolicyDecision'",?:__init__ | ?:audit_evidence | L5:audit_evidence,cls,engine,pure,no,13,Create an allow decision.
policies,L5,policy_mapper,MCPPolicyDecision.deny,"deny(tool_name: str, server_id: str, reason: MCPDenyReason, message: Optional[str], policy_id: Optional[str]) -> 'MCPPolicyDecision'",?:__init__ | ?:audit_evidence | L5:audit_evidence,cls,engine,pure,no,17,Create a deny decision.
policies,L5,policy_mapper,MCPPolicyDecision.to_dict,"to_dict() -> Dict[str, Any]",?:__init__ | ?:audit_evidence | L5:audit_evidence,,engine,pure,no,11,Convert to dictionary for logging/serialization.
policies,L5,policy_mapper,MCPPolicyMapper.__init__,__init__(policy_engine: Optional[Any]),?:__init__ | ?:audit_evidence | L5:audit_evidence,,engine,pure,no,10,Initialize policy mapper.
policies,L5,policy_mapper,MCPPolicyMapper._check_explicit_allow,"async _check_explicit_allow(tenant_id: str, server_id: str, tool_name: str) -> bool",?:__init__ | ?:audit_evidence | L5:audit_evidence,,engine,pure,yes,9,Check if tenant has explicit allow for dangerous tool.
policies,L5,policy_mapper,MCPPolicyMapper._check_rate_limit,"async _check_rate_limit(tenant_id: str, tool_key: str, max_per_minute: int) -> bool",?:__init__ | ?:audit_evidence | L5:audit_evidence,,engine,pure,yes,9,Check if rate limit exceeded.
policies,L5,policy_mapper,MCPPolicyMapper._evaluate_policy,"async _evaluate_policy(tenant_id: str, server_id: str, tool_name: str, run_id: str, required_permissions: List[str]) -> Any",?:__init__ | ?:audit_evidence | L5:audit_evidence,PolicyResult | _get_policy_engine | check_permission | str | warning,engine,pure,yes,55,Evaluate policy engine for required permissions.
policies,L5,policy_mapper,MCPPolicyMapper._get_policy_engine,_get_policy_engine() -> Optional[Any],?:__init__ | ?:audit_evidence | L5:audit_evidence,debug | get_policy_engine,engine,pure,no,12,Get policy engine (lazy initialization).
policies,L5,policy_mapper,MCPPolicyMapper.check_tool_invocation,"async check_tool_invocation(tenant_id: str, server_id: str, tool_name: str, run_id: str, input_params: Optional[Dict[str, Any]]) -> MCPPolicyDecision",?:__init__ | ?:audit_evidence | L5:audit_evidence,_check_explicit_allow | _check_rate_limit | _evaluate_policy | allow | debug | deny | get | info | warning,engine,pure,yes,126,Check if tool invocation is allowed.
policies,L5,policy_mapper,MCPPolicyMapper.register_tool_policy,"async register_tool_policy(server_id: str, tool_name: str, required_permissions: Optional[List[str]], is_dangerous: bool, max_calls_per_minute: Optional[int]) -> MCPToolPolicy",?:__init__ | ?:audit_evidence | L5:audit_evidence,MCPToolPolicy | info,engine,pure,yes,42,Register policy for a tool.
policies,L5,policy_mapper,configure_mcp_policy_mapper,configure_mcp_policy_mapper(policy_engine: Optional[Any]) -> MCPPolicyMapper,?:__init__ | ?:audit_evidence | L5:audit_evidence,MCPPolicyMapper | info,engine,pure,no,22,Configure the singleton MCPPolicyMapper.
policies,L5,policy_mapper,get_mcp_policy_mapper,get_mcp_policy_mapper() -> MCPPolicyMapper,?:__init__ | ?:audit_evidence | L5:audit_evidence,MCPPolicyMapper | info,engine,pure,no,14,Get or create the singleton MCPPolicyMapper.
policies,L5,policy_mapper,reset_mcp_policy_mapper,reset_mcp_policy_mapper() -> None,?:__init__ | ?:audit_evidence | L5:audit_evidence,,engine,pure,no,4,Reset the singleton (for testing).
policies,L5,policy_proposal_engine,PolicyActivationBlockedError.__init__,"__init__(message: str, conflicts: list[dict])",,__init__ | super,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,no,3,
policies,L5,policy_proposal_engine,PolicyDeletionBlockedError.__init__,"__init__(message: str, dependents: list[str])",,__init__ | super,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,no,3,
policies,L5,policy_proposal_engine,PolicyProposalEngine.__init__,"__init__(read_driver: PolicyProposalReadDriver, write_driver: PolicyProposalWriteDriver)",,,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,no,7,
policies,L5,policy_proposal_engine,PolicyProposalEngine._create_policy_rule_from_proposal,"async _create_policy_rule_from_proposal(proposal: dict, approved_by: str) -> str",,check_rule_exists | create_policy_rule | get | info | replace,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,61,Create a policy_rule from an approved proposal.
policies,L5,policy_proposal_engine,PolicyProposalEngine.check_proposal_eligibility,"async check_proposal_eligibility(tenant_id: Optional[UUID], feedback_type: Optional[str], threshold: int) -> list[dict]",,append | fetch_unacknowledged_feedback | get | info | items | len,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,61,Check if feedback patterns are eligible for policy proposals.
policies,L5,policy_proposal_engine,PolicyProposalEngine.create_proposal,async create_proposal(proposal: PolicyProposalCreate) -> str,,create_proposal | info | len | str,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,33,Create a policy proposal.
policies,L5,policy_proposal_engine,PolicyProposalEngine.delete_policy_rule,"async delete_policy_rule(session: 'AsyncSession', rule_id: str, tenant_id: str, deleted_by: str) -> bool",,PolicyDeletionBlockedError | ValueError | check_can_delete | delete_policy_rule | fetch_rule_by_id | get_dependency_engine | info | join | len | warning,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,65,Delete a policy rule with GOV-POL-002 enforcement.
policies,L5,policy_proposal_engine,PolicyProposalEngine.get_proposal_summary,"async get_proposal_summary(tenant_id: Optional[UUID], status: Optional[str], limit: int) -> dict",,fetch_proposals | get | isoformat | len,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,52,Get policy proposal summary for ops visibility.
policies,L5,policy_proposal_engine,PolicyProposalEngine.review_proposal,"async review_proposal(session: 'AsyncSession', proposal_id: UUID, review: PolicyApprovalRequest, audit: Any) -> dict",,PolicyActivationBlockedError | ValueError | _create_policy_rule_from_proposal | count_versions_for_proposal | create_version | detect_conflicts | fetch_proposal_by_id | get_conflict_engine | info | isoformat | len | policy_proposal_approved | policy_proposal_rejected | str | to_dict,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,169,Review (approve/reject) a policy proposal.
policies,L5,policy_proposal_engine,check_proposal_eligibility,"async check_proposal_eligibility(session: 'AsyncSession', tenant_id: Optional[UUID], feedback_type: Optional[str], threshold: int) -> list[dict]",,check_proposal_eligibility | get_policy_proposal_engine,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,13,Backward-compatible wrapper for eligibility checking.
policies,L5,policy_proposal_engine,create_policy_proposal,"async create_policy_proposal(session: 'AsyncSession', proposal: PolicyProposalCreate) -> str",,create_proposal | get_policy_proposal_engine,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,7,Backward-compatible wrapper for proposal creation.
policies,L5,policy_proposal_engine,delete_policy_rule,"async delete_policy_rule(session: 'AsyncSession', rule_id: str, tenant_id: str, deleted_by: str) -> bool",,delete_policy_rule | get_policy_proposal_engine,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,9,Backward-compatible wrapper for rule deletion.
policies,L5,policy_proposal_engine,generate_default_rule,"generate_default_rule(policy_type: str, feedback_type: str) -> dict",,,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,no,28,Generate a default rule template based on policy type.
policies,L5,policy_proposal_engine,get_policy_proposal_engine,get_policy_proposal_engine(session: 'AsyncSession') -> PolicyProposalEngine,,PolicyProposalEngine | get_policy_proposal_read_driver | get_policy_proposal_write_driver,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,no,6,Get a PolicyProposalEngine instance with drivers.
policies,L5,policy_proposal_engine,get_proposal_summary,"async get_proposal_summary(session: 'AsyncSession', tenant_id: Optional[UUID], status: Optional[str], limit: int) -> dict",,get_policy_proposal_engine | get_proposal_summary,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,13,Backward-compatible wrapper for proposal summary.
policies,L5,policy_proposal_engine,review_policy_proposal,"async review_policy_proposal(session: 'AsyncSession', proposal_id: UUID, review: PolicyApprovalRequest, audit: Any) -> dict",,get_policy_proposal_engine | review_proposal,asyncio | audit_ledger | policy | policy_graph_engine | policy_proposal_read_driver | policy_proposal_write_driver | time,pure,yes,9,Backward-compatible wrapper for proposal review.
policies,L5,policy_rules_engine,PolicyRulesService.__init__,"__init__(session: 'AsyncSession', audit: Any)",L4:policies_handler,get_policy_rules_driver,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_rules | policy_rules_driver | time,pure,no,9,Args:
policies,L5,policy_rules_engine,PolicyRulesService._compute_hash,_compute_hash(rule: PolicyRule) -> str,L4:policies_handler,dumps | encode | hexdigest | sha256,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_rules | policy_rules_driver | time,pure,no,14,Compute integrity hash for rule.
policies,L5,policy_rules_engine,PolicyRulesService._get_rule,"async _get_rule(tenant_id: str, rule_id: str) -> PolicyRule",L4:policies_handler,RuleNotFoundError | fetch_rule_by_id,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_rules | policy_rules_driver | time,pure,yes,8,Get rule by ID with tenant check.
policies,L5,policy_rules_engine,PolicyRulesService._to_response,_to_response(rule: PolicyRule) -> PolicyRuleResponse,L4:policies_handler,PolicyRuleResponse,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_rules | policy_rules_driver | time,pure,no,23,Convert model to response.
policies,L5,policy_rules_engine,PolicyRulesService._validate_conditions,"_validate_conditions(conditions: dict[str, Any]) -> None",L4:policies_handler,RuleValidationError | isinstance | keys,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_rules | policy_rules_driver | time,pure,no,20,Validate rule conditions syntax.
policies,L5,policy_rules_engine,PolicyRulesService.create,"async create(tenant_id: str, request: CreatePolicyRuleRequest, created_by: Optional[str]) -> PolicyRuleResponse",L4:policies_handler,Decimal | PolicyRule | PolicyRuleIntegrity | _compute_hash | _to_response | _validate_conditions | add_integrity | add_rule | begin | generate_uuid | policy_rule_created | utc_now,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_rules | policy_rules_driver | time,pure,yes,88,Create a new policy rule.
policies,L5,policy_rules_engine,PolicyRulesService.get,"async get(tenant_id: str, rule_id: str) -> PolicyRuleResponse",L4:policies_handler,_get_rule | _to_response,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_rules | policy_rules_driver | time,pure,yes,20,Get a policy rule by ID.
policies,L5,policy_rules_engine,PolicyRulesService.update,"async update(tenant_id: str, rule_id: str, request: UpdatePolicyRuleRequest, updated_by: Optional[str]) -> PolicyRuleResponse",L4:policies_handler,RuleValidationError | _get_rule | _to_response | _validate_conditions | begin | policy_rule_modified | policy_rule_retired | retire | utc_now,asyncio | audit_ledger | cross_domain | policy_control_plane | policy_rules | policy_rules_driver | time,pure,yes,109,Update an existing policy rule.
policies,L5,prevention_engine,PolicyViolationError.__init__,__init__(result: PreventionResult),?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,__init__ | super,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,pure,no,3,
policies,L5,prevention_engine,PreventionEngine.__init__,__init__(policy_snapshot_id: Optional[str]),?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,pure,no,10,Initialize prevention engine.
policies,L5,prevention_engine,PreventionEngine._evaluate_custom_policy,"_evaluate_custom_policy(policy: dict[str, Any], context: PreventionContext) -> PreventionResult",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,allow | block | get | str,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,pure,no,37,Evaluate a custom policy rule.
policies,L5,prevention_engine,PreventionEngine._evaluate_step_inner,_evaluate_step_inner(context: PreventionContext) -> PreventionResult,?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,PolicyAction | _evaluate_custom_policy | allow | append | block | debug | get | len | resolve_policy_conflict | should_evaluate_policy | str | type | upper | warning,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,pure,no,139,"Inner evaluation logic (GAP-068, GAP-031 integrated)."
policies,L5,prevention_engine,PreventionEngine.evaluate_step,evaluate_step(context: PreventionContext) -> PreventionResult,?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,_evaluate_step_inner | allow | block | handle_evaluation_error | str | warning,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,pure,no,49,Evaluate policy at step checkpoint.
policies,L5,prevention_engine,PreventionEngine.load_snapshot,load_snapshot(snapshot_id: str) -> bool,?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,Session | error | exec | first | get_policies | get_thresholds | info | select | str | verify_integrity | warning | where,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,pure,no,57,Load policy snapshot for evaluation.
policies,L5,prevention_engine,PreventionResult.allow,allow() -> 'PreventionResult',?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,cls,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,pure,no,3,Create an ALLOW result.
policies,L5,prevention_engine,PreventionResult.block,"block(policy_id: str, policy_name: str, violation_type: ViolationType, threshold_value: str, actual_value: str, reason: str) -> 'PreventionResult'",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,cls,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,pure,no,19,Create a BLOCK result.
policies,L5,prevention_engine,PreventionResult.warn,"warn(reason: str, policy_id: Optional[str]) -> 'PreventionResult'",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,cls,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,pure,no,7,Create a WARN result.
policies,L5,prevention_engine,create_policy_snapshot_for_run,"create_policy_snapshot_for_run(tenant_id: str, run_id: str) -> Optional[str]",?:arbitrator | ?:__init__ | ?:scope_resolver | ?:step_enforcement | L7:override_authority | L7:monitor_config | L7:threshold_signal | ?:alert_emitter | ?:authority_checker | L4:alert_emitter,Session | add | create_snapshot | error | flush | info | refresh | str,binding_moment_enforcer | conflict_resolver | db | failure_mode_handler | policy_snapshot | sqlmodel,db_write,no,60,Create a policy snapshot at run start.
policies,L5,prevention_hook,PreventionContext.__post_init__,__post_init__(),?:__init__,now,content_accuracy,pure,no,3,
policies,L5,prevention_hook,PreventionHook.__init__,"__init__(strict_mode: bool, block_on_fail: bool, fallback_message: Optional[str])",?:__init__,ContentAccuracyValidator,content_accuracy,pure,no,14,
policies,L5,prevention_hook,PreventionHook.evaluate,evaluate(ctx: PreventionContext) -> PreventionResult,?:__init__,PreventionResult | int | time | validate,content_accuracy,pure,no,44,Evaluate the LLM output against policies.
policies,L5,prevention_hook,PreventionHook.get_safe_response,get_safe_response(ctx: PreventionContext) -> str,?:__init__,lower,content_accuracy,pure,no,33,Generate a safe response when the original fails validation.
policies,L5,prevention_hook,PreventionResult.__post_init__,__post_init__(),?:__init__,str | uuid4,content_accuracy,pure,no,3,
policies,L5,prevention_hook,PreventionResult.to_dict,"to_dict() -> Dict[str, Any]",?:__init__,,content_accuracy,pure,no,12,
policies,L5,prevention_hook,create_prevention_hook,"create_prevention_hook(strict_mode: bool, block_on_fail: bool) -> PreventionHook",?:__init__,PreventionHook,content_accuracy,pure,no,9,Factory function to create a prevention hook.
policies,L5,prevention_hook,evaluate_response,"evaluate_response(tenant_id: str, call_id: str, user_query: str, context_data: Dict[str, Any], llm_output: str, model: str, user_id: Optional[str]) -> PreventionResult",?:__init__,PreventionContext | evaluate | get_prevention_hook | len | split,content_accuracy,pure,no,40,Convenience function to evaluate an LLM response.
policies,L5,prevention_hook,get_prevention_hook,get_prevention_hook() -> PreventionHook,?:__init__,create_prevention_hook,content_accuracy,pure,no,6,Get the global prevention hook instance.
policies,L5,protection_provider,AbuseProtectionProvider.check_all,"check_all(tenant_id: str, endpoint: str, operation: str) -> ProtectionResult",,,billing | decisions,pure,no,21,Run all protection checks in order.
policies,L5,protection_provider,AbuseProtectionProvider.check_burst,"check_burst(tenant_id: str, endpoint: str) -> ProtectionResult",,,billing | decisions,pure,no,12,Check burst control for a tenant/endpoint combination.
policies,L5,protection_provider,AbuseProtectionProvider.check_cost,"check_cost(tenant_id: str, operation: str) -> ProtectionResult",,,billing | decisions,pure,no,12,Check cost guard for a tenant/operation combination.
policies,L5,protection_provider,AbuseProtectionProvider.check_rate_limit,"check_rate_limit(tenant_id: str, endpoint: str) -> ProtectionResult",,,billing | decisions,pure,no,12,Check rate limit for a tenant/endpoint combination.
policies,L5,protection_provider,AbuseProtectionProvider.detect_anomaly,detect_anomaly(tenant_id: str) -> Optional[AnomalySignal],,,billing | decisions,pure,no,13,Detect usage anomaly for a tenant.
policies,L5,protection_provider,MockAbuseProtectionProvider.__init__,__init__() -> None,,,billing | decisions,pure,no,14,Initialize mock provider with in-memory state.
policies,L5,protection_provider,MockAbuseProtectionProvider.add_cost,"add_cost(tenant_id: str, amount: float) -> None",,get,billing | decisions,pure,no,4,Add cost to tenant's daily accumulator (mock/test only).
policies,L5,protection_provider,MockAbuseProtectionProvider.check_all,"check_all(tenant_id: str, endpoint: str, operation: str) -> ProtectionResult",,allow | check_burst | check_cost | check_rate_limit | detect_anomaly | info | warning,billing | decisions,pure,no,38,Run all protection checks in order.
policies,L5,protection_provider,MockAbuseProtectionProvider.check_burst,"check_burst(tenant_id: str, endpoint: str) -> ProtectionResult",,allow | get | int | throttle | time,billing | decisions,pure,no,27,Check burst control for a tenant/endpoint combination.
policies,L5,protection_provider,MockAbuseProtectionProvider.check_cost,"check_cost(tenant_id: str, operation: str) -> ProtectionResult",,allow | get | get_billing_provider | get_limits | get_plan | reject_cost_limit,billing | decisions,pure,no,27,Check cost guard for a tenant/operation combination.
policies,L5,protection_provider,MockAbuseProtectionProvider.check_rate_limit,"check_rate_limit(tenant_id: str, endpoint: str) -> ProtectionResult",,allow | get | reject_rate_limit,billing | decisions,pure,no,26,Check rate limit for a tenant/endpoint combination.
policies,L5,protection_provider,MockAbuseProtectionProvider.detect_anomaly,detect_anomaly(tenant_id: str) -> Optional[AnomalySignal],,AnomalySignal | float | sum | values,billing | decisions,pure,no,23,Detect usage anomaly for a tenant.
policies,L5,protection_provider,MockAbuseProtectionProvider.reset,reset() -> None,,clear,billing | decisions,pure,no,7,Reset all mock state (for testing).
policies,L5,protection_provider,MockAbuseProtectionProvider.reset_rate_limits,reset_rate_limits(tenant_id: str) -> None,,,billing | decisions,pure,no,4,Reset rate limits for a tenant (for testing).
policies,L5,protection_provider,get_protection_provider,get_protection_provider() -> AbuseProtectionProvider,,MockAbuseProtectionProvider,billing | decisions,pure,no,11,Get the abuse protection provider instance.
policies,L5,protection_provider,set_protection_provider,set_protection_provider(provider: AbuseProtectionProvider) -> None,,,billing | decisions,pure,no,8,Set the abuse protection provider instance.
policies,L5,recovery_evaluation_engine,FailureContext.__post_init__,__post_init__(),?:recovery_evaluator | L4:recovery_decisions | L2:workers | ?:test_m10_recovery_enhanced,now,decisions | recovery_decisions | recovery_evaluator | recovery_matcher | recovery_rule_engine,pure,no,5,
policies,L5,recovery_evaluation_engine,RecoveryDecision.to_dict,"to_dict() -> Dict[str, Any]",?:recovery_evaluator | L4:recovery_decisions | L2:workers | ?:test_m10_recovery_enhanced,,decisions | recovery_decisions | recovery_evaluator | recovery_matcher | recovery_rule_engine,pure,no,13,
policies,L5,recovery_evaluation_engine,RecoveryEvaluationEngine.__init__,__init__(),?:recovery_evaluator | L4:recovery_decisions | L2:workers | ?:test_m10_recovery_enhanced,RecoveryMatcher,decisions | recovery_decisions | recovery_evaluator | recovery_matcher | recovery_rule_engine,pure,no,3,Initialize the evaluation engine.
policies,L5,recovery_evaluation_engine,RecoveryEvaluationEngine.emit_decision_record,"emit_decision_record(decision: RecoveryDecision, evaluated: bool, triggered: bool) -> None",?:recovery_evaluator | L4:recovery_decisions | L2:workers | ?:test_m10_recovery_enhanced,emit_recovery_decision,decisions | recovery_decisions | recovery_evaluator | recovery_matcher | recovery_rule_engine,pure,no,31,Emit recovery decision record (L4 domain responsibility).
policies,L5,recovery_evaluation_engine,RecoveryEvaluationEngine.evaluate,evaluate(context: FailureContext) -> RecoveryDecision,?:recovery_evaluator | L4:recovery_decisions | L2:workers | ?:test_m10_recovery_enhanced,RecoveryDecision | combine_confidences | evaluate_rules | get | info | isoformat | should_auto_execute | should_select_action | suggest | to_dict,decisions | recovery_decisions | recovery_evaluator | recovery_matcher | recovery_rule_engine,pure,no,84,Evaluate failure context and produce recovery decision.
policies,L5,recovery_evaluation_engine,evaluate_and_execute,"async evaluate_and_execute(failure_match_id: str, error_code: str, error_message: str, **kwargs) -> 'EvaluationOutcome'",?:recovery_evaluator | L4:recovery_decisions | L2:workers | ?:test_m10_recovery_enhanced,FailureContext | FailureEvent | RecoveryEvaluationEngine | RecoveryExecutor | emit_decision_record | evaluate | execute_decision | get,decisions | recovery_decisions | recovery_evaluator | recovery_matcher | recovery_rule_engine,pure,yes,74,Full entry point: evaluate failure and execute decision.
policies,L5,recovery_evaluation_engine,evaluate_recovery,"evaluate_recovery(failure_match_id: str, error_code: str, error_message: str, **kwargs) -> RecoveryDecision",?:recovery_evaluator | L4:recovery_decisions | L2:workers | ?:test_m10_recovery_enhanced,FailureContext | RecoveryEvaluationEngine | evaluate | get,decisions | recovery_decisions | recovery_evaluator | recovery_matcher | recovery_rule_engine,pure,no,36,Convenience function to evaluate a failure and get a decision.
policies,L5,runtime_command,execute_query,"execute_query(query_type: str, params: Optional[Dict[str, Any]]) -> QueryResult",?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,QueryResult | get | query_allowed_skills | query_execution_history | query_last_step_outcome | query_remaining_budget | query_skills_for_goal,,pure,no,34,Execute a runtime query.
policies,L5,runtime_command,get_all_skill_descriptors,"get_all_skill_descriptors() -> Dict[str, Dict[str, Any]]",?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,get,,pure,no,21,Get descriptors for all skills.
policies,L5,runtime_command,get_capabilities,"get_capabilities(agent_id: Optional[str], tenant_id: Optional[str]) -> CapabilitiesInfo",?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,CapabilitiesInfo | get | items,,pure,no,42,Get capabilities for an agent/tenant.
policies,L5,runtime_command,get_resource_contract,get_resource_contract(resource_id: str) -> ResourceContractInfo,?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,ResourceContractInfo,,pure,no,18,Get resource contract information.
policies,L5,runtime_command,get_skill_info,get_skill_info(skill_id: str) -> Optional[SkillInfo],?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,SkillInfo | get,,pure,no,27,Get domain information about a skill.
policies,L5,runtime_command,get_supported_query_types,get_supported_query_types() -> List[str],?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,copy,,pure,no,10,Get list of supported query types.
policies,L5,runtime_command,list_skills,list_skills() -> List[str],?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,keys | list,,pure,no,10,List all available skill IDs.
policies,L5,runtime_command,query_allowed_skills,query_allowed_skills() -> QueryResult,?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,QueryResult | keys | len | list,,pure,no,17,Query list of allowed skills.
policies,L5,runtime_command,query_execution_history,"query_execution_history(history: Optional[List[Dict[str, Any]]]) -> QueryResult",?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,QueryResult,,pure,no,16,Query execution history.
policies,L5,runtime_command,query_last_step_outcome,"query_last_step_outcome(outcome: Optional[Dict[str, Any]]) -> QueryResult",?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,QueryResult,,pure,no,16,Query last step outcome.
policies,L5,runtime_command,query_remaining_budget,"query_remaining_budget(spent_cents: int, total_cents: int) -> QueryResult",?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,QueryResult,,pure,no,24,Query remaining budget.
policies,L5,runtime_command,query_skills_for_goal,query_skills_for_goal(goal: str) -> QueryResult,?:runtime | L3:runtime_adapter | L4:runtime_adapter | L2:runtime | ?:__init__,QueryResult | hash | keys | list | ord | sorted | sum,,pure,no,25,Query skills available for a goal.
policies,L5,sandbox_engine,ExecutionRecord.to_dict,"to_dict() -> Dict[str, Any]",,isoformat,sandbox_executor,pure,no,18,Convert to dictionary.
policies,L5,sandbox_engine,SandboxPolicy.to_dict,"to_dict() -> Dict[str, Any]",,list,sandbox_executor,pure,no,19,Convert to dictionary.
policies,L5,sandbox_engine,SandboxPolicy.to_resource_limits,to_resource_limits() -> ResourceLimits,,ResourceLimits,sandbox_executor,pure,no,10,Convert policy to resource limits.
policies,L5,sandbox_engine,SandboxService.__init__,__init__(),,_setup_default_policies,sandbox_executor,pure,no,6,
policies,L5,sandbox_engine,SandboxService._check_quota,"_check_quota(tenant_id: str, policy: SandboxPolicy) -> bool",,get | len | now | strftime,sandbox_executor,pure,no,22,Check if tenant has quota remaining.
policies,L5,sandbox_engine,SandboxService._get_executor,_get_executor(isolation_level: IsolationLevel) -> SandboxExecutor,,create_sandbox_executor,sandbox_executor,pure,no,5,Get or create an executor for the isolation level.
policies,L5,sandbox_engine,SandboxService._get_policy,_get_policy(policy_id: Optional[str]) -> SandboxPolicy,,,sandbox_executor,pure,no,5,"Get a policy by ID, defaulting to 'standard'."
policies,L5,sandbox_engine,SandboxService._setup_default_policies,_setup_default_policies() -> None,,SandboxPolicy,sandbox_executor,pure,no,61,Set up default sandbox policies.
policies,L5,sandbox_engine,SandboxService._track_execution,_track_execution(tenant_id: str) -> None,,append | int | keys | max | now | str | strftime | zfill,sandbox_executor,pure,no,26,Track an execution for quota purposes.
policies,L5,sandbox_engine,SandboxService.define_policy,"define_policy(policy_id: str, name: str, **kwargs) -> SandboxPolicy",,SandboxPolicy | info,sandbox_executor,pure,no,25,Define a new sandbox policy.
policies,L5,sandbox_engine,SandboxService.execute,async execute(request: ExecutionRequest) -> ExecutionResult,,ExecutionRecord | ExecutionResult | _check_quota | _get_executor | _get_policy | _track_execution | append | cleanup | encode | execute | float | hexdigest | info | int | len,sandbox_executor,pure,yes,106,Execute code in a sandbox.
policies,L5,sandbox_engine,SandboxService.get_execution_records,"get_execution_records(tenant_id: Optional[str], user_id: Optional[str], run_id: Optional[str], status: Optional[SandboxStatus], limit: int) -> List[ExecutionRecord]",,,sandbox_executor,pure,no,33,Get execution records with optional filtering.
policies,L5,sandbox_engine,SandboxService.get_execution_stats,"get_execution_stats(tenant_id: Optional[str]) -> Dict[str, Any]",,get | len,sandbox_executor,pure,no,55,Get execution statistics.
policies,L5,sandbox_engine,SandboxService.get_policy,get_policy(policy_id: str) -> Optional[SandboxPolicy],,get,sandbox_executor,pure,no,3,Get a policy by ID.
policies,L5,sandbox_engine,SandboxService.list_policies,"list_policies() -> Dict[str, SandboxPolicy]",,dict,sandbox_executor,pure,no,3,List all defined policies.
policies,L5,snapshot_engine,PolicySnapshotData.compute_hash,compute_hash(content: str) -> str,,encode | hexdigest | sha256,,pure,no,3,Compute SHA256 hash of content.
policies,L5,snapshot_engine,PolicySnapshotData.get_policies,"get_policies() -> list[dict[str, Any]]",,loads,,pure,no,3,Deserialize policies from JSON.
policies,L5,snapshot_engine,PolicySnapshotData.get_thresholds,"get_thresholds() -> dict[str, Any]",,loads,,pure,no,3,Deserialize thresholds from JSON.
policies,L5,snapshot_engine,PolicySnapshotData.to_dict,"to_dict() -> dict[str, Any]",,isoformat | verify_integrity,,pure,no,20,Convert to dictionary.
policies,L5,snapshot_engine,PolicySnapshotData.verify_integrity,verify_integrity() -> bool,,compute_hash,,pure,no,5,Verify content hash matches stored data.
policies,L5,snapshot_engine,PolicySnapshotData.verify_threshold_integrity,verify_threshold_integrity() -> bool,,compute_hash,,pure,no,4,Verify threshold hash matches threshold data.
policies,L5,snapshot_engine,PolicySnapshotError.__init__,"__init__(message: str, snapshot_id: Optional[str], violation_type: Optional[ImmutabilityViolation])",,__init__ | super,,pure,no,10,
policies,L5,snapshot_engine,PolicySnapshotError.to_dict,"to_dict() -> dict[str, Any]",,,,pure,no,9,Convert to dictionary.
policies,L5,snapshot_engine,PolicySnapshotRegistry.__init__,__init__(),,,,pure,no,5,Initialize the registry.
policies,L5,snapshot_engine,PolicySnapshotRegistry._get_next_version,_get_next_version(tenant_id: str) -> int,,get,,pure,no,6,Get the next version number for a tenant.
policies,L5,snapshot_engine,PolicySnapshotRegistry._supersede_active,"_supersede_active(tenant_id: str, new_snapshot_id: str) -> None",,get_active | now,,pure,no,11,Supersede the current active snapshot for a tenant.
policies,L5,snapshot_engine,PolicySnapshotRegistry.archive,archive(snapshot_id: str) -> PolicySnapshotData,,PolicySnapshotError | get,,pure,no,23,Archive a snapshot.
policies,L5,snapshot_engine,PolicySnapshotRegistry.attempt_modify,"attempt_modify(snapshot_id: str, **_kwargs) -> None",,PolicySnapshotError | get,,pure,no,25,Attempt to modify a snapshot (ALWAYS FAILS).
policies,L5,snapshot_engine,PolicySnapshotRegistry.clear_tenant,clear_tenant(tenant_id: str) -> int,,get | list | remove,,pure,no,13,Clear all archived snapshots for a tenant.
policies,L5,snapshot_engine,PolicySnapshotRegistry.create,"create(tenant_id: str, policies: list[dict[str, Any]], thresholds: dict[str, Any], policy_version: Optional[str], description: Optional[str], snapshot_id: Optional[str]) -> PolicySnapshotData",,PolicySnapshotData | _get_next_version | _supersede_active | append | compute_hash | dumps | len | str | uuid4,,pure,no,72,Create an immutable policy snapshot.
policies,L5,snapshot_engine,PolicySnapshotRegistry.delete,delete(snapshot_id: str) -> bool,,PolicySnapshotError | get | remove,,pure,no,28,Delete a snapshot.
policies,L5,snapshot_engine,PolicySnapshotRegistry.get,get(snapshot_id: str) -> Optional[PolicySnapshotData],,get,,pure,no,3,Get a snapshot by ID.
policies,L5,snapshot_engine,PolicySnapshotRegistry.get_active,get_active(tenant_id: str) -> Optional[PolicySnapshotData],,get | reversed,,pure,no,8,Get the current active snapshot for a tenant.
policies,L5,snapshot_engine,PolicySnapshotRegistry.get_by_version,"get_by_version(tenant_id: str, version: int) -> Optional[PolicySnapshotData]",,values,,pure,no,10,Get a snapshot by tenant and version number.
policies,L5,snapshot_engine,PolicySnapshotRegistry.get_history,"get_history(tenant_id: str, limit: int) -> List[PolicySnapshotData]",,sort | values,,pure,no,12,Get version history for a tenant.
policies,L5,snapshot_engine,PolicySnapshotRegistry.get_statistics,get_statistics(tenant_id: Optional[str]) -> SnapshotRegistryStats,,SnapshotRegistryStats | add | len | set | values | verify_integrity,,db_write,no,30,Get registry statistics.
policies,L5,snapshot_engine,PolicySnapshotRegistry.list,"list(tenant_id: Optional[str], status: Optional[SnapshotStatus], limit: int, offset: int) -> List[PolicySnapshotData]",,list | sort | values,,pure,no,20,List snapshots with optional filters.
policies,L5,snapshot_engine,PolicySnapshotRegistry.reset,reset() -> None,,clear,,pure,no,5,Reset all state (for testing).
policies,L5,snapshot_engine,PolicySnapshotRegistry.verify,"verify(snapshot_id: str) -> dict[str, Any]",,PolicySnapshotError | get | verify_integrity | verify_threshold_integrity,,pure,no,27,Verify snapshot integrity.
policies,L5,snapshot_engine,SnapshotRegistryStats.to_dict,"to_dict() -> dict[str, Any]",,,,pure,no,11,Convert to dictionary.
policies,L5,snapshot_engine,_reset_snapshot_registry,_reset_snapshot_registry() -> None,,reset,,pure,no,6,Reset the singleton (for testing).
policies,L5,snapshot_engine,create_policy_snapshot,"create_policy_snapshot(tenant_id: str, policies: list[dict[str, Any]], thresholds: dict[str, Any], policy_version: Optional[str], description: Optional[str]) -> PolicySnapshotData",,create | get_snapshot_registry,,pure,no,16,Create a new immutable policy snapshot.
policies,L5,snapshot_engine,get_active_snapshot,get_active_snapshot(tenant_id: str) -> Optional[PolicySnapshotData],,get_active | get_snapshot_registry,,pure,no,4,Get the active policy snapshot for a tenant.
policies,L5,snapshot_engine,get_policy_snapshot,get_policy_snapshot(snapshot_id: str) -> Optional[PolicySnapshotData],,get | get_snapshot_registry,,pure,no,4,Get a policy snapshot by ID.
policies,L5,snapshot_engine,get_snapshot_history,"get_snapshot_history(tenant_id: str, limit: int) -> List[PolicySnapshotData]",,get_history | get_snapshot_registry,,pure,no,7,Get snapshot version history for a tenant.
policies,L5,snapshot_engine,get_snapshot_registry,get_snapshot_registry() -> PolicySnapshotRegistry,,PolicySnapshotRegistry,,pure,no,6,Get the singleton registry instance.
policies,L5,snapshot_engine,verify_snapshot,"verify_snapshot(snapshot_id: str) -> dict[str, Any]",,get_snapshot_registry | verify,,pure,no,4,Verify snapshot integrity.
policies,L5,state,BillingState.allows_usage,allows_usage() -> bool,?:gateway_config | ?:console_auth | ?:gateway_middleware | ?:rbac_middleware | ?:tier_gating | ?:tenant_auth | ?:killswitch | ?:incidents | ?:lifecycle_gate | ?:billing_gate,,,pure,no,8,Check if this billing state allows product usage.
policies,L5,state,BillingState.default,default() -> 'BillingState',?:gateway_config | ?:console_auth | ?:gateway_middleware | ?:rbac_middleware | ?:tier_gating | ?:tenant_auth | ?:killswitch | ?:incidents | ?:lifecycle_gate | ?:billing_gate,,,pure,no,7,Return the default state for tenants completing onboarding.
policies,L5,state,BillingState.from_string,from_string(value: str) -> 'BillingState',?:gateway_config | ?:console_auth | ?:gateway_middleware | ?:rbac_middleware | ?:tier_gating | ?:tenant_auth | ?:killswitch | ?:incidents | ?:lifecycle_gate | ?:billing_gate,ValueError | lower | strip,,pure,no,12,Parse state from string (case-insensitive).
policies,L5,state,BillingState.is_in_good_standing,is_in_good_standing() -> bool,?:gateway_config | ?:console_auth | ?:gateway_middleware | ?:rbac_middleware | ?:tier_gating | ?:tenant_auth | ?:killswitch | ?:incidents | ?:lifecycle_gate | ?:billing_gate,,,pure,no,8,Check if tenant is in good commercial standing.
policies,L5,tokenizer,Token.__repr__,__repr__() -> str,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,,,pure,no,2,
policies,L5,tokenizer,Token.is_action,is_action() -> bool,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,,,pure,no,3,Check if token is an action.
policies,L5,tokenizer,Token.is_category,is_category() -> bool,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,,,pure,no,9,Check if token is a category.
policies,L5,tokenizer,Tokenizer.__init__,__init__(source: str),?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,,,pure,no,6,
policies,L5,tokenizer,Tokenizer.__iter__,__iter__() -> Iterator[Token],?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,iter | tokenize,,pure,no,5,Iterate over tokens.
policies,L5,tokenizer,Tokenizer.advance,advance() -> str,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,,,pure,no,10,Advance to next character.
policies,L5,tokenizer,Tokenizer.current_char,current_char() -> Optional[str],?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,len,,pure,no,5,Get current character or None if at end.
policies,L5,tokenizer,Tokenizer.peek,peek(offset: int) -> Optional[str],?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,len,,pure,no,6,Peek ahead by offset characters.
policies,L5,tokenizer,Tokenizer.read_identifier,read_identifier() -> Token,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,Token | advance | get | isalnum,,pure,no,12,Read an identifier or keyword.
policies,L5,tokenizer,Tokenizer.read_number,read_number() -> Token,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,Token | advance | isdigit,,pure,no,10,Read a number literal.
policies,L5,tokenizer,Tokenizer.read_operator,read_operator() -> Token,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,Token | TokenizerError | advance | peek,,pure,no,38,Read an operator.
policies,L5,tokenizer,Tokenizer.read_string,read_string() -> Token,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,Token | TokenizerError | advance | get,,pure,no,21,Read a string literal.
policies,L5,tokenizer,Tokenizer.skip_comment,skip_comment() -> None,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,advance,,pure,no,5,Skip single-line comments starting with #.
policies,L5,tokenizer,Tokenizer.skip_whitespace,skip_whitespace() -> None,?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,advance,,pure,no,4,Skip whitespace characters (except newlines for statement separation).
policies,L5,tokenizer,Tokenizer.tokenize,tokenize() -> List[Token],?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,Token | TokenizerError | advance | append | isalpha | isdigit | read_identifier | read_number | read_operator | read_string | skip_comment | skip_whitespace,,pure,no,50,Tokenize the source code.
policies,L5,tokenizer,TokenizerError.__init__,"__init__(message: str, line: int, column: int)",?:__init__ | ?:parser | L5:compiler_parser | ?:test_m20_parser,__init__ | super,,pure,no,5,
policies,L5,validator,PolicyValidator.__init__,"__init__(allowed_metrics: set[str] | None, custom_rules: list[Callable[[PolicyAST], list[ValidationIssue]]] | None) -> None",?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,,__future__ | ast,pure,no,15,Initialize validator.
policies,L5,validator,PolicyValidator._check_warnings,_check_warnings(policy: PolicyAST) -> list[ValidationIssue],?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,ValidationIssue | append | is_block_action | is_require_approval_action,__future__ | ast,pure,no,28,Check for non-blocking issues (warnings).
policies,L5,validator,PolicyValidator._extract_metrics,_extract_metrics(condition: Condition) -> list[str],?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,_extract_metrics | append | extend | is_exists_predicate | is_logical_condition | is_predicate,__future__ | ast,pure,no,13,Extract all metric names from a condition.
policies,L5,validator,PolicyValidator._validate_metrics,_validate_metrics(policy: PolicyAST) -> list[ValidationIssue],?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,ValidationIssue | _extract_metrics | append | enumerate,__future__ | ast,pure,no,23,Validate that all metrics are in the allowed set (if provided).
policies,L5,validator,PolicyValidator._validate_mode_enforcement,_validate_mode_enforcement(policy: PolicyAST) -> list[ValidationIssue],?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,ValidationIssue | append | enumerate | is_block_action | is_require_approval_action,__future__ | ast,pure,no,36,Validate that actions match the policy mode.
policies,L5,validator,PolicyValidator._validate_structure,_validate_structure(policy: PolicyAST) -> list[ValidationIssue],?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,ValidationIssue | append | enumerate,__future__ | ast,pure,no,30,Validate structural integrity (defense in depth).
policies,L5,validator,PolicyValidator.validate,validate(policy: PolicyAST) -> ValidationResult,?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,ValidationResult | _check_warnings | _validate_metrics | _validate_mode_enforcement | _validate_structure | extend | rule | tuple,__future__ | ast,pure,no,20,Validate a policy AST.
policies,L5,validator,ValidationIssue.__str__,__str__() -> str,?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,,__future__ | ast,pure,no,3,
policies,L5,validator,ValidationResult.__bool__,__bool__() -> bool,?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,,__future__ | ast,pure,no,3,ValidationResult is truthy if valid.
policies,L5,validator,ValidationResult.__post_init__,__post_init__() -> None,?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,__setattr__ | any,__future__ | ast,pure,no,4,
policies,L5,validator,ValidationResult.errors,errors() -> list[ValidationIssue],?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,,__future__ | ast,pure,no,3,Return only ERROR severity issues.
policies,L5,validator,ValidationResult.warnings,warnings() -> list[ValidationIssue],?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,,__future__ | ast,pure,no,3,Return only WARNING severity issues.
policies,L5,validator,is_valid,"is_valid(policy: PolicyAST, allowed_metrics: set[str] | None) -> bool",?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,validate,__future__ | ast,pure,no,16,Quick check if a policy is valid.
policies,L5,validator,validate,"validate(policy: PolicyAST, allowed_metrics: set[str] | None) -> ValidationResult",?:__init__ | ?:service | ?:test_validator | ?:test_roundtrip,PolicyValidator | validate,__future__ | ast,pure,no,33,Validate a policy AST.
policies,L5,visitors,BaseVisitor.visit_action_block,visit_action_block(node: ActionBlockNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,accept,grammar | nodes,pure,no,3,
policies,L5,visitors,BaseVisitor.visit_attr_access,visit_attr_access(node: AttrAccessNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,accept,grammar | nodes,pure,no,3,
policies,L5,visitors,BaseVisitor.visit_binary_op,visit_binary_op(node: BinaryOpNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,accept,grammar | nodes,pure,no,5,
policies,L5,visitors,BaseVisitor.visit_condition_block,visit_condition_block(node: ConditionBlockNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,accept,grammar | nodes,pure,no,5,
policies,L5,visitors,BaseVisitor.visit_func_call,visit_func_call(node: FuncCallNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,accept,grammar | nodes,pure,no,5,
policies,L5,visitors,BaseVisitor.visit_ident,visit_ident(node: IdentNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,2,
policies,L5,visitors,BaseVisitor.visit_import,visit_import(node: ImportNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,2,
policies,L5,visitors,BaseVisitor.visit_literal,visit_literal(node: LiteralNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,2,
policies,L5,visitors,BaseVisitor.visit_policy_decl,visit_policy_decl(node: PolicyDeclNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,accept,grammar | nodes,pure,no,3,
policies,L5,visitors,BaseVisitor.visit_priority,visit_priority(node: PriorityNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,2,
policies,L5,visitors,BaseVisitor.visit_program,visit_program(node: ProgramNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,accept,grammar | nodes,pure,no,3,
policies,L5,visitors,BaseVisitor.visit_route_target,visit_route_target(node: RouteTargetNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,2,
policies,L5,visitors,BaseVisitor.visit_rule_decl,visit_rule_decl(node: RuleDeclNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,accept,grammar | nodes,pure,no,3,
policies,L5,visitors,BaseVisitor.visit_rule_ref,visit_rule_ref(node: RuleRefNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,2,
policies,L5,visitors,BaseVisitor.visit_unary_op,visit_unary_op(node: UnaryOpNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,accept,grammar | nodes,pure,no,3,
policies,L5,visitors,CategoryCollector.__init__,__init__(),?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,3,
policies,L5,visitors,CategoryCollector.get_categories,"get_categories() -> Dict[PolicyCategory, List[str]]",?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,2,
policies,L5,visitors,CategoryCollector.visit_policy_decl,visit_policy_decl(node: PolicyDeclNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,append | super | visit_policy_decl,grammar | nodes,pure,no,4,
policies,L5,visitors,CategoryCollector.visit_rule_decl,visit_rule_decl(node: RuleDeclNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,append | super | visit_rule_decl,grammar | nodes,pure,no,7,
policies,L5,visitors,PrintVisitor.__init__,__init__(),?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,3,
policies,L5,visitors,PrintVisitor._emit,_emit(text: str) -> None,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,append,grammar | nodes,pure,no,2,
policies,L5,visitors,PrintVisitor.get_output,get_output() -> str,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,join,grammar | nodes,pure,no,2,
policies,L5,visitors,PrintVisitor.visit_action_block,visit_action_block(node: ActionBlockNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit,grammar | nodes,pure,no,3,
policies,L5,visitors,PrintVisitor.visit_attr_access,visit_attr_access(node: AttrAccessNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit | accept,grammar | nodes,pure,no,6,
policies,L5,visitors,PrintVisitor.visit_binary_op,visit_binary_op(node: BinaryOpNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit | accept,grammar | nodes,pure,no,8,
policies,L5,visitors,PrintVisitor.visit_condition_block,visit_condition_block(node: ConditionBlockNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit | accept,grammar | nodes,pure,no,11,
policies,L5,visitors,PrintVisitor.visit_func_call,visit_func_call(node: FuncCallNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit | accept,grammar | nodes,pure,no,10,
policies,L5,visitors,PrintVisitor.visit_ident,visit_ident(node: IdentNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit,grammar | nodes,pure,no,2,
policies,L5,visitors,PrintVisitor.visit_import,visit_import(node: ImportNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit,grammar | nodes,pure,no,2,
policies,L5,visitors,PrintVisitor.visit_literal,visit_literal(node: LiteralNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit,grammar | nodes,pure,no,2,
policies,L5,visitors,PrintVisitor.visit_policy_decl,visit_policy_decl(node: PolicyDeclNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit | accept,grammar | nodes,pure,no,6,
policies,L5,visitors,PrintVisitor.visit_priority,visit_priority(node: PriorityNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit,grammar | nodes,pure,no,2,
policies,L5,visitors,PrintVisitor.visit_program,visit_program(node: ProgramNode) -> str,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit | accept | get_output,grammar | nodes,pure,no,7,
policies,L5,visitors,PrintVisitor.visit_route_target,visit_route_target(node: RouteTargetNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit,grammar | nodes,pure,no,2,
policies,L5,visitors,PrintVisitor.visit_rule_decl,visit_rule_decl(node: RuleDeclNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit | accept,grammar | nodes,pure,no,6,
policies,L5,visitors,PrintVisitor.visit_rule_ref,visit_rule_ref(node: RuleRefNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit,grammar | nodes,pure,no,2,
policies,L5,visitors,PrintVisitor.visit_unary_op,visit_unary_op(node: UnaryOpNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,_emit | accept,grammar | nodes,pure,no,6,
policies,L5,visitors,RuleExtractor.__init__,__init__(),?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,4,
policies,L5,visitors,RuleExtractor.get_rules,"get_rules() -> Dict[str, Dict[str, Any]]",?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,,grammar | nodes,pure,no,2,
policies,L5,visitors,RuleExtractor.visit_condition_block,visit_condition_block(node: ConditionBlockNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,append | super | visit_condition_block,grammar | nodes,pure,no,11,
policies,L5,visitors,RuleExtractor.visit_policy_decl,visit_policy_decl(node: PolicyDeclNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,super | visit_policy_decl,grammar | nodes,pure,no,14,
policies,L5,visitors,RuleExtractor.visit_rule_decl,visit_rule_decl(node: RuleDeclNode) -> Any,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_parser,append | super | visit_rule_decl,grammar | nodes,pure,no,16,
policies,L5,worker_execution_command,calculate_cost_cents,"calculate_cost_cents(model: str, input_tokens: int, output_tokens: int) -> int",L3:workers_adapter | ?:__init__,calculate_llm_cost_cents,brand | runner | worker,pure,no,20,Calculate LLM cost in cents.
policies,L5,worker_execution_command,convert_brand_request,convert_brand_request(brand_req) -> Any,L3:workers_adapter | ?:__init__,AudienceSegment | BrandSchema | ForbiddenClaim | ToneLevel | ToneRule | VisualIdentity | append | get_brand_schema_types,brand | runner | worker,pure,no,75,Convert API brand request to BrandSchema.
policies,L5,worker_execution_command,execute_worker,"async execute_worker(task: str, brand: Optional[Any], budget: Optional[int], strict_mode: bool, depth: int, run_id: Optional[str], event_bus: Optional[Any]) -> WorkerExecutionResult",L3:workers_adapter | ?:__init__,BusinessBuilderWorker | WorkerExecutionResult | getattr | run,brand | runner | worker,pure,yes,67,Execute Business Builder Worker.
policies,L5,worker_execution_command,get_brand_schema_types,get_brand_schema_types(),L3:workers_adapter | ?:__init__,,brand | runner | worker,pure,no,26,Get brand schema types from L5.
policies,L5,worker_execution_command,replay_execution,"async replay_execution(replay_token: str, run_id: str) -> ReplayResult",L3:workers_adapter | ?:__init__,ReplayResult | replay,brand | runner | worker,pure,yes,37,Replay a previous execution.
policies,L6,arbitrator,PolicyArbitrator.__init__,__init__(session: Optional[Session]),L7:policy_precedence | ?:test_control_action_enhancements,,db | policy_precedence | sqlmodel,pure,no,8,Initialize policy arbitrator.
policies,L6,arbitrator,PolicyArbitrator._get_precedence_map,"_get_precedence_map(session: Session, policy_ids: list[str], tenant_id: str) -> dict[str, PolicyPrecedence]",L7:policy_precedence | ?:test_control_action_enhancements,all | exec | in_ | select | where,db | policy_precedence | sqlmodel,pure,no,13,Get precedence map from database.
policies,L6,arbitrator,PolicyArbitrator._load_precedence_map,"_load_precedence_map(policy_ids: list[str], tenant_id: str) -> dict[str, PolicyPrecedence]",L7:policy_precedence | ?:test_control_action_enhancements,Session | _get_precedence_map,db | policy_precedence | sqlmodel,pure,no,11,Load precedence for all policies.
policies,L6,arbitrator,PolicyArbitrator._resolve_action_conflict,"_resolve_action_conflict(actions: list[PolicyAction], strategy: ConflictStrategy, precedence_map: dict[str, PolicyPrecedence]) -> tuple[str, int]",L7:policy_precedence | ?:test_control_action_enhancements,PolicyPrecedence | get | len | max | sorted,db | policy_precedence | sqlmodel,pure,no,38,Resolve conflicting actions.
policies,L6,arbitrator,PolicyArbitrator._resolve_limit_conflict,"_resolve_limit_conflict(limits: list[PolicyLimit], strategy: ConflictStrategy, precedence_map: dict[str, PolicyPrecedence]) -> tuple[Optional[float], int]",L7:policy_precedence | ?:test_control_action_enhancements,PolicyPrecedence | get | len | min | sorted,db | policy_precedence | sqlmodel,pure,no,38,Resolve conflicting limits.
policies,L6,arbitrator,PolicyArbitrator.arbitrate,"arbitrate(policy_ids: list[str], tenant_id: str, arb_input: Optional[ArbitrationInput]) -> ArbitrationResult",L7:policy_precedence | ?:test_control_action_enhancements,ArbitrationResult | ConflictStrategy | PolicyPrecedence | _load_precedence_map | _resolve_action_conflict | _resolve_limit_conflict | dumps | encode | get | hexdigest | info | int | len | now | sha256,db | policy_precedence | sqlmodel,pure,no,114,Arbitrate between multiple policies.
policies,L6,arbitrator,get_policy_arbitrator,get_policy_arbitrator() -> PolicyArbitrator,L7:policy_precedence | ?:test_control_action_enhancements,PolicyArbitrator,db | policy_precedence | sqlmodel,pure,no,6,Get or create PolicyArbitrator singleton.
policies,L6,optimizer_conflict_resolver,ConflictResolver.__init__,__init__(),,,grammar | ir_nodes,pure,no,3,
policies,L6,optimizer_conflict_resolver,ConflictResolver._detect_action_conflicts,_detect_action_conflicts(module: IRModule) -> None,,PolicyConflict | _get_condition_signature | append | get | isinstance | items | keys | len | list | set | values,grammar | ir_nodes,pure,no,41,Detect conflicting actions for same conditions.
policies,L6,optimizer_conflict_resolver,ConflictResolver._detect_category_conflicts,_detect_category_conflicts(module: IRModule) -> None,,PolicyConflict | _might_override | append | get | items,grammar | ir_nodes,pure,no,35,Detect cross-category interactions that may conflict.
policies,L6,optimizer_conflict_resolver,ConflictResolver._detect_circular_dependencies,_detect_circular_dependencies(module: IRModule) -> None,,PolicyConflict | add | append | get | has_cycle | insert | isinstance | items | join | remove | set | values,grammar | ir_nodes,db_write,no,47,Detect circular route dependencies.
policies,L6,optimizer_conflict_resolver,ConflictResolver._detect_priority_conflicts,_detect_priority_conflicts(module: IRModule) -> None,,PolicyConflict | append | items | len,grammar | ir_nodes,pure,no,23,Detect policies with same priority in same category.
policies,L6,optimizer_conflict_resolver,ConflictResolver._get_actions,_get_actions(func: IRFunction) -> Set[ActionType],,add | isinstance | set | values,grammar | ir_nodes,db_write,no,8,Get all actions in a function.
policies,L6,optimizer_conflict_resolver,ConflictResolver._get_condition_signature,_get_condition_signature(block: IRBlock) -> str,,append | join | type,grammar | ir_nodes,pure,no,7,Get a signature for the condition pattern in a block.
policies,L6,optimizer_conflict_resolver,ConflictResolver._might_override,"_might_override(lower: IRFunction, higher: IRFunction) -> bool",,_get_actions,grammar | ir_nodes,pure,no,10,Check if lower-priority policy might override higher one.
policies,L6,optimizer_conflict_resolver,ConflictResolver._resolve_action_conflict,"_resolve_action_conflict(module: IRModule, conflict: PolicyConflict) -> None",,_get_actions | append | get | get_action_precedence,grammar | ir_nodes,pure,no,22,Resolve action conflict using action precedence.
policies,L6,optimizer_conflict_resolver,ConflictResolver._resolve_category_conflict,"_resolve_category_conflict(module: IRModule, conflict: PolicyConflict) -> None",,append | get | get_category_priority,grammar | ir_nodes,pure,no,21,Resolve category conflict using category precedence.
policies,L6,optimizer_conflict_resolver,ConflictResolver._resolve_circular_conflict,"_resolve_circular_conflict(module: IRModule, conflict: PolicyConflict) -> None",,append | float | get | len,grammar | ir_nodes,pure,no,22,Resolve circular routing by breaking the cycle.
policies,L6,optimizer_conflict_resolver,ConflictResolver._resolve_conflict,"_resolve_conflict(module: IRModule, conflict: PolicyConflict) -> None",,_resolve_action_conflict | _resolve_category_conflict | _resolve_circular_conflict | _resolve_priority_conflict,grammar | ir_nodes,pure,no,14,Resolve a single conflict.
policies,L6,optimizer_conflict_resolver,ConflictResolver._resolve_priority_conflict,"_resolve_priority_conflict(module: IRModule, conflict: PolicyConflict) -> None",,append | enumerate | get | sorted,grammar | ir_nodes,pure,no,12,Resolve priority conflict by adjusting priorities.
policies,L6,optimizer_conflict_resolver,ConflictResolver.resolve,"resolve(module: IRModule) -> Tuple[IRModule, List[PolicyConflict]]",,_detect_action_conflicts | _detect_category_conflicts | _detect_circular_dependencies | _detect_priority_conflicts | _resolve_conflict,grammar | ir_nodes,pure,no,24,Detect and resolve conflicts in module.
policies,L6,optimizer_conflict_resolver,PolicyConflict.__str__,__str__() -> str,,,grammar | ir_nodes,pure,no,3,
policies,L6,policy_engine_driver,PolicyEngineDriver.__init__,__init__(db_url: str),L5:engine,,engine | sqlalchemy,pure,no,4,Initialize with database URL.
policies,L6,policy_engine_driver,PolicyEngineDriver._get_engine,_get_engine() -> Engine,L5:engine,create_engine,engine | sqlalchemy,pure,no,5,Lazy-load engine.
policies,L6,policy_engine_driver,PolicyEngineDriver.activate_version,"activate_version(conn: Connection, version: str) -> None",L5:engine,execute | text,engine | sqlalchemy,pure,no,22,Activate a specific version.
policies,L6,policy_engine_driver,PolicyEngineDriver.cap_temporal_events,"cap_temporal_events(conn: Connection, max_per_policy: int) -> int",L5:engine,execute | text,engine | sqlalchemy,pure,no,30,Cap events per policy to max limit.
policies,L6,policy_engine_driver,PolicyEngineDriver.compact_temporal_events,"compact_temporal_events(conn: Connection, compact_hours: int, retention_hours: int) -> int",L5:engine,execute | text,engine | sqlalchemy,pure,no,56,Compact old events into hourly aggregates.
policies,L6,policy_engine_driver,PolicyEngineDriver.deactivate_all_versions,deactivate_all_versions(conn: Connection) -> None,L5:engine,execute | text,engine | sqlalchemy,pure,no,3,Deactivate all policy versions.
policies,L6,policy_engine_driver,PolicyEngineDriver.delete_old_temporal_events,"delete_old_temporal_events(conn: Connection, retention_hours: int) -> int",L5:engine,execute | text,engine | sqlalchemy,pure,no,23,Delete temporal events older than retention period.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_active_policies_for_integrity,"fetch_active_policies_for_integrity(conn: Connection, table: str, name_col: str) -> List[str]",L5:engine,execute | text,engine | sqlalchemy,pure,no,25,Fetch active policy names for integrity check.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_business_rules,"fetch_business_rules(conn: Connection) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,18,Fetch all active business rules.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_conflicts,"fetch_conflicts(conn: Connection, include_resolved: bool, severity_min: Optional[float]) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,28,Fetch policy conflicts.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_current_active_version,"fetch_current_active_version(conn: Connection) -> Optional[Dict[str, Any]]",L5:engine,dict | execute | first | text,engine | sqlalchemy,pure,no,24,Fetch the currently active policy version.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_dependencies,"fetch_dependencies(conn: Connection) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,20,Fetch all policy dependencies.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_dependency_edges,"fetch_dependency_edges(conn: Connection, active_only: bool) -> List[Tuple[str, str]]",L5:engine,execute | text,engine | sqlalchemy,pure,no,20,Fetch dependency edges for cycle detection.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_dependency_edges_with_type,"fetch_dependency_edges_with_type(conn: Connection) -> List[Tuple[str, str, str]]",L5:engine,execute | text,engine | sqlalchemy,pure,no,20,Fetch active dependency edges with dependency type for DAG validation.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_ethical_constraints,"fetch_ethical_constraints(conn: Connection) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,20,Fetch all active ethical constraints.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_ethical_constraints_for_integrity,"fetch_ethical_constraints_for_integrity(conn: Connection) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,20,Fetch ethical constraints for integrity check.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_policy_version_by_id,"fetch_policy_version_by_id(conn: Connection, version_id: str) -> Optional[Dict[str, Any]]",L5:engine,dict | execute | first | text,engine | sqlalchemy,pure,no,26,Fetch a policy version by ID.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_policy_version_by_id_or_version,"fetch_policy_version_by_id_or_version(conn: Connection, version_id: str) -> Optional[Tuple]",L5:engine,execute | first | text,engine | sqlalchemy,pure,no,26,Fetch a policy version by ID or version string.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_policy_versions,"fetch_policy_versions(conn: Connection, include_inactive: bool, limit: int) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,28,Fetch policy versions.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_provenance,"fetch_provenance(conn: Connection, policy_version: Optional[str], limit: int) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,31,Fetch policy provenance records.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_risk_ceilings,"fetch_risk_ceilings(conn: Connection) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,19,Fetch all active risk ceilings.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_safety_rules,"fetch_safety_rules(conn: Connection) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,19,Fetch all active safety rules.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_temporal_metric_sum,"fetch_temporal_metric_sum(conn: Connection, policy_id: str, window_seconds: int) -> float",L5:engine,execute | first | float | text,engine | sqlalchemy,pure,no,27,Fetch sum of temporal metric events in window.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_temporal_policies,"fetch_temporal_policies(conn: Connection, metric: Optional[str], include_inactive: bool) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,28,Fetch temporal policies.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_temporal_policies_for_integrity,"fetch_temporal_policies_for_integrity(conn: Connection) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,20,Fetch temporal policies for integrity check.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_temporal_policy_for_utilization,"fetch_temporal_policy_for_utilization(conn: Connection, policy_id: str) -> Optional[Tuple[float, int]]",L5:engine,execute | first | text,engine | sqlalchemy,pure,no,25,Fetch temporal policy max_value and window for utilization check.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_temporal_stats,"fetch_temporal_stats(conn: Connection) -> Dict[str, int]",L5:engine,execute | first | text,engine | sqlalchemy,pure,no,20,Fetch temporal event statistics.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_temporal_storage_stats,fetch_temporal_storage_stats(conn: Connection) -> Optional[Tuple],L5:engine,execute | first | text,engine | sqlalchemy,pure,no,26,Fetch comprehensive temporal storage statistics.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_unresolved_conflicts,"fetch_unresolved_conflicts(conn: Connection) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,20,Fetch unresolved conflicts for integrity check.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_version_for_rollback,"fetch_version_for_rollback(conn: Connection, version: str) -> Optional[Dict[str, Any]]",L5:engine,dict | execute | first | text,engine | sqlalchemy,pure,no,27,Fetch version with snapshots for rollback.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_violation_by_id,"fetch_violation_by_id(conn: Connection, violation_id: str) -> Optional[Dict[str, Any]]",L5:engine,dict | execute | first | text,engine | sqlalchemy,pure,no,28,Fetch a single violation by ID.
policies,L6,policy_engine_driver,PolicyEngineDriver.fetch_violations,"fetch_violations(conn: Connection, violation_type: Optional[str], agent_id: Optional[str], tenant_id: Optional[str], severity_min: Optional[float], since: Optional[datetime], limit: int) -> List[Dict[str, Any]]",L5:engine,dict | execute | text,engine | sqlalchemy,pure,no,55,Fetch violations with optional filters.
policies,L6,policy_engine_driver,PolicyEngineDriver.insert_dependency,"insert_dependency(conn: Connection, source_policy: str, target_policy: str, dependency_type: str, resolution_strategy: str, priority: int, description: str) -> None",L5:engine,execute | text,engine | sqlalchemy,pure,no,39,Insert a policy dependency.
policies,L6,policy_engine_driver,PolicyEngineDriver.insert_evaluation,"insert_evaluation(conn: Connection, evaluation_id: str, action_type: str, agent_id: Optional[str], tenant_id: Optional[str], request_context: str, decision: str, decision_reason: str, modifications: str, evaluation_ms: float, policies_checked: int, rules_matched: int, evaluated_at: datetime) -> None",L5:engine,execute | text,engine | sqlalchemy,pure,no,65,Insert policy evaluation record.
policies,L6,policy_engine_driver,PolicyEngineDriver.insert_policy_version,"insert_policy_version(conn: Connection, version_id: str, version: str, policy_hash: str, created_by: str, description: str) -> None",L5:engine,execute | text,engine | sqlalchemy,pure,no,36,Insert a new policy version.
policies,L6,policy_engine_driver,PolicyEngineDriver.insert_provenance,"insert_provenance(conn: Connection, policy_id: str, policy_type: str, action: str, changed_by: str, policy_version: str, reason: str) -> None",L5:engine,execute | text,engine | sqlalchemy,pure,no,39,Insert provenance record.
policies,L6,policy_engine_driver,PolicyEngineDriver.insert_temporal_policy,"insert_temporal_policy(conn: Connection, name: str, description: Optional[str], temporal_type: str, metric: str, max_value: float, window_seconds: int, breach_action: str, cooldown_on_breach: int) -> None",L5:engine,execute | text,engine | sqlalchemy,pure,no,47,Insert a temporal policy.
policies,L6,policy_engine_driver,PolicyEngineDriver.insert_violation,"insert_violation(conn: Connection, violation_id: str, evaluation_id: str, policy_name: str, violation_type: str, severity: str, description: str, evidence: str, agent_id: Optional[str], tenant_id: Optional[str], action_attempted: str, routed_to_governor: bool, governor_action: Optional[str], detected_at: datetime) -> None",L5:engine,execute | text,engine | sqlalchemy,pure,no,68,Insert policy violation record.
policies,L6,policy_engine_driver,PolicyEngineDriver.mark_version_rolled_back,"mark_version_rolled_back(conn: Connection, by: str) -> None",L5:engine,execute | text,engine | sqlalchemy,pure,no,22,Mark current active version as rolled back.
policies,L6,policy_engine_driver,PolicyEngineDriver.reset_risk_ceiling,"reset_risk_ceiling(conn: Connection, ceiling_id: str) -> int",L5:engine,execute | text,engine | sqlalchemy,pure,no,26,Reset a risk ceiling's current value to 0.
policies,L6,policy_engine_driver,PolicyEngineDriver.resolve_conflict,"resolve_conflict(conn: Connection, conflict_id: str, resolution: str, resolved_by: str) -> int",L5:engine,execute | text,engine | sqlalchemy,pure,no,30,Resolve a policy conflict.
policies,L6,policy_engine_driver,PolicyEngineDriver.update_risk_ceiling,"update_risk_ceiling(conn: Connection, ceiling_id: str, updates: Dict[str, Any]) -> None",L5:engine,append | execute | items | join | text,engine | sqlalchemy,pure,no,32,Update a risk ceiling.
policies,L6,policy_engine_driver,PolicyEngineDriver.update_safety_rule,"update_safety_rule(conn: Connection, rule_id: str, updates: Dict[str, Any]) -> None",L5:engine,append | execute | items | join | text,engine | sqlalchemy,pure,no,38,Update a safety rule.
policies,L6,policy_engine_driver,PolicyEngineDriver.update_violation_acknowledged,"update_violation_acknowledged(conn: Connection, violation_id: str, notes: Optional[str]) -> int",L5:engine,execute | text,engine | sqlalchemy,pure,no,29,Acknowledge a violation.
policies,L6,policy_engine_driver,get_policy_engine_driver,get_policy_engine_driver(db_url: str) -> PolicyEngineDriver,L5:engine,PolicyEngineDriver,engine | sqlalchemy,pure,no,3,Factory function for PolicyEngineDriver.
policies,L6,policy_graph_driver,PolicyGraphDriver.__init__,__init__(session: AsyncSession),L5:policy_graph_engine,,asyncio | sqlalchemy,pure,no,3,Initialize driver with async session.
policies,L6,policy_graph_driver,PolicyGraphDriver.fetch_active_limits,"async fetch_active_limits(tenant_id: str) -> list[dict[str, Any]]",L5:policy_graph_engine,execute | fetchall | str | text,asyncio | sqlalchemy,db_write,yes,33,Fetch all active limits for a tenant.
policies,L6,policy_graph_driver,PolicyGraphDriver.fetch_active_policies,"async fetch_active_policies(tenant_id: str) -> list[dict[str, Any]]",L5:policy_graph_engine,execute | fetchall | str | text,asyncio | sqlalchemy,db_write,yes,37,Fetch all active policies for a tenant.
policies,L6,policy_graph_driver,PolicyGraphDriver.fetch_all_limits,"async fetch_all_limits(tenant_id: str) -> list[dict[str, Any]]",L5:policy_graph_engine,execute | fetchall | str | text,asyncio | sqlalchemy,db_write,yes,34,Fetch all limits for a tenant (including inactive).
policies,L6,policy_graph_driver,PolicyGraphDriver.fetch_all_policies,"async fetch_all_policies(tenant_id: str) -> list[dict[str, Any]]",L5:policy_graph_engine,execute | fetchall | str | text,asyncio | sqlalchemy,db_write,yes,38,Fetch all policies for a tenant (including inactive).
policies,L6,policy_graph_driver,PolicyGraphDriver.fetch_resolved_conflicts,"async fetch_resolved_conflicts() -> set[tuple[str, str]]",L5:policy_graph_engine,execute | fetchall | set | str | text,asyncio | sqlalchemy,db_write,yes,17,Get set of resolved conflict pairs.
policies,L6,policy_graph_driver,get_policy_graph_driver,get_policy_graph_driver(session: AsyncSession) -> PolicyGraphDriver,L5:policy_graph_engine,PolicyGraphDriver,asyncio | sqlalchemy,pure,no,3,Get a PolicyGraphDriver instance.
policies,L6,policy_proposal_read_driver,PolicyProposalReadDriver.__init__,__init__(session: AsyncSession),L6:__init__ | L5:policy_proposal_engine,,asyncio | feedback | policy | sqlalchemy,pure,no,2,
policies,L6,policy_proposal_read_driver,PolicyProposalReadDriver.check_rule_exists,async check_rule_exists(rule_id: str) -> bool,L6:__init__ | L5:policy_proposal_engine,execute | scalar_one_or_none | text,asyncio | feedback | policy | sqlalchemy,db_write,yes,10,Check if a policy rule exists by ID.
policies,L6,policy_proposal_read_driver,PolicyProposalReadDriver.count_versions_for_proposal,async count_versions_for_proposal(proposal_id: UUID) -> int,L6:__init__ | L5:policy_proposal_engine,count | execute | scalar | select | select_from | where,asyncio | feedback | policy | sqlalchemy,db_write,yes,11,Count existing versions for a proposal.
policies,L6,policy_proposal_read_driver,PolicyProposalReadDriver.fetch_proposal_by_id,async fetch_proposal_by_id(proposal_id: UUID) -> Optional[dict],L6:__init__ | L5:policy_proposal_engine,execute | getattr | scalar_one_or_none | select | str | where,asyncio | feedback | policy | sqlalchemy,db_write,yes,30,Fetch a proposal by ID. Returns None if not found.
policies,L6,policy_proposal_read_driver,PolicyProposalReadDriver.fetch_proposal_status,async fetch_proposal_status(proposal_id: UUID) -> Optional[str],L6:__init__ | L5:policy_proposal_engine,execute | scalar_one_or_none | select | where,asyncio | feedback | policy | sqlalchemy,db_write,yes,9,Fetch just the status of a proposal.
policies,L6,policy_proposal_read_driver,PolicyProposalReadDriver.fetch_proposals,"async fetch_proposals(tenant_id: Optional[UUID], status: Optional[str], limit: int) -> list[dict]",L6:__init__ | L5:policy_proposal_engine,all | desc | execute | limit | order_by | scalars | select | str | where,asyncio | feedback | policy | sqlalchemy,db_write,yes,35,Fetch proposals with optional filters.
policies,L6,policy_proposal_read_driver,PolicyProposalReadDriver.fetch_rule_by_id,"async fetch_rule_by_id(rule_id: str, tenant_id: str) -> Optional[dict]",L6:__init__ | L5:policy_proposal_engine,execute | fetchone | text,asyncio | feedback | policy | sqlalchemy,db_write,yes,19,Fetch a policy rule by ID with tenant check.
policies,L6,policy_proposal_read_driver,PolicyProposalReadDriver.fetch_unacknowledged_feedback,"async fetch_unacknowledged_feedback(tenant_id: Optional[UUID], feedback_type: Optional[str]) -> list[dict]",L6:__init__ | L5:policy_proposal_engine,all | execute | scalars | select | str | where,asyncio | feedback | policy | sqlalchemy,db_write,yes,31,Fetch unacknowledged pattern feedback records.
policies,L6,policy_proposal_read_driver,get_policy_proposal_read_driver,get_policy_proposal_read_driver(session: AsyncSession) -> PolicyProposalReadDriver,L6:__init__ | L5:policy_proposal_engine,PolicyProposalReadDriver,asyncio | feedback | policy | sqlalchemy,pure,no,3,Factory function for PolicyProposalReadDriver.
policies,L6,policy_proposal_write_driver,PolicyProposalWriteDriver.__init__,__init__(session: AsyncSession),L6:__init__ | L5:policy_proposal_engine,,asyncio | policy | sqlalchemy | time,pure,no,2,
policies,L6,policy_proposal_write_driver,PolicyProposalWriteDriver.create_policy_rule,"async create_policy_rule(rule_id: str, tenant_id: str, name: str, description: str, rule_type: str, conditions: dict[str, Any], actions: dict[str, Any], source_incident_id: Optional[str], is_synthetic: bool, synthetic_scenario_id: Optional[str]) -> str",L6:__init__ | L5:policy_proposal_engine,dumps | execute | text | utc_now,asyncio | policy | sqlalchemy | time,db_write,yes,67,Create a policy rule from an approved proposal.
policies,L6,policy_proposal_write_driver,PolicyProposalWriteDriver.create_proposal,"async create_proposal(tenant_id: str, proposal_name: str, proposal_type: str, rationale: str, proposed_rule: dict[str, Any], triggering_feedback_ids: list[str]) -> str",L6:__init__ | L5:policy_proposal_engine,PolicyProposal | add | flush | str | utc_now,asyncio | policy | sqlalchemy | time,db_write,yes,29,Create a new policy proposal in draft status.
policies,L6,policy_proposal_write_driver,PolicyProposalWriteDriver.create_version,"async create_version(proposal_id: UUID, version_number: int, rule_snapshot: dict[str, Any], created_by: str, change_reason: str) -> str",L6:__init__ | L5:policy_proposal_engine,PolicyVersion | add | flush | str | utc_now,asyncio | policy | sqlalchemy | time,db_write,yes,26,Create a policy version snapshot.
policies,L6,policy_proposal_write_driver,PolicyProposalWriteDriver.delete_policy_rule,"async delete_policy_rule(rule_id: str, tenant_id: str) -> bool",L6:__init__ | L5:policy_proposal_engine,execute | text,asyncio | policy | sqlalchemy | time,db_write,yes,19,Delete a policy rule.
policies,L6,policy_proposal_write_driver,PolicyProposalWriteDriver.update_proposal_status,"async update_proposal_status(proposal_id: UUID, new_status: str, reviewed_at: Optional[datetime], reviewed_by: Optional[str], review_notes: Optional[str], effective_from: Optional[datetime]) -> bool",L6:__init__ | L5:policy_proposal_engine,execute | update | values | where,asyncio | policy | sqlalchemy | time,db_write,yes,30,Update a proposal's status and review metadata.
policies,L6,policy_proposal_write_driver,get_policy_proposal_write_driver,get_policy_proposal_write_driver(session: AsyncSession) -> PolicyProposalWriteDriver,L6:__init__ | L5:policy_proposal_engine,PolicyProposalWriteDriver,asyncio | policy | sqlalchemy | time,pure,no,5,Factory function for PolicyProposalWriteDriver.
policies,L6,policy_read_driver,PolicyReadDriver.__init__,__init__(session: Session),L6:__init__ | L5:customer_policy_read_engine,,killswitch | pydantic | sqlalchemy | sqlmodel | tenant,pure,no,3,Initialize with database session.
policies,L6,policy_read_driver,PolicyReadDriver._to_guardrail_dto,_to_guardrail_dto(guardrail: DefaultGuardrail) -> GuardrailDTO,L6:__init__ | L5:customer_policy_read_engine,GuardrailDTO,killswitch | pydantic | sqlalchemy | sqlmodel | tenant,pure,no,11,Transform ORM model to DTO.
policies,L6,policy_read_driver,PolicyReadDriver.get_guardrail_by_id,get_guardrail_by_id(guardrail_id: str) -> Optional[GuardrailDTO],L6:__init__ | L5:customer_policy_read_engine,_to_guardrail_dto | exec | first | select | where,killswitch | pydantic | sqlalchemy | sqlmodel | tenant,pure,no,20,Get a single guardrail by ID.
policies,L6,policy_read_driver,PolicyReadDriver.get_tenant_budget_settings,get_tenant_budget_settings(tenant_id: str) -> Optional[TenantBudgetDataDTO],L6:__init__ | L5:customer_policy_read_engine,TenantBudgetDataDTO | exec | first | getattr | select | where,killswitch | pydantic | sqlalchemy | sqlmodel | tenant,pure,no,30,Get raw tenant budget settings.
policies,L6,policy_read_driver,PolicyReadDriver.get_usage_sum_since,"get_usage_sum_since(tenant_id: str, since: datetime) -> UsageSumDTO",L6:__init__ | L5:customer_policy_read_engine,UsageSumDTO | and_ | coalesce | exec | first | int | select | sum | where,killswitch | pydantic | sqlalchemy | sqlmodel | tenant,pure,no,25,Get total usage in cents since a given time.
policies,L6,policy_read_driver,PolicyReadDriver.list_all_guardrails,list_all_guardrails() -> List[GuardrailDTO],L6:__init__ | L5:customer_policy_read_engine,_to_guardrail_dto | exec | list | order_by | select,killswitch | pydantic | sqlalchemy | sqlmodel | tenant,pure,no,11,List all guardrails ordered by priority.
policies,L6,policy_read_driver,get_policy_read_driver,get_policy_read_driver(session: Session) -> PolicyReadDriver,L6:__init__ | L5:customer_policy_read_engine,PolicyReadDriver,killswitch | pydantic | sqlalchemy | sqlmodel | tenant,pure,no,11,Get PolicyReadDriver instance.
policies,L6,policy_rules_driver,PolicyRulesDriver.__init__,__init__(session: AsyncSession),L5:policy_rules_engine,,asyncio | policy_control_plane | sqlalchemy,pure,no,2,
policies,L6,policy_rules_driver,PolicyRulesDriver.add_integrity,add_integrity(integrity: 'PolicyRuleIntegrity') -> None,L5:policy_rules_engine,add,asyncio | policy_control_plane | sqlalchemy,db_write,no,8,Add an integrity record to the session.
policies,L6,policy_rules_driver,PolicyRulesDriver.add_rule,add_rule(rule: 'PolicyRule') -> None,L5:policy_rules_engine,add,asyncio | policy_control_plane | sqlalchemy,db_write,no,8,Add a rule to the session.
policies,L6,policy_rules_driver,PolicyRulesDriver.fetch_rule_by_id,"async fetch_rule_by_id(tenant_id: str, rule_id: str) -> Optional['PolicyRule']",L5:policy_rules_engine,execute | scalar_one_or_none | select | where,asyncio | policy_control_plane | sqlalchemy,db_write,yes,23,Fetch a rule by ID with tenant scope.
policies,L6,policy_rules_driver,PolicyRulesDriver.flush,async flush() -> None,L5:policy_rules_engine,flush,asyncio | policy_control_plane | sqlalchemy,db_write,yes,3,Flush pending changes without committing.
policies,L6,policy_rules_driver,get_policy_rules_driver,get_policy_rules_driver(session: AsyncSession) -> PolicyRulesDriver,L5:policy_rules_engine,PolicyRulesDriver,asyncio | policy_control_plane | sqlalchemy,pure,no,3,Factory function for PolicyRulesDriver.
policies,L6,policy_rules_read_driver,PolicyRulesReadDriver.__init__,__init__(session: AsyncSession),L6:__init__ | L5:policies_rules_query_engine,,asyncio | policy_control_plane | sqlalchemy | time,pure,no,2,
policies,L6,policy_rules_read_driver,PolicyRulesReadDriver.count_policy_rules,"async count_policy_rules(tenant_id: str, status: str) -> int",L6:__init__ | L5:policies_rules_query_engine,count | execute | scalar | select | where,asyncio | policy_control_plane | sqlalchemy | time,db_write,yes,13,Count policy rules for tenant.
policies,L6,policy_rules_read_driver,PolicyRulesReadDriver.fetch_policy_rule_by_id,"async fetch_policy_rule_by_id(tenant_id: str, rule_id: str) -> Optional[dict]",L6:__init__ | L5:policies_rules_query_engine,coalesce | count | execute | first | getattr | group_by | join | label | max | outerjoin | select | subquery | timedelta | utc_now | where,asyncio | policy_control_plane | sqlalchemy | time,db_write,yes,69,Fetch policy rule detail. Returns None if not found.
policies,L6,policy_rules_read_driver,PolicyRulesReadDriver.fetch_policy_rules,"async fetch_policy_rules(tenant_id: str) -> tuple[list[dict], int]",L6:__init__ | L5:policies_rules_query_engine,all | and_ | coalesce | count | desc | dict | execute | group_by | join | label | limit | max | nullslast | offset | order_by,asyncio | policy_control_plane | sqlalchemy | time,db_write,yes,117,Fetch policy rules with filters and pagination.
policies,L6,policy_rules_read_driver,get_policy_rules_read_driver,get_policy_rules_read_driver(session: AsyncSession) -> PolicyRulesReadDriver,L6:__init__ | L5:policies_rules_query_engine,PolicyRulesReadDriver,asyncio | policy_control_plane | sqlalchemy | time,pure,no,3,Factory function for PolicyRulesReadDriver.
policies,L6,proposals_read_driver,ProposalsReadDriver.__init__,__init__(session: AsyncSession),L6:__init__ | L5:policies_proposals_query_engine,,asyncio | policy | sqlalchemy | time,pure,no,2,
policies,L6,proposals_read_driver,ProposalsReadDriver.count_draft_proposals,async count_draft_proposals(tenant_id: str) -> int,L6:__init__ | L5:policies_proposals_query_engine,and_ | count | execute | is_ | scalar | select | select_from | where,asyncio | policy | sqlalchemy | time,db_write,yes,18,Count draft proposals (for badge display).
policies,L6,proposals_read_driver,ProposalsReadDriver.fetch_proposal_by_id,"async fetch_proposal_by_id(tenant_id: str, proposal_id: str) -> Optional[dict]",L6:__init__ | L5:policies_proposals_query_engine,and_ | execute | isinstance | len | now | replace | scalar_one_or_none | select | str | where,asyncio | policy | sqlalchemy | time,db_write,yes,44,Fetch a single proposal by ID. Returns None if not found.
policies,L6,proposals_read_driver,ProposalsReadDriver.fetch_proposals,"async fetch_proposals(tenant_id: str) -> tuple[list[dict], int]",L6:__init__ | L5:policies_proposals_query_engine,all | and_ | append | count | desc | execute | is_ | isinstance | len | limit | now | offset | order_by | replace | scalar,asyncio | policy | sqlalchemy | time,db_write,yes,86,Fetch policy proposals with filters and pagination.
policies,L6,proposals_read_driver,get_proposals_read_driver,get_proposals_read_driver(session: AsyncSession) -> ProposalsReadDriver,L6:__init__ | L5:policies_proposals_query_engine,ProposalsReadDriver,asyncio | policy | sqlalchemy | time,pure,no,3,Factory function for ProposalsReadDriver.
policies,L6,recovery_matcher,RecoveryMatcher.__init__,__init__(session: 'Session'),?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,pure,no,8,Initialize matcher with required database session.
policies,L6,recovery_matcher,RecoveryMatcher._calculate_time_weight,_calculate_time_weight(age_days: float) -> float,?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,exp,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,pure,no,3,Calculate time decay weight using exponential decay.
policies,L6,recovery_matcher,RecoveryMatcher._compute_confidence,"_compute_confidence(matches: List[Dict[str, Any]], occurrences: int, has_exact_match: bool) -> Tuple[float, Dict[str, Any]]",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,_calculate_time_weight | fromisoformat | get | isinstance | len | max | min | now | replace | round | total_seconds,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,pure,no,70,Compute confidence score using weighted time-decay algorithm.
policies,L6,recovery_matcher,RecoveryMatcher._count_occurrences,"_count_occurrences(error_code: str, days: int) -> int",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,execute | scalar | text | warning,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,db_write,no,23,Count occurrences of error code in recent history.
policies,L6,recovery_matcher,RecoveryMatcher._escalate_to_llm,"async _escalate_to_llm(error_code: str, error_message: str, context: Optional[Dict[str, Any]]) -> Optional[Dict[str, Any]]",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,AsyncClient | get | getenv | group | info | json | loads | post | sanitize_error_message | search | warning,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,external_api,yes,73,Layer 3: LLM reasoning for complex/novel failures.
policies,L6,recovery_matcher,RecoveryMatcher._find_similar_by_embedding,"async _find_similar_by_embedding(error_message: str, limit: int) -> List[Dict[str, Any]]",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,execute | fetchall | float | get_embedding | info | join | len | sanitize_error_message | str | text | warning,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,db_write,yes,71,Layer 2: Vector similarity search for similar failures.
policies,L6,recovery_matcher,RecoveryMatcher._find_similar_failures,"_find_similar_failures(error_code: str, error_signature: str, limit: int) -> List[Dict[str, Any]]",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,execute | fetchall | str | text | warning,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,db_write,no,45,Find similar failures from history.
policies,L6,recovery_matcher,RecoveryMatcher._generate_suggestion,"_generate_suggestion(error_code: str, error_message: str, similar_recoveries: List[str]) -> str",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,items | startswith | upper,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,pure,no,22,Generate recovery suggestion text.
policies,L6,recovery_matcher,RecoveryMatcher._get_cached_recovery,"_get_cached_recovery(error_signature: str) -> Optional[Dict[str, Any]]",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,debug | from_url | get | getenv | loads,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,pure,no,25,Layer 1: Check in-memory/Redis cache for recovery suggestion.
policies,L6,recovery_matcher,RecoveryMatcher._normalize_error,"_normalize_error(payload: Dict[str, Any]) -> Tuple[str, str]",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,encode | get | hexdigest | lower | sanitize_error_message | sha256 | str | strip,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,pure,no,21,Normalize failure payload for matching.
policies,L6,recovery_matcher,RecoveryMatcher._set_cached_recovery,"_set_cached_recovery(error_signature: str, recovery: Dict[str, Any]) -> None",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,debug | dumps | from_url | getenv | setex,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,pure,no,14,Store recovery suggestion in cache.
policies,L6,recovery_matcher,RecoveryMatcher._upsert_candidate,"_upsert_candidate(failure_match_id: str, suggestion: str, confidence: float, explain: Dict[str, Any], error_code: str, error_signature: str, matched_entry: Optional[Dict[str, Any]], source: str) -> int",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,dumps | execute | fetchone | int | scalar | text,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,db_write,no,94,Upsert recovery candidate with occurrence counting.
policies,L6,recovery_matcher,RecoveryMatcher.approve_candidate,"approve_candidate(candidate_id: int, approved_by: str, decision: str, note: str) -> Dict[str, Any]",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,ValueError | execute | fetchone | isinstance | isoformat | loads | str | text,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,db_write,no,113,Approve or reject a recovery candidate.
policies,L6,recovery_matcher,RecoveryMatcher.get_candidates,"get_candidates(status: str, limit: int, offset: int) -> List[Dict[str, Any]]",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,execute | fetchall | isinstance | isoformat | loads | str | text,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,db_write,no,58,List recovery candidates by status.
policies,L6,recovery_matcher,RecoveryMatcher.suggest,"suggest(request: Dict[str, Any]) -> MatchResult",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,MatchResult | _compute_confidence | _count_occurrences | _find_similar_failures | _generate_suggestion | _normalize_error | _upsert_candidate | error | get | info | str,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,pure,no,84,Generate recovery suggestion for a failure.
policies,L6,recovery_matcher,RecoveryMatcher.suggest_hybrid,"async suggest_hybrid(request: Dict[str, Any]) -> MatchResult",?:recovery | ?:workers | ?:worker | ?:recovery_matcher | ?:__init__ | ?:recovery_evaluation_engine | L5:recovery_evaluation_engine | L2:recovery | ?:check_priority5_intent | ?:test_recovery,MatchResult | _escalate_to_llm | _find_similar_by_embedding | _get_cached_recovery | _normalize_error | _set_cached_recovery | get | info | min | str | suggest,httpx | infra | redis | sanitize | sqlalchemy | sqlmodel | vector_store,pure,yes,107,Hybrid ML recovery suggestion using 3-layer lookup.
policies,L6,recovery_write_driver,RecoveryWriteService.__init__,__init__(session: Session),L2:recovery | L2:recovery_ingest,,infra | sqlalchemy | sqlmodel,pure,no,2,
policies,L6,recovery_write_driver,RecoveryWriteService.enqueue_evaluation_db_fallback,"enqueue_evaluation_db_fallback(candidate_id: int, idempotency_key: str) -> bool",L2:recovery | L2:recovery_ingest,execute | text,infra | sqlalchemy | sqlmodel,db_write,no,34,Enqueue evaluation to DB fallback via stored function.
policies,L6,recovery_write_driver,RecoveryWriteService.get_candidate_by_idempotency_key,"get_candidate_by_idempotency_key(idempotency_key: str) -> Optional[Tuple[int, str]]",L2:recovery | L2:recovery_ingest,execute | fetchone | str | text,infra | sqlalchemy | sqlmodel,db_write,no,24,Get candidate by idempotency key (for conflict handling).
policies,L6,recovery_write_driver,RecoveryWriteService.insert_suggestion_provenance,"insert_suggestion_provenance(suggestion_id: int, event_type: str, details_json: str, action_id: Optional[int], confidence_before: float, actor: str) -> None",L2:recovery | L2:recovery_ingest,execute | text,infra | sqlalchemy | sqlmodel,db_write,no,39,Insert provenance record for a suggestion update.
policies,L6,recovery_write_driver,RecoveryWriteService.update_recovery_candidate,"update_recovery_candidate(candidate_id: int, updates: list, params: Dict[str, Any]) -> None",L2:recovery | L2:recovery_ingest,execute | join | text,infra | sqlalchemy | sqlmodel,db_write,no,18,Update recovery candidate with dynamic field list.
policies,L6,recovery_write_driver,RecoveryWriteService.upsert_recovery_candidate,"upsert_recovery_candidate(failure_match_id: str, suggestion: str, confidence: float, explain_json: str, error_code: str, error_signature: str, source: str, idempotency_key: str) -> Tuple[int, bool, int]",L2:recovery | L2:recovery_ingest,execute | fetchone | text,infra | sqlalchemy | sqlmodel,db_write,no,78,Atomic upsert: INSERT ... ON CONFLICT DO UPDATE RETURNING.
policies,L6,scope_resolver,ScopeResolutionResult.to_snapshot,to_snapshot() -> dict,L7:policy_scope | ?:test_export_scope_resolution | ?:test_scope_selector,,db | policy_scope | sqlmodel,pure,no,17,Convert to snapshot dict for immutable storage.
policies,L6,scope_resolver,ScopeResolver.__init__,__init__(session: Optional[Session]),L7:policy_scope | ?:test_export_scope_resolution | ?:test_scope_selector,,db | policy_scope | sqlmodel,pure,no,8,Initialize scope resolver.
policies,L6,scope_resolver,ScopeResolver._get_scope,"_get_scope(session: Session, policy_id: str, tenant_id: str) -> Optional[PolicyScope]",L7:policy_scope | ?:test_export_scope_resolution | ?:test_scope_selector,exec | first | select | where,db | policy_scope | sqlmodel,pure,no,13,Get scope from database.
policies,L6,scope_resolver,ScopeResolver._load_scopes,"_load_scopes(session: Session, tenant_id: str) -> list[PolicyScope]",L7:policy_scope | ?:test_export_scope_resolution | ?:test_scope_selector,all | exec | list | select | where,db | policy_scope | sqlmodel,pure,no,5,Load all scopes for a tenant.
policies,L6,scope_resolver,ScopeResolver.get_scope_for_policy,"get_scope_for_policy(policy_id: str, tenant_id: str) -> Optional[PolicyScope]",L7:policy_scope | ?:test_export_scope_resolution | ?:test_scope_selector,Session | _get_scope,db | policy_scope | sqlmodel,pure,no,20,Get the scope configuration for a specific policy.
policies,L6,scope_resolver,ScopeResolver.matches_scope,"matches_scope(scope: PolicyScope, context: RunContext) -> bool",L7:policy_scope | ?:test_export_scope_resolution | ?:test_scope_selector,matches,db | policy_scope | sqlmodel,pure,no,16,Check if a single scope matches the run context.
policies,L6,scope_resolver,ScopeResolver.resolve_applicable_policies,resolve_applicable_policies(context: RunContext) -> ScopeResolutionResult,L7:policy_scope | ?:test_export_scope_resolution | ?:test_scope_selector,ScopeResolutionResult | ScopeType | Session | _load_scopes | append | info | isoformat | len | list | matches_scope | now | set,db | policy_scope | sqlmodel,pure,no,76,Resolve all policies that apply to the given run context.
policies,L6,scope_resolver,get_scope_resolver,get_scope_resolver() -> ScopeResolver,L7:policy_scope | ?:test_export_scope_resolution | ?:test_scope_selector,ScopeResolver,db | policy_scope | sqlmodel,pure,no,6,Get or create ScopeResolver singleton.
policies,L6,symbol_table,Scope.define,define(symbol: Symbol) -> None,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,ValueError,grammar,pure,no,5,Define a symbol in this scope.
policies,L6,symbol_table,Scope.get_all_symbols,"get_all_symbols() -> Dict[str, Symbol]",?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,get_all_symbols | update,grammar,pure,no,7,Get all visible symbols including parent scopes.
policies,L6,symbol_table,Scope.lookup,"lookup(name: str, local_only: bool) -> Optional[Symbol]",?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,lookup,grammar,pure,no,16,Look up a symbol by name.
policies,L6,symbol_table,Scope.lookup_by_category,lookup_by_category(category: PolicyCategory) -> List[Symbol],?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,extend | lookup_by_category | values,grammar,pure,no,6,Get all symbols in a category.
policies,L6,symbol_table,Symbol.__repr__,__repr__() -> str,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,,grammar,pure,no,3,
policies,L6,symbol_table,SymbolTable.__init__,__init__(),?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,Scope | _define_builtins,grammar,pure,no,10,
policies,L6,symbol_table,SymbolTable.__str__,__str__() -> str,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,append | join | values,grammar,pure,no,7,
policies,L6,symbol_table,SymbolTable._define_builtins,_define_builtins() -> None,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,Symbol | define,grammar,pure,no,29,Define built-in symbols.
policies,L6,symbol_table,SymbolTable.add_reference,"add_reference(name: str, referenced_from: str) -> None",?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,append | lookup,grammar,pure,no,5,Track a reference to a symbol.
policies,L6,symbol_table,SymbolTable.define,define(symbol: Symbol) -> None,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,append | define,grammar,pure,no,9,Define a symbol in current scope.
policies,L6,symbol_table,SymbolTable.enter_scope,"enter_scope(name: str, category: Optional[PolicyCategory]) -> Scope",?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,Scope | append,grammar,pure,no,20,Enter a new scope.
policies,L6,symbol_table,SymbolTable.exit_scope,exit_scope() -> Scope,?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,ValueError | len | pop,grammar,pure,no,12,"Exit current scope, returning to parent."
policies,L6,symbol_table,SymbolTable.get_policies,get_policies() -> List[Symbol],?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,values,grammar,pure,no,3,Get all policy symbols.
policies,L6,symbol_table,SymbolTable.get_rules,get_rules() -> List[Symbol],?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,extend | values,grammar,pure,no,6,Get all rule symbols.
policies,L6,symbol_table,SymbolTable.get_symbols_by_category,"get_symbols_by_category(category: PolicyCategory, symbol_type: Optional[SymbolType]) -> List[Symbol]",?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,get | sorted,grammar,pure,no,17,Get all symbols in a category.
policies,L6,symbol_table,SymbolTable.get_unreferenced_symbols,get_unreferenced_symbols() -> List[Symbol],?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,append | values,grammar,pure,no,8,Get symbols that are never referenced (for dead code analysis).
policies,L6,symbol_table,SymbolTable.lookup,lookup(name: str) -> Optional[Symbol],?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,lookup,grammar,pure,no,3,Look up a symbol by name.
policies,L6,symbol_table,SymbolTable.lookup_policy,lookup_policy(name: str) -> Optional[Symbol],?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,lookup,grammar,pure,no,6,Look up a policy symbol specifically.
policies,L6,symbol_table,SymbolTable.lookup_rule,"lookup_rule(name: str, policy: Optional[str]) -> Optional[Symbol]",?:ir_builder | ?:__init__ | L5:ir_builder | ?:test_m20_ir,lookup | lookup_policy,grammar,pure,no,24,Look up a rule symbol.
