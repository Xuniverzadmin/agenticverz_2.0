Panel_ID,Domain,Subdomain,Topic,Slot,Class,State,Question,Capability
ACC-BL-IN-O1,ACCOUNT,BILLING,INVOICES,1,evidence,EMPTY,Invoice list & status,
ACC-BL-IN-O2,ACCOUNT,BILLING,INVOICES,2,execution,EMPTY,Download / export invoices,
ACC-BL-US-O1,ACCOUNT,BILLING,USAGE,1,interpretation,EMPTY,"Usage summary (tokens, runs, spend)",
ACC-BL-US-O2,ACCOUNT,BILLING,USAGE,2,interpretation,EMPTY,Cost trend & burn rate,
ACC-PR-OV-O1,ACCOUNT,PROFILE,OVERVIEW,1,interpretation,EMPTY,"Account identity summary (org name, plan, region)",
ACC-PR-OV-O2,ACCOUNT,PROFILE,OVERVIEW,2,interpretation,EMPTY,Primary owner & admin contacts,
ACC-TC-CP-O1,ACCOUNT,TRUST_COMPLIANCE,COMPLIANCE,1,interpretation,EMPTY,"Compliance status (SOC2, ISO, etc.)",
ACC-TC-CP-O2,ACCOUNT,TRUST_COMPLIANCE,COMPLIANCE,2,execution,EMPTY,Evidence export (for auditors / CTO),
ACC-UA-MB-O1,ACCOUNT,USERS_ACCESS,MEMBERS,1,evidence,EMPTY,User list with roles,
ACC-UA-MB-O2,ACCOUNT,USERS_ACCESS,MEMBERS,2,execution,EMPTY,Invite / deactivate users,
ACC-UA-RL-O1,ACCOUNT,USERS_ACCESS,ROLES,1,evidence,EMPTY,Role definitions & permissions,
ACT-LLM-COMP-O1,ACTIVITY,LLM_RUNS,COMPLETED,1,interpretation,DRAFT,Show how many LLM runs have completed in the selected window. What it shows: - Total completed runs count What it explicitly does NOT show: - No success/failure split - No duration - No cost,activity.completed_runs
ACT-LLM-COMP-O2,ACTIVITY,LLM_RUNS,COMPLETED,2,interpretation,DRAFT,Surface how many completed runs finished successfully. What it shows: - Count of successful runs What it explicitly does NOT show: - No quality scoring - No downstream impact - No policy attribution,activity.summary
ACT-LLM-COMP-O3,ACTIVITY,LLM_RUNS,COMPLETED,3,interpretation,EMPTY,Expose completed runs that ended in failure. What it shows: - Count of failed runs What it explicitly does NOT show: - No root cause - No retry controls - No blame attribution,
ACT-LLM-COMP-O4,ACTIVITY,LLM_RUNS,COMPLETED,4,interpretation,DRAFT,Highlight runs that completed but came close to limits. What it shows: - Count of completed runs that were near: - Cost limits - Time limits - Token limits What it explicitly does NOT show: - No violations - No enforcement - No tuning actions,activity.customer_activity
ACT-LLM-COMP-O5,ACTIVITY,LLM_RUNS,COMPLETED,5,interpretation,DRAFT,Show runs that ended intentionally before completion. What it shows: - Count of aborted or cancelled runs What it explicitly does NOT show: - No initiator identity - No reason codes - No recovery options,activity.runtime_traces
ACT-LLM-LIVE-O1,ACTIVITY,LLM_RUNS,LIVE,1,interpretation,DRAFT,"Show how many LLM runs are currently in progress. What it shows: - Total number of live LLM runs What it explicitly does NOT show: - No breakdown by model, agent, user, or cost - No status reasons - No controls",activity.live_runs
ACT-LLM-LIVE-O2,ACTIVITY,LLM_RUNS,LIVE,2,interpretation,EMPTY,"Surface live runs exceeding expected execution time. What it shows: - Count of live runs exceeding time threshold (e.g., > X minutes) What it explicitly does NOT show: - No root cause - No cost data - No termination control",
ACT-LLM-LIVE-O3,ACTIVITY,LLM_RUNS,LIVE,3,interpretation,DRAFT,Highlight live runs that are approaching failure or limits. What it shows: - Count of live runs flagged as near-threshold or unstable What it explicitly does NOT show: - No policy actions - No manual override - No mitigation controls,activity.jobs_list
ACT-LLM-LIVE-O4,ACTIVITY,LLM_RUNS,LIVE,4,interpretation,DRAFT,"Indicate whether telemetry, logs, and traces are flowing for live runs. What it shows: - Percentage or status of live runs emitting evidence What it explicitly does NOT show: - No log contents - No replay - No export",activity.worker_runs
ACT-LLM-LIVE-O5,ACTIVITY,LLM_RUNS,LIVE,5,interpretation,DRAFT,"Provide a coarse distribution of live runs by major dimension. What it shows: - Distribution by LLM provider, agent, or trigger type (exact dimension decided later) What it explicitly does NOT show: - No drill-down - No per-run detail - No controls",activity.health_status
ACT-LLM-SIG-O1,ACTIVITY,LLM_RUNS,SIGNALS,1,interpretation,DRAFT,Surface what is happening right now that matters — the primary attention surface. What it shows: - Critical failures - Critical successes - Active risk conditions - Only currently active or very recent signals What it explicitly does NOT show: - No historical signals - No controls or actions - No policy execution,activity.signals
ACT-LLM-SIG-O2,ACTIVITY,LLM_RUNS,SIGNALS,2,interpretation,DRAFT,"Surface runs approaching failure, policy, or cost limits (threshold proximity). What it shows: - Token limits nearing breach - Cost ceilings approaching - Timeouts nearing SLA breach - Frequency/rate-limit pressure What it explicitly does NOT show: - No actions or mitigations - No policy controls - No historical trends",activity.predictions_list
ACT-LLM-SIG-O3,ACTIVITY,LLM_RUNS,SIGNALS,3,interpretation,DRAFT,Surface temporal signals — behavior patterns over time indicating instability. What it shows: - Frequent retries - Latency spikes - Repeated partial failures - Flapping success/failure patterns What it explicitly does NOT show: - No single-event failures (patterns only) - No root cause analysis - No remediation controls,activity.predictions_summary
ACT-LLM-SIG-O4,ACTIVITY,LLM_RUNS,SIGNALS,4,interpretation,DRAFT,Surface economic deviations — where money is being lost or saved unexpectedly. What it shows: - Cost overruns - Cost savers (first-class signals) - Efficiency anomalies - Unexpected cost spikes or drops What it explicitly does NOT show: - No budget controls - No policy enforcement - No historical cost trends,activity.discovery_list
ACT-LLM-SIG-O5,ACTIVITY,LLM_RUNS,SIGNALS,5,interpretation,DRAFT,"Synthesize attention priority — what to look at first and why. What it shows: - Prioritized attention queue combining: - Severity (O1) - Proximity (O2) - Pattern persistence (O3) - Economic impact (O4) - Grouping by LLM, Agent, or Human - Cross-cutting issues What it explicitly does NOT show: - No decisions or actions - No policy execution - No drill-down details",activity.discovery_stats
ANA-CST-USG-O1,ANALYTICS,COST,USAGE,1,interpretation,EMPTY,"Show limit policy coverage and blast radius — how many policies and how wide. What it shows: - Total active limit policies - Coverage by scope: org-wide, by LLM, by agent, by human executor - Blast radius distribution: narrow, medium, broad - Breakdowns: cost limits, token limits, rate limits, time limits What it explicitly does NOT show: - No configuration controls - No threshold editing - No policy creation",
ANA-CST-USG-O2,ANALYTICS,COST,USAGE,2,interpretation,EMPTY,"Show configured limits vs actual usage — how close are we running to limits. What it shows: - Per limit: configured threshold, actual observed usage (p50/p95/p99) - Headroom remaining (%) - Slices: by LLM, by agent, by human, by policy - Chronic near-limit behavior, over-conservative limits What it explicitly does NOT show: - No threshold editing - No policy changes",
ANA-CST-USG-O3,ANALYTICS,COST,USAGE,3,interpretation,EMPTY,"Show violations, prevented runs, and overrides — what limits actually stopped. What it shows: - Total violation attempts, prevented executions - Allowed-but-flagged executions, explicit human overrides - Categorization: cost overrun, token exhaustion, timeouts, rate bursts - Override analysis: who overrode, which policy, outcome after override What it explicitly does NOT show: - No override controls - No policy editing",
ANA-CST-USG-O4,ANALYTICS,COST,USAGE,4,interpretation,EMPTY,"Show savings and risk reduction attribution — what policies saved. What it shows: - Cost saved, tokens saved, time saved, incidents avoided - Attribution: by policy, by LLM, by agent, by human, by time window - Risk lens: high-savings/low-friction vs low-savings/high-friction What it explicitly does NOT show: - No policy controls - No threshold editing",
ANA-CST-USG-O5,ANALYTICS,COST,USAGE,5,interpretation,EMPTY,"Show policy performance health and recommendations — which policies work. What it shows: - Health classification: effective & stable, effective but noisy, ineffective, harmful - Signals: violation-to-prevention ratio, override frequency, savings vs disruption - Flags for Drafts: consider tightening, relaxing, deprecating (read-only) What it explicitly does NOT show: - No auto-actions - No policy editing",
CON-CS-KY-O1,CONNECTIVITY,CREDENTIALS_SECRETS,KEYS,1,evidence,EMPTY,API keys & secret references,
CON-CS-KY-O2,CONNECTIVITY,CREDENTIALS_SECRETS,KEYS,2,interpretation,EMPTY,Rotation status & expiry risk,
CON-HD-ER-O1,CONNECTIVITY,HEALTH_DIAGNOSTICS,ERRORS,1,evidence,EMPTY,Connection failures & retries,
CON-HD-ER-O2,CONNECTIVITY,HEALTH_DIAGNOSTICS,ERRORS,2,interpretation,EMPTY,Degraded connectivity alerts,
CON-NW-PT-O1,CONNECTIVITY,NETWORK,PATHS,1,interpretation,EMPTY,Network paths & routing overview,
CON-NW-PT-O2,CONNECTIVITY,NETWORK,PATHS,2,interpretation,EMPTY,Latency & failure points,
CON-PR-LM-O1,CONNECTIVITY,PROVIDERS,LIMITS,1,evidence,EMPTY,Provider rate limits & quotas,
CON-PR-ST-O1,CONNECTIVITY,PROVIDERS,STATUS,1,interpretation,EMPTY,"Connected LLM providers (OpenAI, Anthropic, etc.)",
CON-PR-ST-O2,CONNECTIVITY,PROVIDERS,STATUS,2,interpretation,EMPTY,Provider health & outages,
INC-EV-ACT-O1,INCIDENTS,EVENTS,ACTIVE,1,evidence,DRAFT,Show the canonical set of currently active incidents. What it shows: - Failures (hard policy violation) - Near-threshold escalations (policy-defined) - Prevented overruns (policy intervened) - Governance-triggered halts - One row = one incident (no aggregation) What it explicitly does NOT show: - No trends or interpretation - No raw signals (those are in Activity) - No actions or controls,incidents.list
INC-EV-ACT-O2,INCIDENTS,EVENTS,ACTIVE,2,interpretation,DRAFT,"Explain why each incident occurred — cause and trigger classification. What it shows: - Triggering condition (cost threshold, token limit, time SLA, policy rule) - Whether detected after violation or prevented before violation What it explicitly does NOT show: - No blame attribution - No actions or remediation - No policy editing",incidents.summary
INC-EV-ACT-O3,INCIDENTS,EVENTS,ACTIVE,3,interpretation,DRAFT,Show control state — whether each incident is contained or still dangerous. What it shows: - Still active / paused / quarantined - Human override applied or not - Auto-remediation attempted or not - Guardrails holding or breached What it explicitly does NOT show: - No escalation actions - No policy changes - No resolution controls,incidents.metrics
INC-EV-ACT-O4,INCIDENTS,EVENTS,ACTIVE,4,interpretation,DRAFT,Show impact assessment — actual damage or prevented damage. What it shows: - Actual cost incurred - Cost prevented by policy - Downtime / latency impact - Business impact classification (if available) - Prevented damage is first-class What it explicitly does NOT show: - No cost controls - No budget editing - No forecasts,incidents.patterns
INC-EV-ACT-O5,INCIDENTS,EVENTS,ACTIVE,5,interpretation,DRAFT,"Provide attribution and escalation context for each incident. What it shows: - Attribution by LLM, Agent, Human, Policy - Repeated incident marker - Linked past incidents (if any) - Escalation eligibility (yes/no) What it explicitly does NOT show: - No approval actions - No policy changes - No resolution execution",incidents.infra_summary
INC-EV-HIST-O1,INCIDENTS,EVENTS,HISTORICAL,1,interpretation,DRAFT,"Show incident volume and trend baseline over time. What it shows: - Incident counts by time window (day / week / month) - Trend direction (rising, flat, declining) - Aggregates only (no per-incident rows) What it explicitly does NOT show: - No individual incidents - No real-time data - No controls",incidents.historical_list
INC-EV-HIST-O2,INCIDENTS,EVENTS,HISTORICAL,2,interpretation,DRAFT,"Show incident type distribution over time. What it shows: - Failure type distribution (success breach, failure, near-threshold) - Policy-related vs non-policy - Cost-related vs performance-related - Infrastructure vs model vs human-triggered What it explicitly does NOT show: - No individual incidents - No real-time data - No controls",incidents.guard_list
INC-EV-HIST-O3,INCIDENTS,EVENTS,HISTORICAL,3,interpretation,DRAFT,Show repeatability and recurrence analysis — which incidents keep coming back. What it shows: - Same root cause recurring - Same policy involved repeatedly - Same agent / LLM / human role involved - Short recurrence intervals What it explicitly does NOT show: - No blame attribution - No policy changes - No actions,incidents.v1_list
INC-EV-HIST-O4,INCIDENTS,EVENTS,HISTORICAL,4,interpretation,DRAFT,Show cost and impact over time — true economic footprint. What it shows: - Total cost incurred - Total cost prevented (via governance) - Cost avoided vs cost paid ratio - Long-term cost trend - Finalized costs only (no forecasts) What it explicitly does NOT show: - No budget controls - No projections - No real-time data,incidents.ops_list
INC-EV-HIST-O5,INCIDENTS,EVENTS,HISTORICAL,5,interpretation,DRAFT,Surface systemic signals and governance pressure from historical patterns. What it shows: - Policies with chronic incident association - Thresholds consistently too tight or too loose - Areas where human overrides dominate - Domains where automation underperforms What it explicitly does NOT show: - No policy execution - No approvals - No automated actions,incidents.integration_stats
INC-EV-RES-O1,INCIDENTS,EVENTS,RESOLVED,1,evidence,DRAFT,Show the canonical list of recently resolved incidents. What it shows: - Incidents transitioned from Active → Resolved - Within the recent window (not long-term history) - Resolution timestamp mandatory - Original incident ID preserved What it explicitly does NOT show: - No active incidents - No historical aggregation - No controls,incidents.resolved_list
INC-EV-RES-O2,INCIDENTS,EVENTS,RESOLVED,2,interpretation,DRAFT,Show how each incident was resolved — resolution method. What it shows: - Automatic recovery - Policy enforcement succeeded - Human override - Manual intervention - System rollback - External dependency recovery - Each incident maps to exactly one primary resolution method What it explicitly does NOT show: - No resolution actions - No policy editing,incidents.recovery_actions
INC-EV-RES-O3,INCIDENTS,EVENTS,RESOLVED,3,interpretation,DRAFT,Show time-to-resolution (TTR) and SLA compliance. What it shows: - Detection time - Resolution time - Total duration in exception - SLA classification (within / breached) What it explicitly does NOT show: - No contextual judgment - No policy adjustments - No controls,incidents.recovery_candidates
INC-EV-RES-O4,INCIDENTS,EVENTS,RESOLVED,4,interpretation,DRAFT,Show outcome and impact post-resolution — the final reality. What it shows: - Final cost incurred - Cost prevented - Residual impact (if any) - Side effects introduced by resolution What it explicitly does NOT show: - No projections - No forecasts - No policy controls,incidents.graduation_list
INC-EV-RES-O5,INCIDENTS,EVENTS,RESOLVED,5,interpretation,DRAFT,Surface learning and follow-up signals from resolved incidents. What it shows: - Policy change suggested (yes/no) - Recurrence risk (low/medium/high) - Similar past incidents detected - Escalate to: Policies / Humans / Engineering backlog What it explicitly does NOT show: - No policy edits - No approvals - No automated actions,incidents.replay_summary
LOG-REC-AUD-O1,LOGS,RECORDS,AUDIT,1,evidence,DRAFT,Show identity and authentication lifecycle — who accessed and how.,logs.traces_list
LOG-REC-AUD-O2,LOGS,RECORDS,AUDIT,2,evidence,DRAFT,Show authorization and access decisions — what each identity was allowed to do.,logs.rbac_audit
LOG-REC-AUD-O3,LOGS,RECORDS,AUDIT,3,evidence,DRAFT,"Show trace and log access audit — who viewed, exported, or modified observability data.",logs.ops_audit
LOG-REC-AUD-O4,LOGS,RECORDS,AUDIT,4,evidence,DRAFT,Show integrity and tamper detection — was any audit data altered or compromised.,logs.status_history
LOG-REC-AUD-O5,LOGS,RECORDS,AUDIT,5,evidence,DRAFT,"Show compliance and export record — what audit evidence was produced, shared, or certified.",logs.status_stats
LOG-REC-LLM-O1,LOGS,RECORDS,LLM_RUNS,1,evidence,DRAFT,Show the run log envelope — canonical immutable record per run.,logs.runtime_traces
LOG-REC-LLM-O2,LOGS,RECORDS,LLM_RUNS,2,evidence,DRAFT,Show execution trace — step-by-step progression of the run.,logs.activity_runs
LOG-REC-LLM-O3,LOGS,RECORDS,LLM_RUNS,3,evidence,DRAFT,Show threshold and policy interaction trace — governance footprint per run.,logs.customer_runs
LOG-REC-LLM-O4,LOGS,RECORDS,LLM_RUNS,4,evidence,DRAFT,Show 60-second incident replay window — what happened around the inflection point.,logs.tenant_runs
LOG-REC-LLM-O5,LOGS,RECORDS,LLM_RUNS,5,evidence,DRAFT,Show audit and export package — legally defensible evidence bundle.,logs.mismatch_list
LOG-REC-SYS-O1,LOGS,RECORDS,SYSTEM_LOGS,1,evidence,DRAFT,Show environment snapshot — baseline state at run start.,logs.guard_logs
LOG-REC-SYS-O2,LOGS,RECORDS,SYSTEM_LOGS,2,evidence,DRAFT,Show network and bandwidth telemetry — connectivity health during execution.,logs.health_check
LOG-REC-SYS-O3,LOGS,RECORDS,SYSTEM_LOGS,3,evidence,DRAFT,Show infra interrupts and degradation events — what infra did to the run.,logs.ready_check
LOG-REC-SYS-O4,LOGS,RECORDS,SYSTEM_LOGS,4,evidence,DRAFT,Show run-aligned infra replay window — infra state at moment of anomaly.,logs.adapters_health
LOG-REC-SYS-O5,LOGS,RECORDS,SYSTEM_LOGS,5,interpretation,DRAFT,Show infra audit and attribution record — who is responsible.,logs.skills_health
OVR-SUM-CI-O1,OVERVIEW,SUMMARY,COST_INTELLIGENCE,1,interpretation,DRAFT,Give an immediate snapshot of current cost posture without requiring navigation into Activity or Policies. What it shows: - Current spend rate (aggregate) - Trend indicator (up / flat / down) - Primary cost driver (top contributor category) What it explicitly does NOT show: - No historical charts - No per-run breakdown - No policy configuration - No thresholds or limits editing - No predictions,overview.cost_summary
OVR-SUM-CI-O2,OVERVIEW,SUMMARY,COST_INTELLIGENCE,2,interpretation,DRAFT,"Show how total cost is distributed across primary drivers so the user understands what is consuming money at a glance. What it shows: - Cost split by driver category (e.g., model, project, policy bucket) - Percentage contribution per driver - Top 1–3 contributors only What it explicitly does NOT show: - No per-run or per-request logs - No time-series charts - No configuration controls - No thresholds or alerts - No forecasts",overview.cost_by_feature
OVR-SUM-CI-O3,OVERVIEW,SUMMARY,COST_INTELLIGENCE,3,interpretation,DRAFT,"Provide a short-horizon view of cost movement so the user can tell whether spend is trending up, flat, or down. What it shows: - Aggregate cost trend over a short window (e.g., last N periods) - Directional indicator only (↑ ↓ →) - Relative change, not absolute detail What it explicitly does NOT show: - No driver-level breakdown - No per-run or per-model detail - No long-term forecasting - No alerts or policy thresholds - No controls",overview.cost_by_model
OVR-SUM-CI-O4,OVERVIEW,SUMMARY,COST_INTELLIGENCE,4,interpretation,DRAFT,"Provide a near-term cost trajectory based on recent behavior, so the user can anticipate direction without detailed forecasting. What it shows: - Near-term projected direction (increase / stable / decrease) - Confidence band or qualitative confidence (low / medium / high) - Projection based on recent trend only What it explicitly does NOT show: - No long-range forecasts - No scenario modeling - No 'what-if' controls - No policy or alert thresholds - No per-run or per-model detail",overview.cost_anomalies
OVR-SUM-DC-O1,OVERVIEW,SUMMARY,DECISIONS,1,interpretation,DRAFT,"Surface decisions that require explicit human approval or rejection, without forcing navigation into policies or incidents. What it shows: - Count of pending policy approvals - Count of blocked runs awaiting human override - Count of escalations requiring approval/reject decision - Oldest pending decision age (time-based pressure) What it explicitly does NOT show: - No execution buttons - No approval/reject actions - No policy details - No explanations or rationale - No historical decisions",overview.decisions_list
OVR-SUM-DC-O2,OVERVIEW,SUMMARY,DECISIONS,2,interpretation,DRAFT,Provide short-term feedback on recent human decisions so the user can confirm impact without drilling into other domains. What it shows: - Last N decisions taken (approve / reject / override) - Resulting outcome state (resolved / still pending / failed) - Time-to-effect (decision → outcome) What it explicitly does NOT show: - No ability to re-open or change decisions - No policy configuration - No incident details - No execution controls - No historical archive beyond recent window,overview.decisions_count
OVR-SUM-DC-O3,OVERVIEW,SUMMARY,DECISIONS,3,interpretation,DRAFT,"Identify recurrent human decisions that should become policy — where humans are acting as rate limiters instead of governance. What it shows: - Repeated decision patterns (same condition, same outcome, same actors) - Frequency × avoided effort × avoided cost ranking - Cost-overrun overrides approved repeatedly - Manual allowlist decisions trending up - Direct links to Policies → Drafts and Limits → Thresholds (pre-filled context) What it explicitly does NOT show: - No execution or approval ac...",overview.recovery_stats
OVR-SUM-DC-O4,OVERVIEW,SUMMARY,DECISIONS,4,interpretation,DRAFT,"Show decisions avoided by governance — where the system already removed decision load successfully, reinforcing trust in automation. What it shows: - Auto-prevent counts (decisions not escalated) - Avoidance attribution: by policy, by limit, by automation - Trust progression tracking - Policies that avoided human interventions - Agents/LLMs now fully governed (no manual touch for N days) What it explicitly does NOT show: - No execution controls - No policy editing - No approval actions",overview.feedback_summary
OVR-SUM-HL-O1,OVERVIEW,SUMMARY,HIGHLIGHTS,1,interpretation,BOUND,"Provide a single, glanceable snapshot of current system activity so the user can immediately understand whether the system is calm, active, or stressed. What it shows: - Count of currently running LLM executions - Count of executions completed in the last window (e.g. 15 min) - Count of executions currently in near-threshold or risk state - Timestamp of last successful observation update What it explicitly does NOT show: - No logs - No configuration - No historical charts - No per-run drilldo...",overview.activity_snapshot
OVR-SUM-HL-O2,OVERVIEW,SUMMARY,HIGHLIGHTS,2,interpretation,DRAFT,"Surface non-ignorable signals that require human attention, without requiring navigation into domains. What it shows: - Count of active incidents (any severity) - Count of runs currently in near-threshold state - Count of policy violations prevented by governance - Highest severity level currently present (if any) What it explicitly does NOT show: - No historical incidents - No explanations - No root cause - No controls or actions - No links or drilldowns",overview.incident_snapshot
OVR-SUM-HL-O3,OVERVIEW,SUMMARY,HIGHLIGHTS,3,interpretation,EMPTY,"Surface emerging pattern shifts across domains — non-obvious directional change that is not yet an incident but is statistically meaningful. What it shows: - Week-over-week deltas in failure ratios, near-threshold frequency - Prevention counts and escalation latency trends - Trigger frequency and auto-prevent vs override ratio changes - Approach-to-limit velocity shifts - Change-point detection (z-score / percentile shift) What it explicitly does NOT show: - No alerts or actions - No raw data...",
OVR-SUM-HL-O4,OVERVIEW,SUMMARY,HIGHLIGHTS,4,interpretation,DRAFT,"Expose rare but high-impact clusters — low-frequency, high-cost events invisible in averages. What it shows: - Severity-weighted cost/time loss ranking - Long-tail outliers (top 1-2%) - Extreme run duration/cost spikes - Overridden violations with high blast radius - Impact-ranked clusters by LLM, agent, human, policy What it explicitly does NOT show: - No count-based rankings - No actions or controls - No policy editing - No drill-down beyond identification",overview.policy_snapshot
POL-GOV-ACT-O1,POLICIES,GOVERNANCE,ACTIVE,1,evidence,DRAFT,"Show the complete inventory of currently active (enabled + enforced) policies. What it shows: - List/count of all active policies - Grouped by policy type (cost, rate, approval, safety, escalation) - Grouped by scope (global / domain / agent / LLM / user) - Grouped by enforcement mode (hard block / soft block / require approval) What it explicitly does NOT show: - No impact metrics - No drafts or proposals - No historical policies",policies.proposals_list
POL-GOV-ACT-O2,POLICIES,GOVERNANCE,ACTIVE,2,interpretation,DRAFT,"Show enforcement effectiveness — are policies actually doing anything. What it shows: - Per policy: times evaluated, triggered, enforced, bypassed - Dead policies (never triggered) - Over-firing policies - Policies with zero real-world impact What it explicitly does NOT show: - No policy editing - No recommendations - No draft proposals",policies.proposals_summary
POL-GOV-ACT-O3,POLICIES,GOVERNANCE,ACTIVE,3,interpretation,DRAFT,"Show incident prevention and regulation impact — what policies are preventing or shaping. What it shows: - Incidents prevented entirely - Incidents soft-regulated (slowed, capped, gated) - Incidents escalated to human decision - Policy → incident class mapping - Evidence-backed only (claimed prevention without observation ignored) What it explicitly does NOT show: - No policy changes - No enforcement controls - No draft proposals",policies.requests_list
POL-GOV-ACT-O4,POLICIES,GOVERNANCE,ACTIVE,4,interpretation,DRAFT,"Show cost and performance side effects — what governance is costing us. What it shows: - Cost saved by enforcement - Cost introduced by enforcement (latency, friction, rejects) - Trade-offs: safety vs throughput, cost vs success rate, automation vs human load - Numbers only, no value judgment What it explicitly does NOT show: - No policy tuning controls - No recommendations - No forecasts",policies.layer_state
POL-GOV-ACT-O5,POLICIES,GOVERNANCE,ACTIVE,5,interpretation,DRAFT,"Surface governance stress and decision signals — which policies need review. What it shows: - Policies with frequent overrides - Policies frequently hit near thresholds - Policies causing cascading blocks - Policies correlated with user/system friction - Candidates for adjustment, split, promotion, or decommissioning What it explicitly does NOT show: - No policy state changes - No disabling - No auto-rewriting",policies.layer_metrics
POL-GOV-DFT-O1,POLICIES,GOVERNANCE,DRAFTS,1,evidence,DRAFT,"Show observed governance signals — raw lessons from system behavior. What it shows: - Critical failures, critical successes - Near-threshold runs, frequent temporal breaks - Cost overruns, cost savers - High override frequency - Grouped by LLM, Agent, Human actor, Policy gap type What it explicitly does NOT show: - No synthesis or recommendations - No draft policies yet - No actions",policies.drafts_list
POL-GOV-DFT-O2,POLICIES,GOVERNANCE,DRAFTS,2,interpretation,DRAFT,"Show draft policy candidates — machine-proposed governance rules. What it shows: - Auto-generated draft policies - Each draft linked to evidence (incident IDs, run IDs) - Observed benefit or risk per draft - Drafts are non-executable What it explicitly does NOT show: - No enforcement - No activation - No human bypass",policies.versions_list
POL-GOV-DFT-O3,POLICIES,GOVERNANCE,DRAFTS,3,interpretation,DRAFT,"Show draft justification and impact preview — why each draft exists. What it shows: - Problem statement (derived from evidence) - Expected effect: incidents prevented, costs saved, failures reduced - Known risks: false positives, reduced throughput, human friction - Preview based on historical/simulated data only What it explicitly does NOT show: - No future predictions beyond evidence - No black-box proposals - No enforcement",policies.current_version
POL-GOV-DFT-O4,POLICIES,GOVERNANCE,DRAFTS,4,execution,DRAFT,"Provide approval and blast radius control for draft policies. What it shows: - Scope selection: single LLM, group of LLMs, agent class, human role - Blast radius: pilot (1 entity), limited (subset), global - Enforcement mode: observe-only, soft gate, hard block - Explicit choices only, no defaults What it explicitly does NOT show: - No auto-activation - No silent inheritance",policies.conflicts_list
POL-GOV-DFT-O5,POLICIES,GOVERNANCE,DRAFTS,5,execution,DRAFT,"Show decision outcomes and lifecycle routing after human decides. What it shows: - Approved: draft → active policy, moves to Governance → Active - Rejected: draft archived with reason, signal suppressed - Deferred: parked for more data, re-evaluated later - Every draft must end in one of these three states What it explicitly does NOT show: - No silent expiration - No auto-decisions",policies.dependencies_list
POL-GOV-LIB-O1,POLICIES,GOVERNANCE,POLICY_LIBRARY,1,evidence,DRAFT,"Show the global policy catalog — all policies available from Agenticverz backend. What it shows: - Complete list of global policies (cost controls, rate limits, safety guards, etc.) - Metadata: policy ID, category, default enforcement mode, version, maintainer - Read-only discovery surface What it explicitly does NOT show: - No adoption controls - No filtering by current usage - No enforcement",policies.safety_rules
POL-GOV-LIB-O2,POLICIES,GOVERNANCE,POLICY_LIBRARY,2,interpretation,DRAFT,"Show applicability and compatibility matrix — where each policy can be applied. What it shows: - Supported targets: LLMs, agents, human executors, run types - Org-wide applicability - Mutually exclusive policies, required prerequisites, known incompatibilities What it explicitly does NOT show: - No human override at this stage - No adoption actions",policies.ethical_constraints
POL-GOV-LIB-O3,POLICIES,GOVERNANCE,POLICY_LIBRARY,3,interpretation,DRAFT,"Show adoption status and current usage — how and where each policy is used. What it shows: - Adoption state: not used, used in limited scope, used globally - Active scopes: specific LLMs, agents, humans, all runs - Last modified, who approved - Reflects actual enforcement, not intent What it explicitly does NOT show: - No adoption controls - No proposed changes",policies.active_policies
POL-GOV-LIB-O4,POLICIES,GOVERNANCE,POLICY_LIBRARY,4,execution,DRAFT,"Provide attach/detach policy controls — human action to adopt or remove policies. What it shows: - Attach policy to: single LLM, agent group, human role, all runs - Detach policy from any scope - Required inputs: scope selection, enforcement mode, justification (mandatory) - Every change is auditable What it explicitly does NOT show: - No default scope - No silent inheritance",policies.guard_policies
POL-GOV-LIB-O5,POLICIES,GOVERNANCE,POLICY_LIBRARY,5,interpretation,DRAFT,"Show change impact and audit trail for policy adoption changes. What it shows: - Impact preview based on historical runs - Expected prevented incidents, cost savings - Known risks - Audit record: policy, scope before/after, approved by, timestamp - Rollback option (first-class) What it explicitly does NOT show: - No auto-rollback - No silent changes",policies.temporal_policies
POL-LIM-THR-O1,POLICIES,LIMITS,THRESHOLDS,1,evidence,DRAFT,"Show limit policy definition matrix — what limits exist and where they apply. What it shows: - Policy ID/Name, limit type (cost/token/rate/time) - Scope: org/project, LLM(s), agent(s), human executor(s) - Blast radius: single, group, global - Status: active, shadow (observe only), disabled What it explicitly does NOT show: - No usage metrics - No violation data",policies.risk_ceilings
POL-LIM-THR-O2,POLICIES,LIMITS,THRESHOLDS,2,execution,DRAFT,"Provide threshold configuration and fine-tuning controls. What it shows: - Per policy: hard limit (enforced), soft limit (warn/flag) - Grace window: time-based, count-based - Cool-down/reset logic - Dimensions: by LLM, by agent class, by human role, by time - Live preview: 'At current usage, this would have blocked X runs yesterday' What it explicitly does NOT show: - No usage analytics - No violation history",policies.budgets_list
POL-LIM-THR-O3,POLICIES,LIMITS,THRESHOLDS,3,execution,DRAFT,"Provide blast radius and rollout strategy controls. What it shows: - Start scope: single LLM/agent - Expansion path: group → global - Rollout mode: shadow → enforce, partial enforcement (% of runs) - Rollback switch: one-click revert - High blast radius requires shadow period and review acknowledgement What it explicitly does NOT show: - No usage data - No violation data",policies.quota_runs
POL-LIM-THR-O4,POLICIES,LIMITS,THRESHOLDS,4,execution,DRAFT,"Provide experimentation and what-if simulation capabilities. What it shows: - Clone existing policy, modify thresholds - Run simulation against last 7/30/90 days - Compare: blocks vs allows, cost saved vs runs blocked - Comparison view: current policy vs candidate policy - Experiments do not affect live runs, must be named and scoped What it explicitly does NOT show: - No live enforcement - No production impact",policies.quota_tokens
POL-LIM-THR-O5,POLICIES,LIMITS,THRESHOLDS,5,execution,DRAFT,"Provide approval, audit, and activation workflow for threshold changes. What it shows: - Workflow: draft → review → active - Optional multi-approver, activation notes required - Audit log: threshold changes, scope changes, rollbacks, overrides - Post-activation hooks: auto-link to Usage → O3, Governance → Active What it explicitly does NOT show: - No silent activation - No hidden changes",policies.cooldowns_list
POL-LIM-VIO-O1,POLICIES,LIMITS,VIOLATIONS,1,evidence,DRAFT,"Show the violation ledger — immutable record of every limit violation. What it shows: - Run ID, timestamp, policy ID, limit type - Violation mode: hard terminate, temporal fracture, human override - Actor at violation: LLM, agent, human - Enforcement state: blocked, allowed (override) - One row per violation event, overrides do not erase violation What it explicitly does NOT show: - No configuration controls - No policy editing",policies.violations_list
POL-LIM-VIO-O2,POLICIES,LIMITS,VIOLATIONS,2,interpretation,DRAFT,"Show violation classification and attribution — why and who. What it shows: - Aggregations: by limit type, by LLM, by agent, by human, by policy - Primary trigger: cost overrun, token spike, time breach - Secondary factor: retry loop, fan-out, poor prompt, downstream latency What it explicitly does NOT show: - No policy controls - No excuses",policies.cost_incidents
POL-LIM-VIO-O3,POLICIES,LIMITS,VIOLATIONS,3,interpretation,DRAFT,"Show override and fracture analysis — which violations were ignored or softened. What it shows: - Override count, override rate (% of violations) - Fracture duration (avg/p95), re-entry success rate - Breakdown: by role, by policy, by urgency tag - Flags: repeated overrides on same policy, overrides followed by failure What it explicitly does NOT show: - No override controls - No policy editing",policies.simulated_incidents
POL-LIM-VIO-O4,POLICIES,LIMITS,VIOLATIONS,4,interpretation,DRAFT,"Show loss and impact quantification — what violations actually cost. What it shows: - Cost lost, time lost (compute + human wait), productivity lost - Cost saved (from enforced blocks) - Views: by LLM, by agent, by human, by policy, by time window - Saved and lost shown together (no vanity metrics) What it explicitly does NOT show: - No policy controls - No threshold editing",policies.anomalies_list
POL-LIM-VIO-O5,POLICIES,LIMITS,VIOLATIONS,5,interpretation,DRAFT,"Show escalation, evidence, and governance hooks for violations requiring action. What it shows: - Triggers: repeated violations, high override density, high loss concentration - Links to: Governance → Drafts (revision), Governance → Active (review) - Export: audit logs, evidence bundles (SOC2/internal review) - This surface escalates, not fixes What it explicitly does NOT show: - No editing limits - No approvals",policies.divergence_report
